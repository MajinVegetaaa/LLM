{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MajinVegetaaa/LLM/blob/main/Text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Objective:\n",
        "The aim of this project is to make you reivse the contents of Chapter 1 to 5. The best way to reinforce what we have learnt is to try to tweak the examples a little bit.\n",
        "\n",
        "\n",
        "## Task\n",
        "\n",
        "- Create fresh (means random weights) GPT2 type of model as presented in the book.\n",
        "- Download new text from these two books:\n",
        "    - https://www.gutenberg.org/cache/epub/1342/pg1342.txt\n",
        "    - https://www.gutenberg.org/cache/epub/1400/pg1400.txt\n",
        "\n",
        "- Pre-train your model to this text database\n",
        "\n",
        "\n",
        "## Further requirements:\n",
        "    - All codes should be in one collab notebook. Means I should be able to run your collab notebook and it should not break down because of some missing dependendcy files.\n",
        "\n",
        "    - Data should be split up into training and validation set\n",
        "\n",
        "    - There should be a loss graph wrt to for training and validation set.\n",
        "\n",
        "    - There should be a function which accepts a `context_text` and it generates next few tokens.\n",
        "\n",
        "    - When you have trained your model, save the trained model somewhere in the cloud (github/google-drive). In the collab notebook, there should be a cell where I can just fetch the trained model from the cloud, and start generating text with it.\n",
        "\n",
        "\n",

        "\n"
      ],
      "metadata": {
        "id": "_MzXuii_KJW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "brFgfr2Bi8LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "Ko2xquJNprpP",
        "outputId": "fbee1f36-32de-4c4a-9cc2-60e926f4cfd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "1biRgaBiKI6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ6A9JY0KCIX"
      },
      "outputs": [],
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "g00AXoeOoaMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "uRHioxqbogmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "W8_arxlzojT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n"
      ],
      "metadata": {
        "id": "NZDpaPc8pJi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "LwwbrPdDpOLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ibBCp-tApRyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "GOQYq4Z0pU7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "nKt5IKWspcGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    GPT_CONFIG_124M = {\n",
        "        \"vocab_size\": 50257,     # Vocabulary size\n",
        "        \"context_length\": 256,  # Context length\n",
        "        \"emb_dim\": 768,          # Embedding dimension\n",
        "        \"n_heads\": 16,           # Number of attention heads\n",
        "        \"n_layers\": 16,          # Number of layers\n",
        "        \"drop_rate\": 0.01,        # Dropout rate\n",
        "        \"qkv_bias\": False        # Query-Key-Value bias\n",
        "    }\n",
        "\n",
        "    torch.manual_seed(146)\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.train()  # disable dropout\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgnHB7_LwH6B",
        "outputId": "9cf4de44-b050-4b11-fc22-0df5207b59f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.01, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.01, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.01, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import ssl\n",
        "\n",
        "# Create an unverified SSL context\n",
        "ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "file_path = \"combined_text.txt\"\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/1400/pg1400.txt\",\n",
        "]\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(file_path):\n",
        "    text_data = \"\"\n",
        "\n",
        "    for url in urls:\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data += response.read().decode('utf-8') + \"\\n\\n\"  # Add newline for separation\n",
        "\n",
        "    # Save to file\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dB48ORsHwUmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tiktoken\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(model=model,idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlg9zjQf6le3",
        "outputId": "80b262df-d185-42cf-ac21-d4752e8998ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves youench Zergolitics damnedERAL marking Gos enabled murders Marble\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "\n",
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride= GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "m2qJHtWM5xOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input, output in  train_loader:\n",
        "    print (input)\n",
        "    print(output)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "r3Ca-GIc5xlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562ea40a-d18c-4463-c591-a7cf2f98fda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2156, 13884,    11,   326,   996,   617,   661,   743,   869,   683,\n",
            "          6613,    11,   198,    62,    40,    62,   423,  1775,  2147,   286,\n",
            "           340,    13,   447,   251,   198,   198,   447,   250,    40,   373,\n",
            "          1239,   517,  6655,   621,   416,   465,  9172,   284,   514,    13,\n",
            "           632,   373,   517,   198, 14813,  3026,    26,   340,   373,  1107,\n",
            "         45660,    26,   290,   612,   373,   645, 14564,   329,   884,   198,\n",
            "          1078,  1463,    13,  2399, 35552,   351, 10674,   373,   845,   491,\n",
            "         38966,    13,   447,   251,   198,   198,   447,   250,  2514,   307,\n",
            "          1654,    11,   406, 40593,    11,   447,   251,   531,   607, 25949,\n",
            "            11,   564,   250,   258,   318,   407,   523, 22665,   355, 36029,\n",
            "          2763,    26,   198,   273,  2138,   339,   468,   407, 36029,  2763,\n",
            "           447,   247,    82,   954, 36368,    11,   329,   465,  3033,   389,\n",
            "           198, 25833,   306,   922,    13,   887,   703,  1625,   345,   284,\n",
            "          1560,   514,   326,   339,   373,   523,   198,  6381, 49221,   540,\n",
            "            30,   447,   251,   198,   198, 43568,  2859,  1484,  5223,   355,\n",
            "           880,   355,   673,   714,    25,   531,   326,   673,   550,  8288,\n",
            "           198, 38400,  1365,   618,   484,  1138,   287,  8758,   621,   878,\n",
            "            11,   290,   326,   673,   550,  1239,   198, 15898,   683,   523,\n",
            "         15497,   355,   428,  3329,    13,   198,   198,   447,   250,  1537,\n",
            "          3737,   339,   743,   307,   257,  1310, 38411,   605,   287,   465,\n",
            "         36317,  2410,    11,   447,   251,  8712,   198,   372,  7711,    13,\n",
            "           564,   250,  7120,  1049,  1450,  1690,   389,    26,   290,  4361,\n",
            "           314,  2236,   407,  1011,   683,   198,   265,   465,  1573,   546,\n",
            "         12478,    11,   355,   339,  1244,  1487,   465,  2000,  1194,  1110,\n",
            "            11,   290,   198, 40539,   502,   572,   465,  9384,    13,   447,\n",
            "           251,   198,   198, 43568,  2936,   326],\n",
            "        [  597, 23797,   329,   607,    13,   447,   251,   198,   198, 43568,\n",
            "          2936,  5223,  3190,  2077,   287,    13,  1375,   550,  3938,  5150,\n",
            "           852,   198,  1516,  1886,   416, 36029,  2763,   329,   883,   845,\n",
            "         38207,    26,   290,   284,   423,  1770,    13, 14006,   198, 38070,\n",
            "         28112,   372,  2107, 26061,   550,   587,  1239,  4785, 28805,    13,\n",
            "          1318,   373,   645,  1037,   198,  1640,   340,    11,  2158,    13,\n",
            "          1770,    13, 36029,  2763,   447,   247,    82, 12157,   290,   607,\n",
            "           898,   373,   583,  3174,   198, 12381, 16548,   257,  1310,  2392,\n",
            "            11,   290,  1770,    13, 14006,   447,   247,    82,  6961,  6292,\n",
            "           351,   355,   198, 11274,   257, 11542,   355,   673,   714,    13,\n",
            "          1375,   373,   407,   262,  1365, 10607,   351,   465,   198, 39580,\n",
            "         21238,    11,   422,   262,  2126,   340,  5220,   286,  1223,   517,\n",
            "            13,   632,   783,   717,   198, 19554,   694,   607,    11,   326,\n",
            "          4808,  7091,    62,   373,  6163,   422,  1871,   607, 15153,   355,\n",
            "         12733,   286,   198, 11873,   262, 37769,   286,  5900,    82,  3841,\n",
            "           350, 12613,   496,    11,   290,   286, 26508,   284,  1296,   257,\n",
            "           198,   421, 41909,  8270,  3084,   379, 10018,   654,    11,   287,\n",
            "           262,  8889,   286,   517,  8867,  9692,    13,   198,   464,  2126,\n",
            "          2582,  4251,   284, 11375,    11,   355,   673,  6515,   465,  3649,\n",
            "           198,    66,   452,  2410,  3371,  5223,    11,   290,  2982,   465,\n",
            "         10792,  2230,   379,   257,   198, 23855,  3681,   319,   607, 20868,\n",
            "           290,   410,   452,  4355,    26,   290,   996,   517, 40962,   621,\n",
            "           198,  2164,   265,  1431,  5223,   416,   428,  1245,   286,   607,\n",
            "         41700,    11,   340,   373,   407,   890,   878,   198,   372,  2802,\n",
            "          2921,   607,   284,  1833,   326,   262, 12867,   286,   511,  4845,\n",
            "           198,  9776, 32436, 49534,   284,  4808]])\n",
            "tensor([[13884,    11,   326,   996,   617,   661,   743,   869,   683,  6613,\n",
            "            11,   198,    62,    40,    62,   423,  1775,  2147,   286,   340,\n",
            "            13,   447,   251,   198,   198,   447,   250,    40,   373,  1239,\n",
            "           517,  6655,   621,   416,   465,  9172,   284,   514,    13,   632,\n",
            "           373,   517,   198, 14813,  3026,    26,   340,   373,  1107, 45660,\n",
            "            26,   290,   612,   373,   645, 14564,   329,   884,   198,  1078,\n",
            "          1463,    13,  2399, 35552,   351, 10674,   373,   845,   491, 38966,\n",
            "            13,   447,   251,   198,   198,   447,   250,  2514,   307,  1654,\n",
            "            11,   406, 40593,    11,   447,   251,   531,   607, 25949,    11,\n",
            "           564,   250,   258,   318,   407,   523, 22665,   355, 36029,  2763,\n",
            "            26,   198,   273,  2138,   339,   468,   407, 36029,  2763,   447,\n",
            "           247,    82,   954, 36368,    11,   329,   465,  3033,   389,   198,\n",
            "         25833,   306,   922,    13,   887,   703,  1625,   345,   284,  1560,\n",
            "           514,   326,   339,   373,   523,   198,  6381, 49221,   540,    30,\n",
            "           447,   251,   198,   198, 43568,  2859,  1484,  5223,   355,   880,\n",
            "           355,   673,   714,    25,   531,   326,   673,   550,  8288,   198,\n",
            "         38400,  1365,   618,   484,  1138,   287,  8758,   621,   878,    11,\n",
            "           290,   326,   673,   550,  1239,   198, 15898,   683,   523, 15497,\n",
            "           355,   428,  3329,    13,   198,   198,   447,   250,  1537,  3737,\n",
            "           339,   743,   307,   257,  1310, 38411,   605,   287,   465, 36317,\n",
            "          2410,    11,   447,   251,  8712,   198,   372,  7711,    13,   564,\n",
            "           250,  7120,  1049,  1450,  1690,   389,    26,   290,  4361,   314,\n",
            "          2236,   407,  1011,   683,   198,   265,   465,  1573,   546, 12478,\n",
            "            11,   355,   339,  1244,  1487,   465,  2000,  1194,  1110,    11,\n",
            "           290,   198, 40539,   502,   572,   465,  9384,    13,   447,   251,\n",
            "           198,   198, 43568,  2936,   326,   484],\n",
            "        [23797,   329,   607,    13,   447,   251,   198,   198, 43568,  2936,\n",
            "          5223,  3190,  2077,   287,    13,  1375,   550,  3938,  5150,   852,\n",
            "           198,  1516,  1886,   416, 36029,  2763,   329,   883,   845, 38207,\n",
            "            26,   290,   284,   423,  1770,    13, 14006,   198, 38070, 28112,\n",
            "           372,  2107, 26061,   550,   587,  1239,  4785, 28805,    13,  1318,\n",
            "           373,   645,  1037,   198,  1640,   340,    11,  2158,    13,  1770,\n",
            "            13, 36029,  2763,   447,   247,    82, 12157,   290,   607,   898,\n",
            "           373,   583,  3174,   198, 12381, 16548,   257,  1310,  2392,    11,\n",
            "           290,  1770,    13, 14006,   447,   247,    82,  6961,  6292,   351,\n",
            "           355,   198, 11274,   257, 11542,   355,   673,   714,    13,  1375,\n",
            "           373,   407,   262,  1365, 10607,   351,   465,   198, 39580, 21238,\n",
            "            11,   422,   262,  2126,   340,  5220,   286,  1223,   517,    13,\n",
            "           632,   783,   717,   198, 19554,   694,   607,    11,   326,  4808,\n",
            "          7091,    62,   373,  6163,   422,  1871,   607, 15153,   355, 12733,\n",
            "           286,   198, 11873,   262, 37769,   286,  5900,    82,  3841,   350,\n",
            "         12613,   496,    11,   290,   286, 26508,   284,  1296,   257,   198,\n",
            "           421, 41909,  8270,  3084,   379, 10018,   654,    11,   287,   262,\n",
            "          8889,   286,   517,  8867,  9692,    13,   198,   464,  2126,  2582,\n",
            "          4251,   284, 11375,    11,   355,   673,  6515,   465,  3649,   198,\n",
            "            66,   452,  2410,  3371,  5223,    11,   290,  2982,   465, 10792,\n",
            "          2230,   379,   257,   198, 23855,  3681,   319,   607, 20868,   290,\n",
            "           410,   452,  4355,    26,   290,   996,   517, 40962,   621,   198,\n",
            "          2164,   265,  1431,  5223,   416,   428,  1245,   286,   607, 41700,\n",
            "            11,   340,   373,   407,   890,   878,   198,   372,  2802,  2921,\n",
            "           607,   284,  1833,   326,   262, 12867,   286,   511,  4845,   198,\n",
            "          9776, 32436, 49534,   284,  4808,   372]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss"
      ],
      "metadata": {
        "id": "W3s7zRNJ6r-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "cfaqu1mK6uhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "c-4EjoAz6wmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "calc_loss_batch(input, output, model, device)\n"
      ],
      "metadata": {
        "id": "J_p-y8gh60CK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3db06e-c639-4bc0-cd7c-ae9a6e4109a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.9874, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)\n"
      ],
      "metadata": {
        "id": "92JjKILV60oZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203b9883-e37f-4641-96d7-9385f0bc37cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 11.00063987926515\n",
            "Validation loss: 11.005672414252098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BA5z-PsI6020"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n"
      ],
      "metadata": {
        "id": "5hj0lnYE61Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "xwh9T_mX61gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 8\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "wFHAdUZP7B_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13eb01b2-1bf7-4469-8368-d3ed7d3149cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.462, Val loss 9.395\n",
            "Ep 1 (Step 000005): Train loss 7.256, Val loss 7.051\n",
            "Ep 1 (Step 000010): Train loss 7.583, Val loss 6.946\n",
            "Ep 1 (Step 000015): Train loss 7.238, Val loss 6.698\n",
            "Ep 1 (Step 000020): Train loss 7.064, Val loss 6.515\n",
            "Ep 1 (Step 000025): Train loss 7.175, Val loss 6.501\n",
            "Ep 1 (Step 000030): Train loss 6.715, Val loss 6.350\n",
            "Ep 1 (Step 000035): Train loss 6.610, Val loss 6.312\n",
            "Ep 1 (Step 000040): Train loss 6.531, Val loss 6.244\n",
            "Ep 1 (Step 000045): Train loss 6.568, Val loss 6.143\n",
            "Ep 1 (Step 000050): Train loss 6.425, Val loss 6.100\n",
            "Ep 1 (Step 000055): Train loss 6.899, Val loss 6.006\n",
            "Ep 1 (Step 000060): Train loss 6.197, Val loss 6.057\n",
            "Ep 1 (Step 000065): Train loss 6.357, Val loss 5.996\n",
            "Ep 1 (Step 000070): Train loss 6.157, Val loss 5.967\n",
            "Ep 1 (Step 000075): Train loss 6.115, Val loss 5.895\n",
            "Ep 1 (Step 000080): Train loss 5.858, Val loss 5.850\n",
            "Ep 1 (Step 000085): Train loss 6.601, Val loss 6.075\n",
            "Ep 1 (Step 000090): Train loss 6.394, Val loss 5.945\n",
            "Ep 1 (Step 000095): Train loss 6.362, Val loss 5.821\n",
            "Ep 1 (Step 000100): Train loss 6.109, Val loss 5.776\n",
            "Ep 1 (Step 000105): Train loss 5.846, Val loss 5.760\n",
            "Ep 1 (Step 000110): Train loss 6.130, Val loss 5.836\n",
            "Ep 1 (Step 000115): Train loss 6.214, Val loss 5.829\n",
            "Ep 1 (Step 000120): Train loss 6.165, Val loss 5.783\n",
            "Ep 1 (Step 000125): Train loss 5.832, Val loss 5.734\n",
            "Ep 1 (Step 000130): Train loss 6.331, Val loss 5.870\n",
            "Ep 1 (Step 000135): Train loss 6.555, Val loss 5.743\n",
            "Ep 1 (Step 000140): Train loss 5.893, Val loss 5.708\n",
            "Ep 1 (Step 000145): Train loss 5.913, Val loss 5.680\n",
            "Ep 1 (Step 000150): Train loss 6.245, Val loss 5.699\n",
            "Ep 1 (Step 000155): Train loss 5.629, Val loss 5.703\n",
            "Ep 1 (Step 000160): Train loss 5.992, Val loss 5.763\n",
            "Ep 1 (Step 000165): Train loss 5.703, Val loss 5.729\n",
            "Ep 1 (Step 000170): Train loss 5.751, Val loss 5.734\n",
            "Ep 1 (Step 000175): Train loss 5.917, Val loss 5.680\n",
            "Ep 1 (Step 000180): Train loss 5.898, Val loss 5.651\n",
            "Ep 1 (Step 000185): Train loss 5.876, Val loss 5.601\n",
            "Ep 1 (Step 000190): Train loss 6.122, Val loss 5.592\n",
            "Ep 1 (Step 000195): Train loss 5.889, Val loss 5.581\n",
            "Ep 1 (Step 000200): Train loss 6.177, Val loss 5.535\n",
            "Ep 1 (Step 000205): Train loss 5.918, Val loss 5.541\n",
            "Ep 1 (Step 000210): Train loss 5.667, Val loss 5.541\n",
            "Ep 1 (Step 000215): Train loss 5.979, Val loss 5.513\n",
            "Ep 1 (Step 000220): Train loss 6.041, Val loss 5.545\n",
            "Ep 1 (Step 000225): Train loss 5.812, Val loss 5.525\n",
            "Ep 1 (Step 000230): Train loss 5.556, Val loss 5.473\n",
            "Ep 1 (Step 000235): Train loss 5.934, Val loss 5.468\n",
            "Ep 1 (Step 000240): Train loss 5.220, Val loss 5.445\n",
            "Ep 1 (Step 000245): Train loss 5.694, Val loss 5.490\n",
            "Ep 1 (Step 000250): Train loss 5.640, Val loss 5.509\n",
            "Ep 1 (Step 000255): Train loss 5.616, Val loss 5.441\n",
            "Ep 1 (Step 000260): Train loss 5.277, Val loss 5.472\n",
            "Ep 1 (Step 000265): Train loss 5.761, Val loss 5.473\n",
            "Ep 1 (Step 000270): Train loss 5.282, Val loss 5.462\n",
            "Ep 1 (Step 000275): Train loss 5.765, Val loss 5.465\n",
            "Ep 1 (Step 000280): Train loss 5.668, Val loss 5.472\n",
            "Ep 1 (Step 000285): Train loss 5.493, Val loss 5.460\n",
            "Ep 1 (Step 000290): Train loss 5.868, Val loss 5.479\n",
            "Ep 1 (Step 000295): Train loss 5.949, Val loss 5.436\n",
            "Ep 1 (Step 000300): Train loss 5.013, Val loss 5.421\n",
            "Ep 1 (Step 000305): Train loss 5.629, Val loss 5.432\n",
            "Ep 1 (Step 000310): Train loss 5.389, Val loss 5.424\n",
            "Ep 1 (Step 000315): Train loss 5.583, Val loss 5.373\n",
            "Ep 1 (Step 000320): Train loss 5.592, Val loss 5.391\n",
            "Ep 1 (Step 000325): Train loss 5.514, Val loss 5.377\n",
            "Ep 1 (Step 000330): Train loss 5.486, Val loss 5.312\n",
            "Ep 1 (Step 000335): Train loss 5.686, Val loss 5.289\n",
            "Ep 1 (Step 000340): Train loss 5.123, Val loss 5.311\n",
            "Ep 1 (Step 000345): Train loss 5.537, Val loss 5.318\n",
            "Ep 1 (Step 000350): Train loss 5.315, Val loss 5.288\n",
            "Ep 1 (Step 000355): Train loss 5.611, Val loss 5.281\n",
            "Ep 1 (Step 000360): Train loss 5.450, Val loss 5.288\n",
            "Ep 1 (Step 000365): Train loss 6.249, Val loss 5.256\n",
            "Ep 1 (Step 000370): Train loss 5.399, Val loss 5.272\n",
            "Ep 1 (Step 000375): Train loss 5.558, Val loss 5.270\n",
            "Ep 1 (Step 000380): Train loss 5.764, Val loss 5.267\n",
            "Ep 1 (Step 000385): Train loss 5.078, Val loss 5.281\n",
            "Ep 1 (Step 000390): Train loss 5.471, Val loss 5.190\n",
            "Ep 1 (Step 000395): Train loss 5.486, Val loss 5.190\n",
            "Ep 1 (Step 000400): Train loss 5.631, Val loss 5.170\n",
            "Ep 1 (Step 000405): Train loss 5.457, Val loss 5.163\n",
            "Ep 1 (Step 000410): Train loss 5.308, Val loss 5.160\n",
            "Ep 1 (Step 000415): Train loss 5.276, Val loss 5.188\n",
            "Ep 1 (Step 000420): Train loss 5.622, Val loss 5.199\n",
            "Ep 1 (Step 000425): Train loss 5.661, Val loss 5.169\n",
            "Ep 1 (Step 000430): Train loss 5.241, Val loss 5.159\n",
            "Ep 1 (Step 000435): Train loss 5.452, Val loss 5.135\n",
            "Ep 1 (Step 000440): Train loss 5.776, Val loss 5.113\n",
            "Ep 1 (Step 000445): Train loss 5.290, Val loss 5.085\n",
            "Ep 1 (Step 000450): Train loss 5.588, Val loss 5.104\n",
            "Ep 1 (Step 000455): Train loss 5.377, Val loss 5.093\n",
            "Ep 1 (Step 000460): Train loss 4.950, Val loss 5.085\n",
            "Ep 1 (Step 000465): Train loss 4.988, Val loss 5.109\n",
            "Ep 1 (Step 000470): Train loss 5.418, Val loss 5.127\n",
            "Ep 1 (Step 000475): Train loss 5.432, Val loss 5.078\n",
            "Ep 1 (Step 000480): Train loss 5.335, Val loss 5.089\n",
            "Ep 1 (Step 000485): Train loss 5.470, Val loss 5.070\n",
            "Ep 1 (Step 000490): Train loss 4.990, Val loss 5.063\n",
            "Ep 1 (Step 000495): Train loss 5.376, Val loss 5.089\n",
            "Ep 1 (Step 000500): Train loss 5.349, Val loss 5.075\n",
            "Ep 1 (Step 000505): Train loss 5.200, Val loss 5.068\n",
            "Ep 1 (Step 000510): Train loss 5.322, Val loss 5.048\n",
            "Ep 1 (Step 000515): Train loss 5.069, Val loss 5.046\n",
            "Ep 1 (Step 000520): Train loss 5.440, Val loss 5.065\n",
            "Ep 1 (Step 000525): Train loss 5.539, Val loss 5.036\n",
            "Ep 1 (Step 000530): Train loss 5.678, Val loss 5.031\n",
            "Ep 1 (Step 000535): Train loss 5.041, Val loss 5.032\n",
            "Ep 1 (Step 000540): Train loss 5.380, Val loss 4.997\n",
            "Ep 1 (Step 000545): Train loss 4.887, Val loss 4.995\n",
            "Ep 1 (Step 000550): Train loss 5.259, Val loss 5.007\n",
            "Ep 1 (Step 000555): Train loss 4.972, Val loss 4.991\n",
            "Ep 1 (Step 000560): Train loss 5.311, Val loss 4.990\n",
            "Ep 1 (Step 000565): Train loss 5.398, Val loss 5.004\n",
            "Ep 1 (Step 000570): Train loss 4.959, Val loss 4.999\n",
            "Ep 1 (Step 000575): Train loss 5.418, Val loss 5.010\n",
            "Ep 1 (Step 000580): Train loss 5.333, Val loss 4.986\n",
            "Ep 1 (Step 000585): Train loss 5.238, Val loss 4.995\n",
            "Ep 1 (Step 000590): Train loss 5.364, Val loss 4.999\n",
            "Ep 1 (Step 000595): Train loss 5.092, Val loss 5.012\n",
            "Ep 1 (Step 000600): Train loss 5.166, Val loss 4.990\n",
            "Ep 1 (Step 000605): Train loss 5.127, Val loss 4.975\n",
            "Ep 1 (Step 000610): Train loss 5.194, Val loss 4.966\n",
            "Ep 1 (Step 000615): Train loss 5.130, Val loss 4.948\n",
            "Ep 1 (Step 000620): Train loss 4.908, Val loss 4.922\n",
            "Ep 1 (Step 000625): Train loss 5.062, Val loss 4.895\n",
            "Ep 1 (Step 000630): Train loss 5.690, Val loss 4.926\n",
            "Ep 1 (Step 000635): Train loss 5.170, Val loss 4.906\n",
            "Ep 1 (Step 000640): Train loss 5.361, Val loss 4.910\n",
            "Ep 1 (Step 000645): Train loss 4.962, Val loss 4.905\n",
            "Ep 1 (Step 000650): Train loss 5.276, Val loss 4.955\n",
            "Ep 1 (Step 000655): Train loss 4.664, Val loss 4.885\n",
            "Ep 1 (Step 000660): Train loss 4.691, Val loss 4.891\n",
            "Ep 1 (Step 000665): Train loss 5.251, Val loss 4.931\n",
            "Ep 1 (Step 000670): Train loss 5.450, Val loss 4.938\n",
            "Ep 1 (Step 000675): Train loss 5.392, Val loss 4.948\n",
            "Ep 1 (Step 000680): Train loss 5.035, Val loss 4.944\n",
            "Ep 1 (Step 000685): Train loss 4.824, Val loss 4.953\n",
            "Ep 1 (Step 000690): Train loss 5.094, Val loss 4.986\n",
            "Ep 1 (Step 000695): Train loss 5.054, Val loss 5.013\n",
            "Ep 1 (Step 000700): Train loss 5.216, Val loss 4.941\n",
            "Ep 1 (Step 000705): Train loss 5.053, Val loss 4.943\n",
            "Ep 1 (Step 000710): Train loss 5.195, Val loss 4.938\n",
            "Ep 1 (Step 000715): Train loss 5.132, Val loss 4.858\n",
            "Ep 1 (Step 000720): Train loss 5.147, Val loss 4.864\n",
            "Ep 1 (Step 000725): Train loss 5.190, Val loss 4.867\n",
            "Ep 1 (Step 000730): Train loss 5.143, Val loss 4.868\n",
            "Ep 1 (Step 000735): Train loss 5.097, Val loss 4.863\n",
            "Ep 1 (Step 000740): Train loss 5.147, Val loss 4.890\n",
            "Ep 1 (Step 000745): Train loss 4.709, Val loss 4.868\n",
            "Ep 1 (Step 000750): Train loss 5.204, Val loss 4.861\n",
            "Ep 1 (Step 000755): Train loss 4.957, Val loss 4.858\n",
            "Ep 1 (Step 000760): Train loss 5.172, Val loss 4.849\n",
            "Ep 1 (Step 000765): Train loss 5.107, Val loss 4.817\n",
            "Ep 1 (Step 000770): Train loss 5.021, Val loss 4.820\n",
            "Ep 1 (Step 000775): Train loss 5.167, Val loss 4.798\n",
            "Ep 1 (Step 000780): Train loss 5.236, Val loss 4.814\n",
            "Ep 1 (Step 000785): Train loss 5.358, Val loss 4.799\n",
            "Ep 1 (Step 000790): Train loss 4.942, Val loss 4.816\n",
            "Ep 1 (Step 000795): Train loss 5.271, Val loss 4.832\n",
            "Ep 1 (Step 000800): Train loss 4.984, Val loss 4.859\n",
            "Ep 1 (Step 000805): Train loss 4.996, Val loss 4.826\n",
            "Ep 1 (Step 000810): Train loss 5.229, Val loss 4.821\n",
            "Ep 1 (Step 000815): Train loss 4.859, Val loss 4.821\n",
            "Ep 1 (Step 000820): Train loss 5.276, Val loss 4.800\n",
            "Ep 1 (Step 000825): Train loss 5.522, Val loss 4.800\n",
            "Ep 1 (Step 000830): Train loss 5.056, Val loss 4.798\n",
            "Ep 1 (Step 000835): Train loss 5.161, Val loss 4.865\n",
            "Ep 1 (Step 000840): Train loss 5.356, Val loss 4.860\n",
            "Every effort moves you, and                                                \n",
            "Ep 2 (Step 000845): Train loss 4.877, Val loss 4.838\n",
            "Ep 2 (Step 000850): Train loss 4.929, Val loss 4.830\n",
            "Ep 2 (Step 000855): Train loss 4.819, Val loss 4.829\n",
            "Ep 2 (Step 000860): Train loss 5.207, Val loss 4.824\n",
            "Ep 2 (Step 000865): Train loss 5.101, Val loss 4.832\n",
            "Ep 2 (Step 000870): Train loss 5.098, Val loss 4.810\n",
            "Ep 2 (Step 000875): Train loss 5.340, Val loss 4.798\n",
            "Ep 2 (Step 000880): Train loss 5.069, Val loss 4.799\n",
            "Ep 2 (Step 000885): Train loss 4.978, Val loss 4.803\n",
            "Ep 2 (Step 000890): Train loss 5.091, Val loss 4.809\n",
            "Ep 2 (Step 000895): Train loss 5.118, Val loss 4.817\n",
            "Ep 2 (Step 000900): Train loss 4.965, Val loss 4.786\n",
            "Ep 2 (Step 000905): Train loss 4.982, Val loss 4.781\n",
            "Ep 2 (Step 000910): Train loss 5.049, Val loss 4.781\n",
            "Ep 2 (Step 000915): Train loss 5.082, Val loss 4.807\n",
            "Ep 2 (Step 000920): Train loss 4.479, Val loss 4.778\n",
            "Ep 2 (Step 000925): Train loss 5.125, Val loss 4.783\n",
            "Ep 2 (Step 000930): Train loss 4.966, Val loss 4.778\n",
            "Ep 2 (Step 000935): Train loss 5.003, Val loss 4.775\n",
            "Ep 2 (Step 000940): Train loss 4.908, Val loss 4.776\n",
            "Ep 2 (Step 000945): Train loss 5.065, Val loss 4.749\n",
            "Ep 2 (Step 000950): Train loss 4.855, Val loss 4.782\n",
            "Ep 2 (Step 000955): Train loss 4.668, Val loss 4.793\n",
            "Ep 2 (Step 000960): Train loss 5.284, Val loss 4.809\n",
            "Ep 2 (Step 000965): Train loss 4.904, Val loss 4.820\n",
            "Ep 2 (Step 000970): Train loss 5.050, Val loss 4.800\n",
            "Ep 2 (Step 000975): Train loss 5.021, Val loss 4.837\n",
            "Ep 2 (Step 000980): Train loss 5.191, Val loss 4.820\n",
            "Ep 2 (Step 000985): Train loss 5.220, Val loss 4.784\n",
            "Ep 2 (Step 000990): Train loss 4.588, Val loss 4.758\n",
            "Ep 2 (Step 000995): Train loss 4.949, Val loss 4.776\n",
            "Ep 2 (Step 001000): Train loss 4.869, Val loss 4.780\n",
            "Ep 2 (Step 001005): Train loss 5.055, Val loss 4.770\n",
            "Ep 2 (Step 001010): Train loss 5.015, Val loss 4.738\n",
            "Ep 2 (Step 001015): Train loss 5.148, Val loss 4.730\n",
            "Ep 2 (Step 001020): Train loss 4.943, Val loss 4.741\n",
            "Ep 2 (Step 001025): Train loss 5.177, Val loss 4.782\n",
            "Ep 2 (Step 001030): Train loss 4.383, Val loss 4.747\n",
            "Ep 2 (Step 001035): Train loss 4.934, Val loss 4.747\n",
            "Ep 2 (Step 001040): Train loss 4.853, Val loss 4.772\n",
            "Ep 2 (Step 001045): Train loss 5.081, Val loss 4.778\n",
            "Ep 2 (Step 001050): Train loss 5.152, Val loss 4.805\n",
            "Ep 2 (Step 001055): Train loss 4.995, Val loss 4.801\n",
            "Ep 2 (Step 001060): Train loss 5.047, Val loss 4.748\n",
            "Ep 2 (Step 001065): Train loss 4.850, Val loss 4.738\n",
            "Ep 2 (Step 001070): Train loss 5.085, Val loss 4.766\n",
            "Ep 2 (Step 001075): Train loss 5.087, Val loss 4.735\n",
            "Ep 2 (Step 001080): Train loss 5.181, Val loss 4.737\n",
            "Ep 2 (Step 001085): Train loss 5.139, Val loss 4.694\n",
            "Ep 2 (Step 001090): Train loss 4.779, Val loss 4.726\n",
            "Ep 2 (Step 001095): Train loss 4.944, Val loss 4.715\n",
            "Ep 2 (Step 001100): Train loss 4.890, Val loss 4.754\n",
            "Ep 2 (Step 001105): Train loss 5.018, Val loss 4.749\n",
            "Ep 2 (Step 001110): Train loss 4.958, Val loss 4.771\n",
            "Ep 2 (Step 001115): Train loss 4.871, Val loss 4.783\n",
            "Ep 2 (Step 001120): Train loss 4.774, Val loss 4.776\n",
            "Ep 2 (Step 001125): Train loss 4.942, Val loss 4.775\n",
            "Ep 2 (Step 001130): Train loss 5.335, Val loss 4.793\n",
            "Ep 2 (Step 001135): Train loss 4.957, Val loss 4.762\n",
            "Ep 2 (Step 001140): Train loss 4.864, Val loss 4.737\n",
            "Ep 2 (Step 001145): Train loss 4.988, Val loss 4.727\n",
            "Ep 2 (Step 001150): Train loss 5.137, Val loss 4.732\n",
            "Ep 2 (Step 001155): Train loss 5.024, Val loss 4.738\n",
            "Ep 2 (Step 001160): Train loss 4.780, Val loss 4.726\n",
            "Ep 2 (Step 001165): Train loss 4.951, Val loss 4.732\n",
            "Ep 2 (Step 001170): Train loss 4.690, Val loss 4.767\n",
            "Ep 2 (Step 001175): Train loss 5.048, Val loss 4.732\n",
            "Ep 2 (Step 001180): Train loss 5.052, Val loss 4.720\n",
            "Ep 2 (Step 001185): Train loss 4.912, Val loss 4.709\n",
            "Ep 2 (Step 001190): Train loss 4.935, Val loss 4.715\n",
            "Ep 2 (Step 001195): Train loss 4.974, Val loss 4.738\n",
            "Ep 2 (Step 001200): Train loss 5.236, Val loss 4.768\n",
            "Ep 2 (Step 001205): Train loss 4.979, Val loss 4.739\n",
            "Ep 2 (Step 001210): Train loss 4.927, Val loss 4.721\n",
            "Ep 2 (Step 001215): Train loss 4.948, Val loss 4.741\n",
            "Ep 2 (Step 001220): Train loss 4.893, Val loss 4.709\n",
            "Ep 2 (Step 001225): Train loss 4.868, Val loss 4.696\n",
            "Ep 2 (Step 001230): Train loss 5.184, Val loss 4.713\n",
            "Ep 2 (Step 001235): Train loss 4.915, Val loss 4.699\n",
            "Ep 2 (Step 001240): Train loss 4.641, Val loss 4.687\n",
            "Ep 2 (Step 001245): Train loss 4.901, Val loss 4.702\n",
            "Ep 2 (Step 001250): Train loss 5.091, Val loss 4.692\n",
            "Ep 2 (Step 001255): Train loss 4.880, Val loss 4.686\n",
            "Ep 2 (Step 001260): Train loss 4.967, Val loss 4.668\n",
            "Ep 2 (Step 001265): Train loss 5.055, Val loss 4.664\n",
            "Ep 2 (Step 001270): Train loss 5.070, Val loss 4.664\n",
            "Ep 2 (Step 001275): Train loss 4.481, Val loss 4.670\n",
            "Ep 2 (Step 001280): Train loss 5.329, Val loss 4.670\n",
            "Ep 2 (Step 001285): Train loss 5.047, Val loss 4.676\n",
            "Ep 2 (Step 001290): Train loss 5.069, Val loss 4.653\n",
            "Ep 2 (Step 001295): Train loss 5.012, Val loss 4.661\n",
            "Ep 2 (Step 001300): Train loss 4.958, Val loss 4.649\n",
            "Ep 2 (Step 001305): Train loss 4.873, Val loss 4.653\n",
            "Ep 2 (Step 001310): Train loss 4.868, Val loss 4.670\n",
            "Ep 2 (Step 001315): Train loss 4.771, Val loss 4.664\n",
            "Ep 2 (Step 001320): Train loss 5.031, Val loss 4.669\n",
            "Ep 2 (Step 001325): Train loss 4.805, Val loss 4.682\n",
            "Ep 2 (Step 001330): Train loss 4.965, Val loss 4.682\n",
            "Ep 2 (Step 001335): Train loss 4.811, Val loss 4.664\n",
            "Ep 2 (Step 001340): Train loss 4.809, Val loss 4.664\n",
            "Ep 2 (Step 001345): Train loss 4.587, Val loss 4.658\n",
            "Ep 2 (Step 001350): Train loss 4.801, Val loss 4.653\n",
            "Ep 2 (Step 001355): Train loss 4.856, Val loss 4.661\n",
            "Ep 2 (Step 001360): Train loss 4.912, Val loss 4.677\n",
            "Ep 2 (Step 001365): Train loss 4.827, Val loss 4.675\n",
            "Ep 2 (Step 001370): Train loss 4.887, Val loss 4.656\n",
            "Ep 2 (Step 001375): Train loss 4.927, Val loss 4.638\n",
            "Ep 2 (Step 001380): Train loss 4.548, Val loss 4.665\n",
            "Ep 2 (Step 001385): Train loss 4.924, Val loss 4.657\n",
            "Ep 2 (Step 001390): Train loss 4.960, Val loss 4.688\n",
            "Ep 2 (Step 001395): Train loss 5.052, Val loss 4.687\n",
            "Ep 2 (Step 001400): Train loss 4.873, Val loss 4.688\n",
            "Ep 2 (Step 001405): Train loss 4.808, Val loss 4.699\n",
            "Ep 2 (Step 001410): Train loss 4.876, Val loss 4.719\n",
            "Ep 2 (Step 001415): Train loss 4.979, Val loss 4.723\n",
            "Ep 2 (Step 001420): Train loss 4.831, Val loss 4.718\n",
            "Ep 2 (Step 001425): Train loss 5.162, Val loss 4.729\n",
            "Ep 2 (Step 001430): Train loss 4.512, Val loss 4.708\n",
            "Ep 2 (Step 001435): Train loss 4.897, Val loss 4.685\n",
            "Ep 2 (Step 001440): Train loss 4.632, Val loss 4.681\n",
            "Ep 2 (Step 001445): Train loss 4.958, Val loss 4.661\n",
            "Ep 2 (Step 001450): Train loss 4.698, Val loss 4.673\n",
            "Ep 2 (Step 001455): Train loss 4.292, Val loss 4.712\n",
            "Ep 2 (Step 001460): Train loss 4.804, Val loss 4.670\n",
            "Ep 2 (Step 001465): Train loss 5.009, Val loss 4.655\n",
            "Ep 2 (Step 001470): Train loss 4.467, Val loss 4.637\n",
            "Ep 2 (Step 001475): Train loss 5.003, Val loss 4.638\n",
            "Ep 2 (Step 001480): Train loss 5.139, Val loss 4.637\n",
            "Ep 2 (Step 001485): Train loss 4.920, Val loss 4.630\n",
            "Ep 2 (Step 001490): Train loss 5.010, Val loss 4.629\n",
            "Ep 2 (Step 001495): Train loss 4.645, Val loss 4.650\n",
            "Ep 2 (Step 001500): Train loss 4.947, Val loss 4.678\n",
            "Ep 2 (Step 001505): Train loss 4.344, Val loss 4.643\n",
            "Ep 2 (Step 001510): Train loss 4.905, Val loss 4.621\n",
            "Ep 2 (Step 001515): Train loss 5.018, Val loss 4.692\n",
            "Ep 2 (Step 001520): Train loss 5.049, Val loss 4.666\n",
            "Ep 2 (Step 001525): Train loss 5.118, Val loss 4.665\n",
            "Ep 2 (Step 001530): Train loss 4.811, Val loss 4.686\n",
            "Ep 2 (Step 001535): Train loss 4.869, Val loss 4.695\n",
            "Ep 2 (Step 001540): Train loss 4.740, Val loss 4.688\n",
            "Ep 2 (Step 001545): Train loss 4.664, Val loss 4.649\n",
            "Ep 2 (Step 001550): Train loss 4.477, Val loss 4.652\n",
            "Ep 2 (Step 001555): Train loss 4.814, Val loss 4.646\n",
            "Ep 2 (Step 001560): Train loss 4.601, Val loss 4.629\n",
            "Ep 2 (Step 001565): Train loss 4.497, Val loss 4.652\n",
            "Ep 2 (Step 001570): Train loss 4.886, Val loss 4.633\n",
            "Ep 2 (Step 001575): Train loss 5.065, Val loss 4.664\n",
            "Ep 2 (Step 001580): Train loss 4.606, Val loss 4.658\n",
            "Ep 2 (Step 001585): Train loss 5.010, Val loss 4.643\n",
            "Ep 2 (Step 001590): Train loss 4.665, Val loss 4.653\n",
            "Ep 2 (Step 001595): Train loss 4.903, Val loss 4.645\n",
            "Ep 2 (Step 001600): Train loss 4.785, Val loss 4.653\n",
            "Ep 2 (Step 001605): Train loss 4.709, Val loss 4.683\n",
            "Ep 2 (Step 001610): Train loss 4.712, Val loss 4.685\n",
            "Ep 2 (Step 001615): Train loss 4.472, Val loss 4.685\n",
            "Ep 2 (Step 001620): Train loss 4.322, Val loss 4.697\n",
            "Ep 2 (Step 001625): Train loss 4.808, Val loss 4.676\n",
            "Ep 2 (Step 001630): Train loss 4.782, Val loss 4.663\n",
            "Ep 2 (Step 001635): Train loss 4.836, Val loss 4.674\n",
            "Ep 2 (Step 001640): Train loss 4.773, Val loss 4.659\n",
            "Ep 2 (Step 001645): Train loss 4.888, Val loss 4.682\n",
            "Ep 2 (Step 001650): Train loss 4.867, Val loss 4.703\n",
            "Ep 2 (Step 001655): Train loss 4.875, Val loss 4.686\n",
            "Ep 2 (Step 001660): Train loss 4.755, Val loss 4.678\n",
            "Ep 2 (Step 001665): Train loss 4.477, Val loss 4.679\n",
            "Ep 2 (Step 001670): Train loss 4.729, Val loss 4.667\n",
            "Ep 2 (Step 001675): Train loss 4.935, Val loss 4.666\n",
            "Ep 2 (Step 001680): Train loss 4.940, Val loss 4.675\n",
            "Ep 2 (Step 001685): Train loss 5.296, Val loss 4.669\n",
            "Every effort moves you                                                \n",
            "Ep 3 (Step 001690): Train loss 4.305, Val loss 4.667\n",
            "Ep 3 (Step 001695): Train loss 4.541, Val loss 4.661\n",
            "Ep 3 (Step 001700): Train loss 4.972, Val loss 4.665\n",
            "Ep 3 (Step 001705): Train loss 4.687, Val loss 4.672\n",
            "Ep 3 (Step 001710): Train loss 4.709, Val loss 4.659\n",
            "Ep 3 (Step 001715): Train loss 4.834, Val loss 4.654\n",
            "Ep 3 (Step 001720): Train loss 4.570, Val loss 4.648\n",
            "Ep 3 (Step 001725): Train loss 4.868, Val loss 4.653\n",
            "Ep 3 (Step 001730): Train loss 5.026, Val loss 4.650\n",
            "Ep 3 (Step 001735): Train loss 4.789, Val loss 4.690\n",
            "Ep 3 (Step 001740): Train loss 5.348, Val loss 4.680\n",
            "Ep 3 (Step 001745): Train loss 4.674, Val loss 4.675\n",
            "Ep 3 (Step 001750): Train loss 4.948, Val loss 4.657\n",
            "Ep 3 (Step 001755): Train loss 4.813, Val loss 4.640\n",
            "Ep 3 (Step 001760): Train loss 4.854, Val loss 4.640\n",
            "Ep 3 (Step 001765): Train loss 4.668, Val loss 4.667\n",
            "Ep 3 (Step 001770): Train loss 4.882, Val loss 4.660\n",
            "Ep 3 (Step 001775): Train loss 4.725, Val loss 4.660\n",
            "Ep 3 (Step 001780): Train loss 4.460, Val loss 4.654\n",
            "Ep 3 (Step 001785): Train loss 4.733, Val loss 4.629\n",
            "Ep 3 (Step 001790): Train loss 4.773, Val loss 4.620\n",
            "Ep 3 (Step 001795): Train loss 5.056, Val loss 4.620\n",
            "Ep 3 (Step 001800): Train loss 4.732, Val loss 4.643\n",
            "Ep 3 (Step 001805): Train loss 4.648, Val loss 4.647\n",
            "Ep 3 (Step 001810): Train loss 5.019, Val loss 4.625\n",
            "Ep 3 (Step 001815): Train loss 4.426, Val loss 4.640\n",
            "Ep 3 (Step 001820): Train loss 4.314, Val loss 4.683\n",
            "Ep 3 (Step 001825): Train loss 4.961, Val loss 4.656\n",
            "Ep 3 (Step 001830): Train loss 4.731, Val loss 4.642\n",
            "Ep 3 (Step 001835): Train loss 4.903, Val loss 4.635\n",
            "Ep 3 (Step 001840): Train loss 4.665, Val loss 4.643\n",
            "Ep 3 (Step 001845): Train loss 4.762, Val loss 4.661\n",
            "Ep 3 (Step 001850): Train loss 4.272, Val loss 4.658\n",
            "Ep 3 (Step 001855): Train loss 5.067, Val loss 4.616\n",
            "Ep 3 (Step 001860): Train loss 4.567, Val loss 4.609\n",
            "Ep 3 (Step 001865): Train loss 4.763, Val loss 4.622\n",
            "Ep 3 (Step 001870): Train loss 4.624, Val loss 4.621\n",
            "Ep 3 (Step 001875): Train loss 4.682, Val loss 4.618\n",
            "Ep 3 (Step 001880): Train loss 4.858, Val loss 4.639\n",
            "Ep 3 (Step 001885): Train loss 4.569, Val loss 4.636\n",
            "Ep 3 (Step 001890): Train loss 4.594, Val loss 4.631\n",
            "Ep 3 (Step 001895): Train loss 4.826, Val loss 4.627\n",
            "Ep 3 (Step 001900): Train loss 4.784, Val loss 4.620\n",
            "Ep 3 (Step 001905): Train loss 4.568, Val loss 4.620\n",
            "Ep 3 (Step 001910): Train loss 4.818, Val loss 4.615\n",
            "Ep 3 (Step 001915): Train loss 4.558, Val loss 4.614\n",
            "Ep 3 (Step 001920): Train loss 4.684, Val loss 4.627\n",
            "Ep 3 (Step 001925): Train loss 5.046, Val loss 4.640\n",
            "Ep 3 (Step 001930): Train loss 4.812, Val loss 4.650\n",
            "Ep 3 (Step 001935): Train loss 4.696, Val loss 4.647\n",
            "Ep 3 (Step 001940): Train loss 4.721, Val loss 4.655\n",
            "Ep 3 (Step 001945): Train loss 4.507, Val loss 4.653\n",
            "Ep 3 (Step 001950): Train loss 4.541, Val loss 4.657\n",
            "Ep 3 (Step 001955): Train loss 4.956, Val loss 4.656\n",
            "Ep 3 (Step 001960): Train loss 4.891, Val loss 4.644\n",
            "Ep 3 (Step 001965): Train loss 4.875, Val loss 4.632\n",
            "Ep 3 (Step 001970): Train loss 4.853, Val loss 4.636\n",
            "Ep 3 (Step 001975): Train loss 4.669, Val loss 4.620\n",
            "Ep 3 (Step 001980): Train loss 4.658, Val loss 4.608\n",
            "Ep 3 (Step 001985): Train loss 4.746, Val loss 4.620\n",
            "Ep 3 (Step 001990): Train loss 4.852, Val loss 4.632\n",
            "Ep 3 (Step 001995): Train loss 4.584, Val loss 4.655\n",
            "Ep 3 (Step 002000): Train loss 4.851, Val loss 4.639\n",
            "Ep 3 (Step 002005): Train loss 4.983, Val loss 4.646\n",
            "Ep 3 (Step 002010): Train loss 4.835, Val loss 4.676\n",
            "Ep 3 (Step 002015): Train loss 4.662, Val loss 4.673\n",
            "Ep 3 (Step 002020): Train loss 4.706, Val loss 4.644\n",
            "Ep 3 (Step 002025): Train loss 4.977, Val loss 4.636\n",
            "Ep 3 (Step 002030): Train loss 4.741, Val loss 4.665\n",
            "Ep 3 (Step 002035): Train loss 4.916, Val loss 4.655\n",
            "Ep 3 (Step 002040): Train loss 5.043, Val loss 4.649\n",
            "Ep 3 (Step 002045): Train loss 4.803, Val loss 4.637\n",
            "Ep 3 (Step 002050): Train loss 4.933, Val loss 4.641\n",
            "Ep 3 (Step 002055): Train loss 4.950, Val loss 4.643\n",
            "Ep 3 (Step 002060): Train loss 4.823, Val loss 4.645\n",
            "Ep 3 (Step 002065): Train loss 4.730, Val loss 4.649\n",
            "Ep 3 (Step 002070): Train loss 4.676, Val loss 4.632\n",
            "Ep 3 (Step 002075): Train loss 4.576, Val loss 4.637\n",
            "Ep 3 (Step 002080): Train loss 4.817, Val loss 4.631\n",
            "Ep 3 (Step 002085): Train loss 5.195, Val loss 4.653\n",
            "Ep 3 (Step 002090): Train loss 4.721, Val loss 4.629\n",
            "Ep 3 (Step 002095): Train loss 4.714, Val loss 4.626\n",
            "Ep 3 (Step 002100): Train loss 4.696, Val loss 4.617\n",
            "Ep 3 (Step 002105): Train loss 4.724, Val loss 4.601\n",
            "Ep 3 (Step 002110): Train loss 4.932, Val loss 4.637\n",
            "Ep 3 (Step 002115): Train loss 5.063, Val loss 4.626\n",
            "Ep 3 (Step 002120): Train loss 5.018, Val loss 4.654\n",
            "Ep 3 (Step 002125): Train loss 4.926, Val loss 4.632\n",
            "Ep 3 (Step 002130): Train loss 4.984, Val loss 4.647\n",
            "Ep 3 (Step 002135): Train loss 4.552, Val loss 4.656\n",
            "Ep 3 (Step 002140): Train loss 5.268, Val loss 4.653\n",
            "Ep 3 (Step 002145): Train loss 5.012, Val loss 4.638\n",
            "Ep 3 (Step 002150): Train loss 4.657, Val loss 4.679\n",
            "Ep 3 (Step 002155): Train loss 4.641, Val loss 4.661\n",
            "Ep 3 (Step 002160): Train loss 4.754, Val loss 4.668\n",
            "Ep 3 (Step 002165): Train loss 5.013, Val loss 4.659\n",
            "Ep 3 (Step 002170): Train loss 4.588, Val loss 4.674\n",
            "Ep 3 (Step 002175): Train loss 4.649, Val loss 4.648\n",
            "Ep 3 (Step 002180): Train loss 4.779, Val loss 4.649\n",
            "Ep 3 (Step 002185): Train loss 4.724, Val loss 4.619\n",
            "Ep 3 (Step 002190): Train loss 4.591, Val loss 4.641\n",
            "Ep 3 (Step 002195): Train loss 4.611, Val loss 4.647\n",
            "Ep 3 (Step 002200): Train loss 4.909, Val loss 4.644\n",
            "Ep 3 (Step 002205): Train loss 4.615, Val loss 4.637\n",
            "Ep 3 (Step 002210): Train loss 4.685, Val loss 4.647\n",
            "Ep 3 (Step 002215): Train loss 5.038, Val loss 4.649\n",
            "Ep 3 (Step 002220): Train loss 4.812, Val loss 4.665\n",
            "Ep 3 (Step 002225): Train loss 4.470, Val loss 4.656\n",
            "Ep 3 (Step 002230): Train loss 4.815, Val loss 4.626\n",
            "Ep 3 (Step 002235): Train loss 4.604, Val loss 4.633\n",
            "Ep 3 (Step 002240): Train loss 4.987, Val loss 4.639\n",
            "Ep 3 (Step 002245): Train loss 4.670, Val loss 4.658\n",
            "Ep 3 (Step 002250): Train loss 4.830, Val loss 4.633\n",
            "Ep 3 (Step 002255): Train loss 4.671, Val loss 4.605\n",
            "Ep 3 (Step 002260): Train loss 4.845, Val loss 4.603\n",
            "Ep 3 (Step 002265): Train loss 4.740, Val loss 4.597\n",
            "Ep 3 (Step 002270): Train loss 4.756, Val loss 4.624\n",
            "Ep 3 (Step 002275): Train loss 4.784, Val loss 4.628\n",
            "Ep 3 (Step 002280): Train loss 4.691, Val loss 4.636\n",
            "Ep 3 (Step 002285): Train loss 4.288, Val loss 4.627\n",
            "Ep 3 (Step 002290): Train loss 4.724, Val loss 4.606\n",
            "Ep 3 (Step 002295): Train loss 4.801, Val loss 4.590\n",
            "Ep 3 (Step 002300): Train loss 4.762, Val loss 4.578\n",
            "Ep 3 (Step 002305): Train loss 4.850, Val loss 4.583\n",
            "Ep 3 (Step 002310): Train loss 4.808, Val loss 4.595\n",
            "Ep 3 (Step 002315): Train loss 4.353, Val loss 4.600\n",
            "Ep 3 (Step 002320): Train loss 4.305, Val loss 4.598\n",
            "Ep 3 (Step 002325): Train loss 4.855, Val loss 4.601\n",
            "Ep 3 (Step 002330): Train loss 4.551, Val loss 4.604\n",
            "Ep 3 (Step 002335): Train loss 4.730, Val loss 4.621\n",
            "Ep 3 (Step 002340): Train loss 4.795, Val loss 4.616\n",
            "Ep 3 (Step 002345): Train loss 4.603, Val loss 4.604\n",
            "Ep 3 (Step 002350): Train loss 4.688, Val loss 4.592\n",
            "Ep 3 (Step 002355): Train loss 4.710, Val loss 4.594\n",
            "Ep 3 (Step 002360): Train loss 4.073, Val loss 4.586\n",
            "Ep 3 (Step 002365): Train loss 4.480, Val loss 4.575\n",
            "Ep 3 (Step 002370): Train loss 4.729, Val loss 4.570\n",
            "Ep 3 (Step 002375): Train loss 5.003, Val loss 4.576\n",
            "Ep 3 (Step 002380): Train loss 4.558, Val loss 4.587\n",
            "Ep 3 (Step 002385): Train loss 4.465, Val loss 4.622\n",
            "Ep 3 (Step 002390): Train loss 4.652, Val loss 4.622\n",
            "Ep 3 (Step 002395): Train loss 4.730, Val loss 4.609\n",
            "Ep 3 (Step 002400): Train loss 4.887, Val loss 4.613\n",
            "Ep 3 (Step 002405): Train loss 4.353, Val loss 4.623\n",
            "Ep 3 (Step 002410): Train loss 4.668, Val loss 4.632\n",
            "Ep 3 (Step 002415): Train loss 4.733, Val loss 4.645\n",
            "Ep 3 (Step 002420): Train loss 4.869, Val loss 4.644\n",
            "Ep 3 (Step 002425): Train loss 4.926, Val loss 4.655\n",
            "Ep 3 (Step 002430): Train loss 4.601, Val loss 4.622\n",
            "Ep 3 (Step 002435): Train loss 4.632, Val loss 4.643\n",
            "Ep 3 (Step 002440): Train loss 4.476, Val loss 4.624\n",
            "Ep 3 (Step 002445): Train loss 4.897, Val loss 4.638\n",
            "Ep 3 (Step 002450): Train loss 4.094, Val loss 4.610\n",
            "Ep 3 (Step 002455): Train loss 4.677, Val loss 4.607\n",
            "Ep 3 (Step 002460): Train loss 4.656, Val loss 4.588\n",
            "Ep 3 (Step 002465): Train loss 4.732, Val loss 4.596\n",
            "Ep 3 (Step 002470): Train loss 4.448, Val loss 4.594\n",
            "Ep 3 (Step 002475): Train loss 4.855, Val loss 4.590\n",
            "Ep 3 (Step 002480): Train loss 5.060, Val loss 4.584\n",
            "Ep 3 (Step 002485): Train loss 4.630, Val loss 4.592\n",
            "Ep 3 (Step 002490): Train loss 4.867, Val loss 4.590\n",
            "Ep 3 (Step 002495): Train loss 4.412, Val loss 4.582\n",
            "Ep 3 (Step 002500): Train loss 4.634, Val loss 4.589\n",
            "Ep 3 (Step 002505): Train loss 4.763, Val loss 4.595\n",
            "Ep 3 (Step 002510): Train loss 4.870, Val loss 4.615\n",
            "Ep 3 (Step 002515): Train loss 4.848, Val loss 4.570\n",
            "Ep 3 (Step 002520): Train loss 4.584, Val loss 4.574\n",
            "Ep 3 (Step 002525): Train loss 4.842, Val loss 4.579\n",
            "Every effort moves you, I am not be             I\n",
            "Ep 4 (Step 002530): Train loss 5.065, Val loss 4.594\n",
            "Ep 4 (Step 002535): Train loss 4.600, Val loss 4.632\n",
            "Ep 4 (Step 002540): Train loss 5.096, Val loss 4.605\n",
            "Ep 4 (Step 002545): Train loss 4.436, Val loss 4.604\n",
            "Ep 4 (Step 002550): Train loss 4.495, Val loss 4.594\n",
            "Ep 4 (Step 002555): Train loss 4.763, Val loss 4.606\n",
            "Ep 4 (Step 002560): Train loss 4.698, Val loss 4.621\n",
            "Ep 4 (Step 002565): Train loss 4.584, Val loss 4.618\n",
            "Ep 4 (Step 002570): Train loss 4.932, Val loss 4.589\n",
            "Ep 4 (Step 002575): Train loss 4.539, Val loss 4.587\n",
            "Ep 4 (Step 002580): Train loss 4.775, Val loss 4.586\n",
            "Ep 4 (Step 002585): Train loss 4.717, Val loss 4.585\n",
            "Ep 4 (Step 002590): Train loss 4.756, Val loss 4.575\n",
            "Ep 4 (Step 002595): Train loss 4.539, Val loss 4.612\n",
            "Ep 4 (Step 002600): Train loss 4.460, Val loss 4.602\n",
            "Ep 4 (Step 002605): Train loss 4.311, Val loss 4.607\n",
            "Ep 4 (Step 002610): Train loss 4.389, Val loss 4.606\n",
            "Ep 4 (Step 002615): Train loss 4.453, Val loss 4.610\n",
            "Ep 4 (Step 002620): Train loss 4.557, Val loss 4.601\n",
            "Ep 4 (Step 002625): Train loss 4.287, Val loss 4.615\n",
            "Ep 4 (Step 002630): Train loss 4.414, Val loss 4.635\n",
            "Ep 4 (Step 002635): Train loss 4.466, Val loss 4.628\n",
            "Ep 4 (Step 002640): Train loss 4.374, Val loss 4.643\n",
            "Ep 4 (Step 002645): Train loss 4.768, Val loss 4.633\n",
            "Ep 4 (Step 002650): Train loss 4.727, Val loss 4.660\n",
            "Ep 4 (Step 002655): Train loss 4.859, Val loss 4.647\n",
            "Ep 4 (Step 002660): Train loss 4.839, Val loss 4.619\n",
            "Ep 4 (Step 002665): Train loss 4.743, Val loss 4.594\n",
            "Ep 4 (Step 002670): Train loss 4.736, Val loss 4.590\n",
            "Ep 4 (Step 002675): Train loss 4.660, Val loss 4.597\n",
            "Ep 4 (Step 002680): Train loss 4.915, Val loss 4.595\n",
            "Ep 4 (Step 002685): Train loss 5.198, Val loss 4.590\n",
            "Ep 4 (Step 002690): Train loss 4.667, Val loss 4.584\n",
            "Ep 4 (Step 002695): Train loss 3.892, Val loss 4.608\n",
            "Ep 4 (Step 002700): Train loss 4.390, Val loss 4.603\n",
            "Ep 4 (Step 002705): Train loss 4.436, Val loss 4.670\n",
            "Ep 4 (Step 002710): Train loss 4.653, Val loss 4.636\n",
            "Ep 4 (Step 002715): Train loss 4.813, Val loss 4.629\n",
            "Ep 4 (Step 002720): Train loss 4.591, Val loss 4.627\n",
            "Ep 4 (Step 002725): Train loss 4.585, Val loss 4.652\n",
            "Ep 4 (Step 002730): Train loss 4.472, Val loss 4.619\n",
            "Ep 4 (Step 002735): Train loss 4.547, Val loss 4.609\n",
            "Ep 4 (Step 002740): Train loss 4.354, Val loss 4.597\n",
            "Ep 4 (Step 002745): Train loss 4.670, Val loss 4.592\n",
            "Ep 4 (Step 002750): Train loss 4.482, Val loss 4.637\n",
            "Ep 4 (Step 002755): Train loss 4.579, Val loss 4.605\n",
            "Ep 4 (Step 002760): Train loss 4.852, Val loss 4.603\n",
            "Ep 4 (Step 002765): Train loss 4.673, Val loss 4.594\n",
            "Ep 4 (Step 002770): Train loss 4.886, Val loss 4.613\n",
            "Ep 4 (Step 002775): Train loss 4.600, Val loss 4.602\n",
            "Ep 4 (Step 002780): Train loss 4.449, Val loss 4.580\n",
            "Ep 4 (Step 002785): Train loss 4.871, Val loss 4.593\n",
            "Ep 4 (Step 002790): Train loss 4.418, Val loss 4.599\n",
            "Ep 4 (Step 002795): Train loss 4.657, Val loss 4.605\n",
            "Ep 4 (Step 002800): Train loss 4.697, Val loss 4.601\n",
            "Ep 4 (Step 002805): Train loss 4.432, Val loss 4.602\n",
            "Ep 4 (Step 002810): Train loss 4.521, Val loss 4.599\n",
            "Ep 4 (Step 002815): Train loss 4.643, Val loss 4.589\n",
            "Ep 4 (Step 002820): Train loss 4.648, Val loss 4.600\n",
            "Ep 4 (Step 002825): Train loss 4.155, Val loss 4.598\n",
            "Ep 4 (Step 002830): Train loss 4.337, Val loss 4.603\n",
            "Ep 4 (Step 002835): Train loss 4.330, Val loss 4.619\n",
            "Ep 4 (Step 002840): Train loss 4.597, Val loss 4.614\n",
            "Ep 4 (Step 002845): Train loss 4.750, Val loss 4.634\n",
            "Ep 4 (Step 002850): Train loss 4.193, Val loss 4.625\n",
            "Ep 4 (Step 002855): Train loss 4.615, Val loss 4.618\n",
            "Ep 4 (Step 002860): Train loss 4.629, Val loss 4.614\n",
            "Ep 4 (Step 002865): Train loss 4.545, Val loss 4.614\n",
            "Ep 4 (Step 002870): Train loss 4.863, Val loss 4.601\n",
            "Ep 4 (Step 002875): Train loss 4.595, Val loss 4.592\n",
            "Ep 4 (Step 002880): Train loss 4.785, Val loss 4.597\n",
            "Ep 4 (Step 002885): Train loss 4.800, Val loss 4.612\n",
            "Ep 4 (Step 002890): Train loss 4.330, Val loss 4.588\n",
            "Ep 4 (Step 002895): Train loss 4.516, Val loss 4.583\n",
            "Ep 4 (Step 002900): Train loss 4.924, Val loss 4.608\n",
            "Ep 4 (Step 002905): Train loss 4.501, Val loss 4.589\n",
            "Ep 4 (Step 002910): Train loss 4.928, Val loss 4.568\n",
            "Ep 4 (Step 002915): Train loss 4.299, Val loss 4.589\n",
            "Ep 4 (Step 002920): Train loss 4.569, Val loss 4.584\n",
            "Ep 4 (Step 002925): Train loss 4.765, Val loss 4.569\n",
            "Ep 4 (Step 002930): Train loss 4.686, Val loss 4.591\n",
            "Ep 4 (Step 002935): Train loss 3.798, Val loss 4.570\n",
            "Ep 4 (Step 002940): Train loss 4.800, Val loss 4.563\n",
            "Ep 4 (Step 002945): Train loss 4.398, Val loss 4.566\n",
            "Ep 4 (Step 002950): Train loss 4.362, Val loss 4.564\n",
            "Ep 4 (Step 002955): Train loss 4.662, Val loss 4.577\n",
            "Ep 4 (Step 002960): Train loss 4.634, Val loss 4.567\n",
            "Ep 4 (Step 002965): Train loss 4.802, Val loss 4.589\n",
            "Ep 4 (Step 002970): Train loss 4.499, Val loss 4.573\n",
            "Ep 4 (Step 002975): Train loss 4.544, Val loss 4.580\n",
            "Ep 4 (Step 002980): Train loss 4.818, Val loss 4.571\n",
            "Ep 4 (Step 002985): Train loss 4.358, Val loss 4.562\n",
            "Ep 4 (Step 002990): Train loss 4.298, Val loss 4.572\n",
            "Ep 4 (Step 002995): Train loss 4.497, Val loss 4.577\n",
            "Ep 4 (Step 003000): Train loss 4.671, Val loss 4.569\n",
            "Ep 4 (Step 003005): Train loss 4.641, Val loss 4.580\n",
            "Ep 4 (Step 003010): Train loss 4.769, Val loss 4.598\n",
            "Ep 4 (Step 003015): Train loss 4.187, Val loss 4.588\n",
            "Ep 4 (Step 003020): Train loss 4.590, Val loss 4.563\n",
            "Ep 4 (Step 003025): Train loss 4.545, Val loss 4.553\n",
            "Ep 4 (Step 003030): Train loss 4.844, Val loss 4.559\n",
            "Ep 4 (Step 003035): Train loss 4.755, Val loss 4.583\n",
            "Ep 4 (Step 003040): Train loss 4.561, Val loss 4.597\n",
            "Ep 4 (Step 003045): Train loss 4.436, Val loss 4.591\n",
            "Ep 4 (Step 003050): Train loss 4.500, Val loss 4.590\n",
            "Ep 4 (Step 003055): Train loss 4.603, Val loss 4.597\n",
            "Ep 4 (Step 003060): Train loss 4.803, Val loss 4.615\n",
            "Ep 4 (Step 003065): Train loss 4.635, Val loss 4.578\n",
            "Ep 4 (Step 003070): Train loss 4.686, Val loss 4.601\n",
            "Ep 4 (Step 003075): Train loss 4.761, Val loss 4.651\n",
            "Ep 4 (Step 003080): Train loss 4.752, Val loss 4.629\n",
            "Ep 4 (Step 003085): Train loss 4.225, Val loss 4.607\n",
            "Ep 4 (Step 003090): Train loss 4.228, Val loss 4.602\n",
            "Ep 4 (Step 003095): Train loss 4.737, Val loss 4.610\n",
            "Ep 4 (Step 003100): Train loss 4.761, Val loss 4.607\n",
            "Ep 4 (Step 003105): Train loss 4.717, Val loss 4.599\n",
            "Ep 4 (Step 003110): Train loss 4.632, Val loss 4.571\n",
            "Ep 4 (Step 003115): Train loss 4.588, Val loss 4.575\n",
            "Ep 4 (Step 003120): Train loss 4.477, Val loss 4.553\n",
            "Ep 4 (Step 003125): Train loss 4.774, Val loss 4.559\n",
            "Ep 4 (Step 003130): Train loss 4.673, Val loss 4.565\n",
            "Ep 4 (Step 003135): Train loss 4.411, Val loss 4.576\n",
            "Ep 4 (Step 003140): Train loss 4.475, Val loss 4.595\n",
            "Ep 4 (Step 003145): Train loss 4.369, Val loss 4.596\n",
            "Ep 4 (Step 003150): Train loss 4.269, Val loss 4.566\n",
            "Ep 4 (Step 003155): Train loss 4.619, Val loss 4.558\n",
            "Ep 4 (Step 003160): Train loss 4.524, Val loss 4.542\n",
            "Ep 4 (Step 003165): Train loss 4.819, Val loss 4.570\n",
            "Ep 4 (Step 003170): Train loss 4.497, Val loss 4.591\n",
            "Ep 4 (Step 003175): Train loss 4.752, Val loss 4.630\n",
            "Ep 4 (Step 003180): Train loss 4.742, Val loss 4.636\n",
            "Ep 4 (Step 003185): Train loss 4.558, Val loss 4.623\n",
            "Ep 4 (Step 003190): Train loss 4.627, Val loss 4.591\n",
            "Ep 4 (Step 003195): Train loss 4.631, Val loss 4.596\n",
            "Ep 4 (Step 003200): Train loss 4.589, Val loss 4.576\n",
            "Ep 4 (Step 003205): Train loss 4.649, Val loss 4.637\n",
            "Ep 4 (Step 003210): Train loss 4.433, Val loss 4.583\n",
            "Ep 4 (Step 003215): Train loss 4.540, Val loss 4.586\n",
            "Ep 4 (Step 003220): Train loss 4.530, Val loss 4.575\n",
            "Ep 4 (Step 003225): Train loss 4.791, Val loss 4.613\n",
            "Ep 4 (Step 003230): Train loss 4.847, Val loss 4.581\n",
            "Ep 4 (Step 003235): Train loss 4.523, Val loss 4.566\n",
            "Ep 4 (Step 003240): Train loss 4.561, Val loss 4.550\n",
            "Ep 4 (Step 003245): Train loss 4.649, Val loss 4.562\n",
            "Ep 4 (Step 003250): Train loss 4.679, Val loss 4.543\n",
            "Ep 4 (Step 003255): Train loss 4.436, Val loss 4.596\n",
            "Ep 4 (Step 003260): Train loss 4.264, Val loss 4.600\n",
            "Ep 4 (Step 003265): Train loss 4.488, Val loss 4.581\n",
            "Ep 4 (Step 003270): Train loss 4.440, Val loss 4.596\n",
            "Ep 4 (Step 003275): Train loss 4.645, Val loss 4.560\n",
            "Ep 4 (Step 003280): Train loss 5.000, Val loss 4.552\n",
            "Ep 4 (Step 003285): Train loss 4.510, Val loss 4.557\n",
            "Ep 4 (Step 003290): Train loss 4.659, Val loss 4.572\n",
            "Ep 4 (Step 003295): Train loss 4.810, Val loss 4.547\n",
            "Ep 4 (Step 003300): Train loss 4.596, Val loss 4.530\n",
            "Ep 4 (Step 003305): Train loss 4.709, Val loss 4.540\n",
            "Ep 4 (Step 003310): Train loss 4.601, Val loss 4.548\n",
            "Ep 4 (Step 003315): Train loss 4.751, Val loss 4.550\n",
            "Ep 4 (Step 003320): Train loss 4.580, Val loss 4.573\n",
            "Ep 4 (Step 003325): Train loss 4.295, Val loss 4.598\n",
            "Ep 4 (Step 003330): Train loss 4.320, Val loss 4.548\n",
            "Ep 4 (Step 003335): Train loss 4.558, Val loss 4.544\n",
            "Ep 4 (Step 003340): Train loss 4.680, Val loss 4.542\n",
            "Ep 4 (Step 003345): Train loss 4.442, Val loss 4.560\n",
            "Ep 4 (Step 003350): Train loss 4.630, Val loss 4.551\n",
            "Ep 4 (Step 003355): Train loss 4.599, Val loss 4.561\n",
            "Ep 4 (Step 003360): Train loss 4.638, Val loss 4.568\n",
            "Ep 4 (Step 003365): Train loss 4.510, Val loss 4.564\n",
            "Ep 4 (Step 003370): Train loss 4.331, Val loss 4.550\n",
            "Every effort moves you. I have been   I have been a man,    I have I have been  \n",
            "Ep 5 (Step 003375): Train loss 4.508, Val loss 4.544\n",
            "Ep 5 (Step 003380): Train loss 4.437, Val loss 4.571\n",
            "Ep 5 (Step 003385): Train loss 4.477, Val loss 4.574\n",
            "Ep 5 (Step 003390): Train loss 4.524, Val loss 4.556\n",
            "Ep 5 (Step 003395): Train loss 4.632, Val loss 4.578\n",
            "Ep 5 (Step 003400): Train loss 4.499, Val loss 4.595\n",
            "Ep 5 (Step 003405): Train loss 4.613, Val loss 4.576\n",
            "Ep 5 (Step 003410): Train loss 4.602, Val loss 4.635\n",
            "Ep 5 (Step 003415): Train loss 4.573, Val loss 4.649\n",
            "Ep 5 (Step 003420): Train loss 4.846, Val loss 4.669\n",
            "Ep 5 (Step 003425): Train loss 4.182, Val loss 4.638\n",
            "Ep 5 (Step 003430): Train loss 4.122, Val loss 4.637\n",
            "Ep 5 (Step 003435): Train loss 4.185, Val loss 4.634\n",
            "Ep 5 (Step 003440): Train loss 4.707, Val loss 4.604\n",
            "Ep 5 (Step 003445): Train loss 4.248, Val loss 4.605\n",
            "Ep 5 (Step 003450): Train loss 4.763, Val loss 4.593\n",
            "Ep 5 (Step 003455): Train loss 4.162, Val loss 4.605\n",
            "Ep 5 (Step 003460): Train loss 4.643, Val loss 4.585\n",
            "Ep 5 (Step 003465): Train loss 4.499, Val loss 4.594\n",
            "Ep 5 (Step 003470): Train loss 4.854, Val loss 4.620\n",
            "Ep 5 (Step 003475): Train loss 4.315, Val loss 4.616\n",
            "Ep 5 (Step 003480): Train loss 4.548, Val loss 4.626\n",
            "Ep 5 (Step 003485): Train loss 4.768, Val loss 4.620\n",
            "Ep 5 (Step 003490): Train loss 4.467, Val loss 4.608\n",
            "Ep 5 (Step 003495): Train loss 4.755, Val loss 4.608\n",
            "Ep 5 (Step 003500): Train loss 4.752, Val loss 4.608\n",
            "Ep 5 (Step 003505): Train loss 4.780, Val loss 4.613\n",
            "Ep 5 (Step 003510): Train loss 3.943, Val loss 4.636\n",
            "Ep 5 (Step 003515): Train loss 4.584, Val loss 4.635\n",
            "Ep 5 (Step 003520): Train loss 4.604, Val loss 4.607\n",
            "Ep 5 (Step 003525): Train loss 4.598, Val loss 4.606\n",
            "Ep 5 (Step 003530): Train loss 4.576, Val loss 4.599\n",
            "Ep 5 (Step 003535): Train loss 4.569, Val loss 4.599\n",
            "Ep 5 (Step 003540): Train loss 4.610, Val loss 4.593\n",
            "Ep 5 (Step 003545): Train loss 4.628, Val loss 4.574\n",
            "Ep 5 (Step 003550): Train loss 4.455, Val loss 4.587\n",
            "Ep 5 (Step 003555): Train loss 4.620, Val loss 4.577\n",
            "Ep 5 (Step 003560): Train loss 4.429, Val loss 4.600\n",
            "Ep 5 (Step 003565): Train loss 4.406, Val loss 4.606\n",
            "Ep 5 (Step 003570): Train loss 4.622, Val loss 4.606\n",
            "Ep 5 (Step 003575): Train loss 4.501, Val loss 4.612\n",
            "Ep 5 (Step 003580): Train loss 4.531, Val loss 4.602\n",
            "Ep 5 (Step 003585): Train loss 4.539, Val loss 4.620\n",
            "Ep 5 (Step 003590): Train loss 4.178, Val loss 4.633\n",
            "Ep 5 (Step 003595): Train loss 4.391, Val loss 4.641\n",
            "Ep 5 (Step 003600): Train loss 4.399, Val loss 4.640\n",
            "Ep 5 (Step 003605): Train loss 4.609, Val loss 4.639\n",
            "Ep 5 (Step 003610): Train loss 4.850, Val loss 4.649\n",
            "Ep 5 (Step 003615): Train loss 4.760, Val loss 4.626\n",
            "Ep 5 (Step 003620): Train loss 4.756, Val loss 4.637\n",
            "Ep 5 (Step 003625): Train loss 4.363, Val loss 4.636\n",
            "Ep 5 (Step 003630): Train loss 4.804, Val loss 4.607\n",
            "Ep 5 (Step 003635): Train loss 4.388, Val loss 4.601\n",
            "Ep 5 (Step 003640): Train loss 4.530, Val loss 4.601\n",
            "Ep 5 (Step 003645): Train loss 4.815, Val loss 4.612\n",
            "Ep 5 (Step 003650): Train loss 4.904, Val loss 4.636\n",
            "Ep 5 (Step 003655): Train loss 4.621, Val loss 4.625\n",
            "Ep 5 (Step 003660): Train loss 4.525, Val loss 4.589\n",
            "Ep 5 (Step 003665): Train loss 4.373, Val loss 4.582\n",
            "Ep 5 (Step 003670): Train loss 4.561, Val loss 4.598\n",
            "Ep 5 (Step 003675): Train loss 4.308, Val loss 4.600\n",
            "Ep 5 (Step 003680): Train loss 4.717, Val loss 4.601\n",
            "Ep 5 (Step 003685): Train loss 4.335, Val loss 4.571\n",
            "Ep 5 (Step 003690): Train loss 4.512, Val loss 4.581\n",
            "Ep 5 (Step 003695): Train loss 4.574, Val loss 4.580\n",
            "Ep 5 (Step 003700): Train loss 4.917, Val loss 4.576\n",
            "Ep 5 (Step 003705): Train loss 4.739, Val loss 4.584\n",
            "Ep 5 (Step 003710): Train loss 4.325, Val loss 4.577\n",
            "Ep 5 (Step 003715): Train loss 4.703, Val loss 4.582\n",
            "Ep 5 (Step 003720): Train loss 4.543, Val loss 4.573\n",
            "Ep 5 (Step 003725): Train loss 4.768, Val loss 4.593\n",
            "Ep 5 (Step 003730): Train loss 4.418, Val loss 4.582\n",
            "Ep 5 (Step 003735): Train loss 4.480, Val loss 4.598\n",
            "Ep 5 (Step 003740): Train loss 4.094, Val loss 4.573\n",
            "Ep 5 (Step 003745): Train loss 4.539, Val loss 4.586\n",
            "Ep 5 (Step 003750): Train loss 4.655, Val loss 4.570\n",
            "Ep 5 (Step 003755): Train loss 4.183, Val loss 4.573\n",
            "Ep 5 (Step 003760): Train loss 4.582, Val loss 4.608\n",
            "Ep 5 (Step 003765): Train loss 4.745, Val loss 4.590\n",
            "Ep 5 (Step 003770): Train loss 4.752, Val loss 4.591\n",
            "Ep 5 (Step 003775): Train loss 4.221, Val loss 4.582\n",
            "Ep 5 (Step 003780): Train loss 4.671, Val loss 4.602\n",
            "Ep 5 (Step 003785): Train loss 4.787, Val loss 4.612\n",
            "Ep 5 (Step 003790): Train loss 4.671, Val loss 4.588\n",
            "Ep 5 (Step 003795): Train loss 4.502, Val loss 4.573\n",
            "Ep 5 (Step 003800): Train loss 4.679, Val loss 4.556\n",
            "Ep 5 (Step 003805): Train loss 4.543, Val loss 4.580\n",
            "Ep 5 (Step 003810): Train loss 4.574, Val loss 4.566\n",
            "Ep 5 (Step 003815): Train loss 4.361, Val loss 4.554\n",
            "Ep 5 (Step 003820): Train loss 4.702, Val loss 4.560\n",
            "Ep 5 (Step 003825): Train loss 4.824, Val loss 4.563\n",
            "Ep 5 (Step 003830): Train loss 4.705, Val loss 4.560\n",
            "Ep 5 (Step 003835): Train loss 4.589, Val loss 4.601\n",
            "Ep 5 (Step 003840): Train loss 4.634, Val loss 4.580\n",
            "Ep 5 (Step 003845): Train loss 4.619, Val loss 4.575\n",
            "Ep 5 (Step 003850): Train loss 4.223, Val loss 4.583\n",
            "Ep 5 (Step 003855): Train loss 4.471, Val loss 4.590\n",
            "Ep 5 (Step 003860): Train loss 4.413, Val loss 4.594\n",
            "Ep 5 (Step 003865): Train loss 4.453, Val loss 4.596\n",
            "Ep 5 (Step 003870): Train loss 4.683, Val loss 4.579\n",
            "Ep 5 (Step 003875): Train loss 4.689, Val loss 4.569\n",
            "Ep 5 (Step 003880): Train loss 4.597, Val loss 4.566\n",
            "Ep 5 (Step 003885): Train loss 4.513, Val loss 4.585\n",
            "Ep 5 (Step 003890): Train loss 4.371, Val loss 4.580\n",
            "Ep 5 (Step 003895): Train loss 4.571, Val loss 4.550\n",
            "Ep 5 (Step 003900): Train loss 4.668, Val loss 4.583\n",
            "Ep 5 (Step 003905): Train loss 4.469, Val loss 4.576\n",
            "Ep 5 (Step 003910): Train loss 4.615, Val loss 4.559\n",
            "Ep 5 (Step 003915): Train loss 4.619, Val loss 4.564\n",
            "Ep 5 (Step 003920): Train loss 4.507, Val loss 4.569\n",
            "Ep 5 (Step 003925): Train loss 4.693, Val loss 4.593\n",
            "Ep 5 (Step 003930): Train loss 4.270, Val loss 4.590\n",
            "Ep 5 (Step 003935): Train loss 4.435, Val loss 4.559\n",
            "Ep 5 (Step 003940): Train loss 4.627, Val loss 4.576\n",
            "Ep 5 (Step 003945): Train loss 4.704, Val loss 4.595\n",
            "Ep 5 (Step 003950): Train loss 4.626, Val loss 4.591\n",
            "Ep 5 (Step 003955): Train loss 4.833, Val loss 4.578\n",
            "Ep 5 (Step 003960): Train loss 4.755, Val loss 4.575\n",
            "Ep 5 (Step 003965): Train loss 4.472, Val loss 4.562\n",
            "Ep 5 (Step 003970): Train loss 5.115, Val loss 4.579\n",
            "Ep 5 (Step 003975): Train loss 4.532, Val loss 4.590\n",
            "Ep 5 (Step 003980): Train loss 4.606, Val loss 4.580\n",
            "Ep 5 (Step 003985): Train loss 4.346, Val loss 4.576\n",
            "Ep 5 (Step 003990): Train loss 4.219, Val loss 4.586\n",
            "Ep 5 (Step 003995): Train loss 4.390, Val loss 4.590\n",
            "Ep 5 (Step 004000): Train loss 4.527, Val loss 4.584\n",
            "Ep 5 (Step 004005): Train loss 4.471, Val loss 4.578\n",
            "Ep 5 (Step 004010): Train loss 4.652, Val loss 4.587\n",
            "Ep 5 (Step 004015): Train loss 4.538, Val loss 4.601\n",
            "Ep 5 (Step 004020): Train loss 4.242, Val loss 4.578\n",
            "Ep 5 (Step 004025): Train loss 4.779, Val loss 4.571\n",
            "Ep 5 (Step 004030): Train loss 4.709, Val loss 4.590\n",
            "Ep 5 (Step 004035): Train loss 4.615, Val loss 4.568\n",
            "Ep 5 (Step 004040): Train loss 4.486, Val loss 4.580\n",
            "Ep 5 (Step 004045): Train loss 4.280, Val loss 4.560\n",
            "Ep 5 (Step 004050): Train loss 4.677, Val loss 4.567\n",
            "Ep 5 (Step 004055): Train loss 4.341, Val loss 4.549\n",
            "Ep 5 (Step 004060): Train loss 4.588, Val loss 4.573\n",
            "Ep 5 (Step 004065): Train loss 4.590, Val loss 4.570\n",
            "Ep 5 (Step 004070): Train loss 4.673, Val loss 4.576\n",
            "Ep 5 (Step 004075): Train loss 4.555, Val loss 4.560\n",
            "Ep 5 (Step 004080): Train loss 4.400, Val loss 4.577\n",
            "Ep 5 (Step 004085): Train loss 4.680, Val loss 4.600\n",
            "Ep 5 (Step 004090): Train loss 4.581, Val loss 4.624\n",
            "Ep 5 (Step 004095): Train loss 3.946, Val loss 4.619\n",
            "Ep 5 (Step 004100): Train loss 4.807, Val loss 4.599\n",
            "Ep 5 (Step 004105): Train loss 4.662, Val loss 4.560\n",
            "Ep 5 (Step 004110): Train loss 4.497, Val loss 4.547\n",
            "Ep 5 (Step 004115): Train loss 4.613, Val loss 4.561\n",
            "Ep 5 (Step 004120): Train loss 4.671, Val loss 4.555\n",
            "Ep 5 (Step 004125): Train loss 4.512, Val loss 4.590\n",
            "Ep 5 (Step 004130): Train loss 4.298, Val loss 4.560\n",
            "Ep 5 (Step 004135): Train loss 4.510, Val loss 4.551\n",
            "Ep 5 (Step 004140): Train loss 4.217, Val loss 4.564\n",
            "Ep 5 (Step 004145): Train loss 4.468, Val loss 4.561\n",
            "Ep 5 (Step 004150): Train loss 4.584, Val loss 4.555\n",
            "Ep 5 (Step 004155): Train loss 4.320, Val loss 4.562\n",
            "Ep 5 (Step 004160): Train loss 4.122, Val loss 4.565\n",
            "Ep 5 (Step 004165): Train loss 4.455, Val loss 4.567\n",
            "Ep 5 (Step 004170): Train loss 4.649, Val loss 4.568\n",
            "Ep 5 (Step 004175): Train loss 4.649, Val loss 4.564\n",
            "Ep 5 (Step 004180): Train loss 4.503, Val loss 4.554\n",
            "Ep 5 (Step 004185): Train loss 4.574, Val loss 4.543\n",
            "Ep 5 (Step 004190): Train loss 4.214, Val loss 4.536\n",
            "Ep 5 (Step 004195): Train loss 3.985, Val loss 4.523\n",
            "Ep 5 (Step 004200): Train loss 4.374, Val loss 4.540\n",
            "Ep 5 (Step 004205): Train loss 4.814, Val loss 4.554\n",
            "Ep 5 (Step 004210): Train loss 4.254, Val loss 4.563\n",
            "Every effort moves you.       I am sure you will be             I am sure you.      \n",
            "Ep 6 (Step 004215): Train loss 4.550, Val loss 4.579\n",
            "Ep 6 (Step 004220): Train loss 4.618, Val loss 4.535\n",
            "Ep 6 (Step 004225): Train loss 4.706, Val loss 4.596\n",
            "Ep 6 (Step 004230): Train loss 4.494, Val loss 4.615\n",
            "Ep 6 (Step 004235): Train loss 4.891, Val loss 4.596\n",
            "Ep 6 (Step 004240): Train loss 4.621, Val loss 4.596\n",
            "Ep 6 (Step 004245): Train loss 4.686, Val loss 4.593\n",
            "Ep 6 (Step 004250): Train loss 4.478, Val loss 4.571\n",
            "Ep 6 (Step 004255): Train loss 4.669, Val loss 4.598\n",
            "Ep 6 (Step 004260): Train loss 4.641, Val loss 4.589\n",
            "Ep 6 (Step 004265): Train loss 4.699, Val loss 4.618\n",
            "Ep 6 (Step 004270): Train loss 4.642, Val loss 4.609\n",
            "Ep 6 (Step 004275): Train loss 4.072, Val loss 4.605\n",
            "Ep 6 (Step 004280): Train loss 4.698, Val loss 4.625\n",
            "Ep 6 (Step 004285): Train loss 4.754, Val loss 4.659\n",
            "Ep 6 (Step 004290): Train loss 4.660, Val loss 4.611\n",
            "Ep 6 (Step 004295): Train loss 4.793, Val loss 4.640\n",
            "Ep 6 (Step 004300): Train loss 4.639, Val loss 4.621\n",
            "Ep 6 (Step 004305): Train loss 4.687, Val loss 4.624\n",
            "Ep 6 (Step 004310): Train loss 4.695, Val loss 4.600\n",
            "Ep 6 (Step 004315): Train loss 4.151, Val loss 4.602\n",
            "Ep 6 (Step 004320): Train loss 4.737, Val loss 4.596\n",
            "Ep 6 (Step 004325): Train loss 4.409, Val loss 4.618\n",
            "Ep 6 (Step 004330): Train loss 4.021, Val loss 4.613\n",
            "Ep 6 (Step 004335): Train loss 4.521, Val loss 4.622\n",
            "Ep 6 (Step 004340): Train loss 4.790, Val loss 4.619\n",
            "Ep 6 (Step 004345): Train loss 4.691, Val loss 4.619\n",
            "Ep 6 (Step 004350): Train loss 4.445, Val loss 4.610\n",
            "Ep 6 (Step 004355): Train loss 4.496, Val loss 4.605\n",
            "Ep 6 (Step 004360): Train loss 4.737, Val loss 4.595\n",
            "Ep 6 (Step 004365): Train loss 4.211, Val loss 4.603\n",
            "Ep 6 (Step 004370): Train loss 4.470, Val loss 4.600\n",
            "Ep 6 (Step 004375): Train loss 4.650, Val loss 4.604\n",
            "Ep 6 (Step 004380): Train loss 4.771, Val loss 4.627\n",
            "Ep 6 (Step 004385): Train loss 4.222, Val loss 4.640\n",
            "Ep 6 (Step 004390): Train loss 4.594, Val loss 4.612\n",
            "Ep 6 (Step 004395): Train loss 4.580, Val loss 4.594\n",
            "Ep 6 (Step 004400): Train loss 4.422, Val loss 4.614\n",
            "Ep 6 (Step 004405): Train loss 4.715, Val loss 4.609\n",
            "Ep 6 (Step 004410): Train loss 4.693, Val loss 4.613\n",
            "Ep 6 (Step 004415): Train loss 4.544, Val loss 4.603\n",
            "Ep 6 (Step 004420): Train loss 4.251, Val loss 4.586\n",
            "Ep 6 (Step 004425): Train loss 4.505, Val loss 4.586\n",
            "Ep 6 (Step 004430): Train loss 4.532, Val loss 4.594\n",
            "Ep 6 (Step 004435): Train loss 4.460, Val loss 4.618\n",
            "Ep 6 (Step 004440): Train loss 4.600, Val loss 4.618\n",
            "Ep 6 (Step 004445): Train loss 4.778, Val loss 4.613\n",
            "Ep 6 (Step 004450): Train loss 4.487, Val loss 4.641\n",
            "Ep 6 (Step 004455): Train loss 4.761, Val loss 4.645\n",
            "Ep 6 (Step 004460): Train loss 4.710, Val loss 4.634\n",
            "Ep 6 (Step 004465): Train loss 4.800, Val loss 4.621\n",
            "Ep 6 (Step 004470): Train loss 4.817, Val loss 4.614\n",
            "Ep 6 (Step 004475): Train loss 4.712, Val loss 4.641\n",
            "Ep 6 (Step 004480): Train loss 4.520, Val loss 4.617\n",
            "Ep 6 (Step 004485): Train loss 4.725, Val loss 4.646\n",
            "Ep 6 (Step 004490): Train loss 4.653, Val loss 4.655\n",
            "Ep 6 (Step 004495): Train loss 4.509, Val loss 4.708\n",
            "Ep 6 (Step 004500): Train loss 4.612, Val loss 4.667\n",
            "Ep 6 (Step 004505): Train loss 4.666, Val loss 4.655\n",
            "Ep 6 (Step 004510): Train loss 4.717, Val loss 4.630\n",
            "Ep 6 (Step 004515): Train loss 4.662, Val loss 4.660\n",
            "Ep 6 (Step 004520): Train loss 4.314, Val loss 4.611\n",
            "Ep 6 (Step 004525): Train loss 4.564, Val loss 4.615\n",
            "Ep 6 (Step 004530): Train loss 4.805, Val loss 4.611\n",
            "Ep 6 (Step 004535): Train loss 4.576, Val loss 4.612\n",
            "Ep 6 (Step 004540): Train loss 4.682, Val loss 4.613\n",
            "Ep 6 (Step 004545): Train loss 4.597, Val loss 4.605\n",
            "Ep 6 (Step 004550): Train loss 4.811, Val loss 4.632\n",
            "Ep 6 (Step 004555): Train loss 4.601, Val loss 4.615\n",
            "Ep 6 (Step 004560): Train loss 4.554, Val loss 4.610\n",
            "Ep 6 (Step 004565): Train loss 4.322, Val loss 4.612\n",
            "Ep 6 (Step 004570): Train loss 4.595, Val loss 4.632\n",
            "Ep 6 (Step 004575): Train loss 4.542, Val loss 4.631\n",
            "Ep 6 (Step 004580): Train loss 4.752, Val loss 4.646\n",
            "Ep 6 (Step 004585): Train loss 4.643, Val loss 4.644\n",
            "Ep 6 (Step 004590): Train loss 4.647, Val loss 4.643\n",
            "Ep 6 (Step 004595): Train loss 4.302, Val loss 4.621\n",
            "Ep 6 (Step 004600): Train loss 4.577, Val loss 4.634\n",
            "Ep 6 (Step 004605): Train loss 4.622, Val loss 4.621\n",
            "Ep 6 (Step 004610): Train loss 4.231, Val loss 4.645\n",
            "Ep 6 (Step 004615): Train loss 4.707, Val loss 4.610\n",
            "Ep 6 (Step 004620): Train loss 4.121, Val loss 4.607\n",
            "Ep 6 (Step 004625): Train loss 4.669, Val loss 4.600\n",
            "Ep 6 (Step 004630): Train loss 4.772, Val loss 4.607\n",
            "Ep 6 (Step 004635): Train loss 4.805, Val loss 4.582\n",
            "Ep 6 (Step 004640): Train loss 4.414, Val loss 4.618\n",
            "Ep 6 (Step 004645): Train loss 3.956, Val loss 4.591\n",
            "Ep 6 (Step 004650): Train loss 4.517, Val loss 4.601\n",
            "Ep 6 (Step 004655): Train loss 4.629, Val loss 4.662\n",
            "Ep 6 (Step 004660): Train loss 4.439, Val loss 4.627\n",
            "Ep 6 (Step 004665): Train loss 4.663, Val loss 4.589\n",
            "Ep 6 (Step 004670): Train loss 4.474, Val loss 4.607\n",
            "Ep 6 (Step 004675): Train loss 4.523, Val loss 4.637\n",
            "Ep 6 (Step 004680): Train loss 4.728, Val loss 4.625\n",
            "Ep 6 (Step 004685): Train loss 4.631, Val loss 4.650\n",
            "Ep 6 (Step 004690): Train loss 4.525, Val loss 4.646\n",
            "Ep 6 (Step 004695): Train loss 4.569, Val loss 4.641\n",
            "Ep 6 (Step 004700): Train loss 4.680, Val loss 4.646\n",
            "Ep 6 (Step 004705): Train loss 4.674, Val loss 4.626\n",
            "Ep 6 (Step 004710): Train loss 4.129, Val loss 4.607\n",
            "Ep 6 (Step 004715): Train loss 4.397, Val loss 4.588\n",
            "Ep 6 (Step 004720): Train loss 4.476, Val loss 4.620\n",
            "Ep 6 (Step 004725): Train loss 4.681, Val loss 4.617\n",
            "Ep 6 (Step 004730): Train loss 4.510, Val loss 4.625\n",
            "Ep 6 (Step 004735): Train loss 4.819, Val loss 4.614\n",
            "Ep 6 (Step 004740): Train loss 4.584, Val loss 4.628\n",
            "Ep 6 (Step 004745): Train loss 4.553, Val loss 4.641\n",
            "Ep 6 (Step 004750): Train loss 4.726, Val loss 4.644\n",
            "Ep 6 (Step 004755): Train loss 4.733, Val loss 4.617\n",
            "Ep 6 (Step 004760): Train loss 4.559, Val loss 4.616\n",
            "Ep 6 (Step 004765): Train loss 4.530, Val loss 4.630\n",
            "Ep 6 (Step 004770): Train loss 4.591, Val loss 4.632\n",
            "Ep 6 (Step 004775): Train loss 4.155, Val loss 4.618\n",
            "Ep 6 (Step 004780): Train loss 4.604, Val loss 4.609\n",
            "Ep 6 (Step 004785): Train loss 4.565, Val loss 4.633\n",
            "Ep 6 (Step 004790): Train loss 4.635, Val loss 4.634\n",
            "Ep 6 (Step 004795): Train loss 4.610, Val loss 4.609\n",
            "Ep 6 (Step 004800): Train loss 4.602, Val loss 4.594\n",
            "Ep 6 (Step 004805): Train loss 4.663, Val loss 4.625\n",
            "Ep 6 (Step 004810): Train loss 4.561, Val loss 4.629\n",
            "Ep 6 (Step 004815): Train loss 4.617, Val loss 4.633\n",
            "Ep 6 (Step 004820): Train loss 4.405, Val loss 4.626\n",
            "Ep 6 (Step 004825): Train loss 4.424, Val loss 4.615\n",
            "Ep 6 (Step 004830): Train loss 4.820, Val loss 4.604\n",
            "Ep 6 (Step 004835): Train loss 4.537, Val loss 4.600\n",
            "Ep 6 (Step 004840): Train loss 4.469, Val loss 4.626\n",
            "Ep 6 (Step 004845): Train loss 4.467, Val loss 4.619\n",
            "Ep 6 (Step 004850): Train loss 4.538, Val loss 4.593\n",
            "Ep 6 (Step 004855): Train loss 4.673, Val loss 4.579\n",
            "Ep 6 (Step 004860): Train loss 4.495, Val loss 4.578\n",
            "Ep 6 (Step 004865): Train loss 4.433, Val loss 4.580\n",
            "Ep 6 (Step 004870): Train loss 4.210, Val loss 4.572\n",
            "Ep 6 (Step 004875): Train loss 4.506, Val loss 4.564\n",
            "Ep 6 (Step 004880): Train loss 4.537, Val loss 4.556\n",
            "Ep 6 (Step 004885): Train loss 4.113, Val loss 4.559\n",
            "Ep 6 (Step 004890): Train loss 4.422, Val loss 4.553\n",
            "Ep 6 (Step 004895): Train loss 4.776, Val loss 4.558\n",
            "Ep 6 (Step 004900): Train loss 4.190, Val loss 4.549\n",
            "Ep 6 (Step 004905): Train loss 4.250, Val loss 4.555\n",
            "Ep 6 (Step 004910): Train loss 4.502, Val loss 4.545\n",
            "Ep 6 (Step 004915): Train loss 4.447, Val loss 4.542\n",
            "Ep 6 (Step 004920): Train loss 4.461, Val loss 4.540\n",
            "Ep 6 (Step 004925): Train loss 3.888, Val loss 4.543\n",
            "Ep 6 (Step 004930): Train loss 4.643, Val loss 4.560\n",
            "Ep 6 (Step 004935): Train loss 4.408, Val loss 4.568\n",
            "Ep 6 (Step 004940): Train loss 4.567, Val loss 4.565\n",
            "Ep 6 (Step 004945): Train loss 4.502, Val loss 4.585\n",
            "Ep 6 (Step 004950): Train loss 4.349, Val loss 4.570\n",
            "Ep 6 (Step 004955): Train loss 4.335, Val loss 4.559\n",
            "Ep 6 (Step 004960): Train loss 4.626, Val loss 4.528\n",
            "Ep 6 (Step 004965): Train loss 4.480, Val loss 4.510\n",
            "Ep 6 (Step 004970): Train loss 4.490, Val loss 4.509\n",
            "Ep 6 (Step 004975): Train loss 4.200, Val loss 4.527\n",
            "Ep 6 (Step 004980): Train loss 4.495, Val loss 4.516\n",
            "Ep 6 (Step 004985): Train loss 4.271, Val loss 4.526\n",
            "Ep 6 (Step 004990): Train loss 4.454, Val loss 4.565\n",
            "Ep 6 (Step 004995): Train loss 4.578, Val loss 4.575\n",
            "Ep 6 (Step 005000): Train loss 4.251, Val loss 4.593\n",
            "Ep 6 (Step 005005): Train loss 4.240, Val loss 4.570\n",
            "Ep 6 (Step 005010): Train loss 4.208, Val loss 4.543\n",
            "Ep 6 (Step 005015): Train loss 4.590, Val loss 4.573\n",
            "Ep 6 (Step 005020): Train loss 4.454, Val loss 4.564\n",
            "Ep 6 (Step 005025): Train loss 4.469, Val loss 4.582\n",
            "Ep 6 (Step 005030): Train loss 4.716, Val loss 4.581\n",
            "Ep 6 (Step 005035): Train loss 4.555, Val loss 4.612\n",
            "Ep 6 (Step 005040): Train loss 4.631, Val loss 4.610\n",
            "Ep 6 (Step 005045): Train loss 4.339, Val loss 4.603\n",
            "Ep 6 (Step 005050): Train loss 4.528, Val loss 4.580\n",
            "Ep 6 (Step 005055): Train loss 4.865, Val loss 4.565\n",
            "Every effort moves you have                                                 \n",
            "Ep 7 (Step 005060): Train loss 4.562, Val loss 4.560\n",
            "Ep 7 (Step 005065): Train loss 4.645, Val loss 4.561\n",
            "Ep 7 (Step 005070): Train loss 4.071, Val loss 4.559\n",
            "Ep 7 (Step 005075): Train loss 4.800, Val loss 4.592\n",
            "Ep 7 (Step 005080): Train loss 4.519, Val loss 4.576\n",
            "Ep 7 (Step 005085): Train loss 4.656, Val loss 4.549\n",
            "Ep 7 (Step 005090): Train loss 3.962, Val loss 4.565\n",
            "Ep 7 (Step 005095): Train loss 4.648, Val loss 4.607\n",
            "Ep 7 (Step 005100): Train loss 4.576, Val loss 4.616\n",
            "Ep 7 (Step 005105): Train loss 4.692, Val loss 4.645\n",
            "Ep 7 (Step 005110): Train loss 4.522, Val loss 4.592\n",
            "Ep 7 (Step 005115): Train loss 4.490, Val loss 4.614\n",
            "Ep 7 (Step 005120): Train loss 4.649, Val loss 4.633\n",
            "Ep 7 (Step 005125): Train loss 4.505, Val loss 4.640\n",
            "Ep 7 (Step 005130): Train loss 4.581, Val loss 4.637\n",
            "Ep 7 (Step 005135): Train loss 4.485, Val loss 4.608\n",
            "Ep 7 (Step 005140): Train loss 4.640, Val loss 4.579\n",
            "Ep 7 (Step 005145): Train loss 4.896, Val loss 4.599\n",
            "Ep 7 (Step 005150): Train loss 4.483, Val loss 4.567\n",
            "Ep 7 (Step 005155): Train loss 4.587, Val loss 4.554\n",
            "Ep 7 (Step 005160): Train loss 4.754, Val loss 4.570\n",
            "Ep 7 (Step 005165): Train loss 4.425, Val loss 4.616\n",
            "Ep 7 (Step 005170): Train loss 4.376, Val loss 4.615\n",
            "Ep 7 (Step 005175): Train loss 4.606, Val loss 4.572\n",
            "Ep 7 (Step 005180): Train loss 4.468, Val loss 4.588\n",
            "Ep 7 (Step 005185): Train loss 4.387, Val loss 4.623\n",
            "Ep 7 (Step 005190): Train loss 4.717, Val loss 4.664\n",
            "Ep 7 (Step 005195): Train loss 4.642, Val loss 4.631\n",
            "Ep 7 (Step 005200): Train loss 4.818, Val loss 4.598\n",
            "Ep 7 (Step 005205): Train loss 4.386, Val loss 4.612\n",
            "Ep 7 (Step 005210): Train loss 4.415, Val loss 4.586\n",
            "Ep 7 (Step 005215): Train loss 4.647, Val loss 4.598\n",
            "Ep 7 (Step 005220): Train loss 4.507, Val loss 4.572\n",
            "Ep 7 (Step 005225): Train loss 4.443, Val loss 4.575\n",
            "Ep 7 (Step 005230): Train loss 4.467, Val loss 4.575\n",
            "Ep 7 (Step 005235): Train loss 4.172, Val loss 4.570\n",
            "Ep 7 (Step 005240): Train loss 4.641, Val loss 4.575\n",
            "Ep 7 (Step 005245): Train loss 4.754, Val loss 4.583\n",
            "Ep 7 (Step 005250): Train loss 4.662, Val loss 4.593\n",
            "Ep 7 (Step 005255): Train loss 4.822, Val loss 4.576\n",
            "Ep 7 (Step 005260): Train loss 4.784, Val loss 4.588\n",
            "Ep 7 (Step 005265): Train loss 4.719, Val loss 4.609\n",
            "Ep 7 (Step 005270): Train loss 4.269, Val loss 4.603\n",
            "Ep 7 (Step 005275): Train loss 4.484, Val loss 4.583\n",
            "Ep 7 (Step 005280): Train loss 4.547, Val loss 4.595\n",
            "Ep 7 (Step 005285): Train loss 4.427, Val loss 4.585\n",
            "Ep 7 (Step 005290): Train loss 4.489, Val loss 4.577\n",
            "Ep 7 (Step 005295): Train loss 4.475, Val loss 4.559\n",
            "Ep 7 (Step 005300): Train loss 4.460, Val loss 4.549\n",
            "Ep 7 (Step 005305): Train loss 4.661, Val loss 4.583\n",
            "Ep 7 (Step 005310): Train loss 4.346, Val loss 4.584\n",
            "Ep 7 (Step 005315): Train loss 4.221, Val loss 4.565\n",
            "Ep 7 (Step 005320): Train loss 4.517, Val loss 4.566\n",
            "Ep 7 (Step 005325): Train loss 4.168, Val loss 4.587\n",
            "Ep 7 (Step 005330): Train loss 4.646, Val loss 4.588\n",
            "Ep 7 (Step 005335): Train loss 4.580, Val loss 4.603\n",
            "Ep 7 (Step 005340): Train loss 4.693, Val loss 4.595\n",
            "Ep 7 (Step 005345): Train loss 4.001, Val loss 4.588\n",
            "Ep 7 (Step 005350): Train loss 4.512, Val loss 4.604\n",
            "Ep 7 (Step 005355): Train loss 4.560, Val loss 4.606\n",
            "Ep 7 (Step 005360): Train loss 4.309, Val loss 4.569\n",
            "Ep 7 (Step 005365): Train loss 4.546, Val loss 4.651\n",
            "Ep 7 (Step 005370): Train loss 4.383, Val loss 4.677\n",
            "Ep 7 (Step 005375): Train loss 4.806, Val loss 4.684\n",
            "Ep 7 (Step 005380): Train loss 4.436, Val loss 4.717\n",
            "Ep 7 (Step 005385): Train loss 4.748, Val loss 4.701\n",
            "Ep 7 (Step 005390): Train loss 4.660, Val loss 4.703\n",
            "Ep 7 (Step 005395): Train loss 4.260, Val loss 4.648\n",
            "Ep 7 (Step 005400): Train loss 4.692, Val loss 4.630\n",
            "Ep 7 (Step 005405): Train loss 4.626, Val loss 4.639\n",
            "Ep 7 (Step 005410): Train loss 4.899, Val loss 4.638\n",
            "Ep 7 (Step 005415): Train loss 4.634, Val loss 4.627\n",
            "Ep 7 (Step 005420): Train loss 4.549, Val loss 4.596\n",
            "Ep 7 (Step 005425): Train loss 4.626, Val loss 4.590\n",
            "Ep 7 (Step 005430): Train loss 4.484, Val loss 4.583\n",
            "Ep 7 (Step 005435): Train loss 4.663, Val loss 4.601\n",
            "Ep 7 (Step 005440): Train loss 4.697, Val loss 4.643\n",
            "Ep 7 (Step 005445): Train loss 4.341, Val loss 4.626\n",
            "Ep 7 (Step 005450): Train loss 4.628, Val loss 4.607\n",
            "Ep 7 (Step 005455): Train loss 4.433, Val loss 4.625\n",
            "Ep 7 (Step 005460): Train loss 4.590, Val loss 4.605\n",
            "Ep 7 (Step 005465): Train loss 4.654, Val loss 4.605\n",
            "Ep 7 (Step 005470): Train loss 4.520, Val loss 4.560\n",
            "Ep 7 (Step 005475): Train loss 4.422, Val loss 4.580\n",
            "Ep 7 (Step 005480): Train loss 4.658, Val loss 4.586\n",
            "Ep 7 (Step 005485): Train loss 4.456, Val loss 4.567\n",
            "Ep 7 (Step 005490): Train loss 4.536, Val loss 4.574\n",
            "Ep 7 (Step 005495): Train loss 4.285, Val loss 4.586\n",
            "Ep 7 (Step 005500): Train loss 4.497, Val loss 4.613\n",
            "Ep 7 (Step 005505): Train loss 4.378, Val loss 4.600\n",
            "Ep 7 (Step 005510): Train loss 4.639, Val loss 4.597\n",
            "Ep 7 (Step 005515): Train loss 4.364, Val loss 4.596\n",
            "Ep 7 (Step 005520): Train loss 4.473, Val loss 4.612\n",
            "Ep 7 (Step 005525): Train loss 4.517, Val loss 4.616\n",
            "Ep 7 (Step 005530): Train loss 4.406, Val loss 4.609\n",
            "Ep 7 (Step 005535): Train loss 4.408, Val loss 4.608\n",
            "Ep 7 (Step 005540): Train loss 4.542, Val loss 4.619\n",
            "Ep 7 (Step 005545): Train loss 3.859, Val loss 4.574\n",
            "Ep 7 (Step 005550): Train loss 4.294, Val loss 4.584\n",
            "Ep 7 (Step 005555): Train loss 4.545, Val loss 4.579\n",
            "Ep 7 (Step 005560): Train loss 4.680, Val loss 4.561\n",
            "Ep 7 (Step 005565): Train loss 4.526, Val loss 4.570\n",
            "Ep 7 (Step 005570): Train loss 4.627, Val loss 4.581\n",
            "Ep 7 (Step 005575): Train loss 4.401, Val loss 4.580\n",
            "Ep 7 (Step 005580): Train loss 4.525, Val loss 4.607\n",
            "Ep 7 (Step 005585): Train loss 4.416, Val loss 4.595\n",
            "Ep 7 (Step 005590): Train loss 4.401, Val loss 4.607\n",
            "Ep 7 (Step 005595): Train loss 4.142, Val loss 4.608\n",
            "Ep 7 (Step 005600): Train loss 4.615, Val loss 4.632\n",
            "Ep 7 (Step 005605): Train loss 4.501, Val loss 4.623\n",
            "Ep 7 (Step 005610): Train loss 4.553, Val loss 4.597\n",
            "Ep 7 (Step 005615): Train loss 4.631, Val loss 4.590\n",
            "Ep 7 (Step 005620): Train loss 4.396, Val loss 4.578\n",
            "Ep 7 (Step 005625): Train loss 4.372, Val loss 4.569\n",
            "Ep 7 (Step 005630): Train loss 4.487, Val loss 4.573\n",
            "Ep 7 (Step 005635): Train loss 4.549, Val loss 4.553\n",
            "Ep 7 (Step 005640): Train loss 4.572, Val loss 4.555\n",
            "Ep 7 (Step 005645): Train loss 4.629, Val loss 4.558\n",
            "Ep 7 (Step 005650): Train loss 4.629, Val loss 4.577\n",
            "Ep 7 (Step 005655): Train loss 4.394, Val loss 4.566\n",
            "Ep 7 (Step 005660): Train loss 4.700, Val loss 4.561\n",
            "Ep 7 (Step 005665): Train loss 4.434, Val loss 4.558\n",
            "Ep 7 (Step 005670): Train loss 4.312, Val loss 4.548\n",
            "Ep 7 (Step 005675): Train loss 4.710, Val loss 4.553\n",
            "Ep 7 (Step 005680): Train loss 4.344, Val loss 4.556\n",
            "Ep 7 (Step 005685): Train loss 4.508, Val loss 4.547\n",
            "Ep 7 (Step 005690): Train loss 4.155, Val loss 4.540\n",
            "Ep 7 (Step 005695): Train loss 4.337, Val loss 4.542\n",
            "Ep 7 (Step 005700): Train loss 4.446, Val loss 4.530\n",
            "Ep 7 (Step 005705): Train loss 4.288, Val loss 4.546\n",
            "Ep 7 (Step 005710): Train loss 4.408, Val loss 4.549\n",
            "Ep 7 (Step 005715): Train loss 4.267, Val loss 4.589\n",
            "Ep 7 (Step 005720): Train loss 4.694, Val loss 4.605\n",
            "Ep 7 (Step 005725): Train loss 4.070, Val loss 4.598\n",
            "Ep 7 (Step 005730): Train loss 4.590, Val loss 4.618\n",
            "Ep 7 (Step 005735): Train loss 4.533, Val loss 4.604\n",
            "Ep 7 (Step 005740): Train loss 4.456, Val loss 4.605\n",
            "Ep 7 (Step 005745): Train loss 4.325, Val loss 4.579\n",
            "Ep 7 (Step 005750): Train loss 4.420, Val loss 4.569\n",
            "Ep 7 (Step 005755): Train loss 4.500, Val loss 4.565\n",
            "Ep 7 (Step 005760): Train loss 4.534, Val loss 4.581\n",
            "Ep 7 (Step 005765): Train loss 4.744, Val loss 4.592\n",
            "Ep 7 (Step 005770): Train loss 3.640, Val loss 4.585\n",
            "Ep 7 (Step 005775): Train loss 4.208, Val loss 4.588\n",
            "Ep 7 (Step 005780): Train loss 4.666, Val loss 4.596\n",
            "Ep 7 (Step 005785): Train loss 4.705, Val loss 4.600\n",
            "Ep 7 (Step 005790): Train loss 4.618, Val loss 4.601\n",
            "Ep 7 (Step 005795): Train loss 4.318, Val loss 4.620\n",
            "Ep 7 (Step 005800): Train loss 4.272, Val loss 4.613\n",
            "Ep 7 (Step 005805): Train loss 4.615, Val loss 4.601\n",
            "Ep 7 (Step 005810): Train loss 4.320, Val loss 4.596\n",
            "Ep 7 (Step 005815): Train loss 4.406, Val loss 4.599\n",
            "Ep 7 (Step 005820): Train loss 4.379, Val loss 4.602\n",
            "Ep 7 (Step 005825): Train loss 4.564, Val loss 4.588\n",
            "Ep 7 (Step 005830): Train loss 4.299, Val loss 4.596\n",
            "Ep 7 (Step 005835): Train loss 4.486, Val loss 4.579\n",
            "Ep 7 (Step 005840): Train loss 4.751, Val loss 4.579\n",
            "Ep 7 (Step 005845): Train loss 4.113, Val loss 4.601\n",
            "Ep 7 (Step 005850): Train loss 4.165, Val loss 4.591\n",
            "Ep 7 (Step 005855): Train loss 4.580, Val loss 4.578\n",
            "Ep 7 (Step 005860): Train loss 4.554, Val loss 4.571\n",
            "Ep 7 (Step 005865): Train loss 4.627, Val loss 4.582\n",
            "Ep 7 (Step 005870): Train loss 4.427, Val loss 4.576\n",
            "Ep 7 (Step 005875): Train loss 4.566, Val loss 4.562\n",
            "Ep 7 (Step 005880): Train loss 4.437, Val loss 4.575\n",
            "Ep 7 (Step 005885): Train loss 4.762, Val loss 4.547\n",
            "Ep 7 (Step 005890): Train loss 4.321, Val loss 4.546\n",
            "Ep 7 (Step 005895): Train loss 4.471, Val loss 4.524\n",
            "Ep 7 (Step 005900): Train loss 4.329, Val loss 4.533\n",
            "Every effort moves you  I am sure, said I have not have been            I am sure, said I have been at the \n",
            "Ep 8 (Step 005905): Train loss 4.476, Val loss 4.525\n",
            "Ep 8 (Step 005910): Train loss 4.459, Val loss 4.528\n",
            "Ep 8 (Step 005915): Train loss 4.423, Val loss 4.538\n",
            "Ep 8 (Step 005920): Train loss 4.566, Val loss 4.571\n",
            "Ep 8 (Step 005925): Train loss 4.512, Val loss 4.595\n",
            "Ep 8 (Step 005930): Train loss 4.592, Val loss 4.601\n",
            "Ep 8 (Step 005935): Train loss 4.110, Val loss 4.582\n",
            "Ep 8 (Step 005940): Train loss 4.107, Val loss 4.577\n",
            "Ep 8 (Step 005945): Train loss 4.130, Val loss 4.593\n",
            "Ep 8 (Step 005950): Train loss 4.394, Val loss 4.595\n",
            "Ep 8 (Step 005955): Train loss 4.413, Val loss 4.605\n",
            "Ep 8 (Step 005960): Train loss 4.867, Val loss 4.604\n",
            "Ep 8 (Step 005965): Train loss 4.579, Val loss 4.598\n",
            "Ep 8 (Step 005970): Train loss 4.303, Val loss 4.566\n",
            "Ep 8 (Step 005975): Train loss 4.615, Val loss 4.569\n",
            "Ep 8 (Step 005980): Train loss 4.522, Val loss 4.574\n",
            "Ep 8 (Step 005985): Train loss 4.170, Val loss 4.577\n",
            "Ep 8 (Step 005990): Train loss 4.313, Val loss 4.589\n",
            "Ep 8 (Step 005995): Train loss 4.648, Val loss 4.580\n",
            "Ep 8 (Step 006000): Train loss 4.413, Val loss 4.568\n",
            "Ep 8 (Step 006005): Train loss 4.621, Val loss 4.600\n",
            "Ep 8 (Step 006010): Train loss 4.403, Val loss 4.620\n",
            "Ep 8 (Step 006015): Train loss 4.551, Val loss 4.606\n",
            "Ep 8 (Step 006020): Train loss 4.373, Val loss 4.582\n",
            "Ep 8 (Step 006025): Train loss 4.496, Val loss 4.610\n",
            "Ep 8 (Step 006030): Train loss 4.284, Val loss 4.626\n",
            "Ep 8 (Step 006035): Train loss 3.777, Val loss 4.611\n",
            "Ep 8 (Step 006040): Train loss 4.090, Val loss 4.593\n",
            "Ep 8 (Step 006045): Train loss 4.421, Val loss 4.642\n",
            "Ep 8 (Step 006050): Train loss 4.517, Val loss 4.630\n",
            "Ep 8 (Step 006055): Train loss 3.783, Val loss 4.644\n",
            "Ep 8 (Step 006060): Train loss 4.307, Val loss 4.633\n",
            "Ep 8 (Step 006065): Train loss 4.604, Val loss 4.650\n",
            "Ep 8 (Step 006070): Train loss 4.517, Val loss 4.657\n",
            "Ep 8 (Step 006075): Train loss 4.518, Val loss 4.634\n",
            "Ep 8 (Step 006080): Train loss 4.186, Val loss 4.645\n",
            "Ep 8 (Step 006085): Train loss 4.420, Val loss 4.635\n",
            "Ep 8 (Step 006090): Train loss 4.547, Val loss 4.618\n",
            "Ep 8 (Step 006095): Train loss 4.534, Val loss 4.645\n",
            "Ep 8 (Step 006100): Train loss 4.717, Val loss 4.610\n",
            "Ep 8 (Step 006105): Train loss 4.464, Val loss 4.603\n",
            "Ep 8 (Step 006110): Train loss 4.196, Val loss 4.605\n",
            "Ep 8 (Step 006115): Train loss 4.015, Val loss 4.579\n",
            "Ep 8 (Step 006120): Train loss 4.719, Val loss 4.594\n",
            "Ep 8 (Step 006125): Train loss 4.454, Val loss 4.611\n",
            "Ep 8 (Step 006130): Train loss 4.636, Val loss 4.627\n",
            "Ep 8 (Step 006135): Train loss 4.293, Val loss 4.653\n",
            "Ep 8 (Step 006140): Train loss 4.188, Val loss 4.662\n",
            "Ep 8 (Step 006145): Train loss 4.584, Val loss 4.660\n",
            "Ep 8 (Step 006150): Train loss 4.414, Val loss 4.670\n",
            "Ep 8 (Step 006155): Train loss 4.186, Val loss 4.654\n",
            "Ep 8 (Step 006160): Train loss 4.595, Val loss 4.634\n",
            "Ep 8 (Step 006165): Train loss 4.477, Val loss 4.615\n",
            "Ep 8 (Step 006170): Train loss 4.535, Val loss 4.611\n",
            "Ep 8 (Step 006175): Train loss 4.650, Val loss 4.591\n",
            "Ep 8 (Step 006180): Train loss 4.054, Val loss 4.608\n",
            "Ep 8 (Step 006185): Train loss 4.468, Val loss 4.566\n",
            "Ep 8 (Step 006190): Train loss 4.536, Val loss 4.562\n",
            "Ep 8 (Step 006195): Train loss 4.410, Val loss 4.560\n",
            "Ep 8 (Step 006200): Train loss 4.382, Val loss 4.562\n",
            "Ep 8 (Step 006205): Train loss 3.655, Val loss 4.580\n",
            "Ep 8 (Step 006210): Train loss 3.614, Val loss 4.573\n",
            "Ep 8 (Step 006215): Train loss 4.592, Val loss 4.588\n",
            "Ep 8 (Step 006220): Train loss 4.154, Val loss 4.558\n",
            "Ep 8 (Step 006225): Train loss 4.396, Val loss 4.576\n",
            "Ep 8 (Step 006230): Train loss 4.350, Val loss 4.571\n",
            "Ep 8 (Step 006235): Train loss 4.175, Val loss 4.555\n",
            "Ep 8 (Step 006240): Train loss 4.243, Val loss 4.549\n",
            "Ep 8 (Step 006245): Train loss 4.532, Val loss 4.547\n",
            "Ep 8 (Step 006250): Train loss 4.611, Val loss 4.555\n",
            "Ep 8 (Step 006255): Train loss 4.611, Val loss 4.575\n",
            "Ep 8 (Step 006260): Train loss 4.413, Val loss 4.567\n",
            "Ep 8 (Step 006265): Train loss 4.328, Val loss 4.539\n",
            "Ep 8 (Step 006270): Train loss 4.328, Val loss 4.545\n",
            "Ep 8 (Step 006275): Train loss 4.308, Val loss 4.561\n",
            "Ep 8 (Step 006280): Train loss 4.342, Val loss 4.573\n",
            "Ep 8 (Step 006285): Train loss 4.075, Val loss 4.579\n",
            "Ep 8 (Step 006290): Train loss 4.596, Val loss 4.628\n",
            "Ep 8 (Step 006295): Train loss 4.478, Val loss 4.603\n",
            "Ep 8 (Step 006300): Train loss 4.586, Val loss 4.558\n",
            "Ep 8 (Step 006305): Train loss 4.602, Val loss 4.582\n",
            "Ep 8 (Step 006310): Train loss 4.489, Val loss 4.592\n",
            "Ep 8 (Step 006315): Train loss 4.575, Val loss 4.559\n",
            "Ep 8 (Step 006320): Train loss 4.320, Val loss 4.547\n",
            "Ep 8 (Step 006325): Train loss 4.391, Val loss 4.560\n",
            "Ep 8 (Step 006330): Train loss 4.647, Val loss 4.578\n",
            "Ep 8 (Step 006335): Train loss 4.280, Val loss 4.543\n",
            "Ep 8 (Step 006340): Train loss 4.578, Val loss 4.554\n",
            "Ep 8 (Step 006345): Train loss 4.214, Val loss 4.545\n",
            "Ep 8 (Step 006350): Train loss 4.591, Val loss 4.550\n",
            "Ep 8 (Step 006355): Train loss 4.617, Val loss 4.552\n",
            "Ep 8 (Step 006360): Train loss 4.304, Val loss 4.573\n",
            "Ep 8 (Step 006365): Train loss 4.285, Val loss 4.565\n",
            "Ep 8 (Step 006370): Train loss 4.581, Val loss 4.598\n",
            "Ep 8 (Step 006375): Train loss 4.643, Val loss 4.586\n",
            "Ep 8 (Step 006380): Train loss 4.201, Val loss 4.549\n",
            "Ep 8 (Step 006385): Train loss 4.474, Val loss 4.548\n",
            "Ep 8 (Step 006390): Train loss 4.019, Val loss 4.548\n",
            "Ep 8 (Step 006395): Train loss 4.496, Val loss 4.544\n",
            "Ep 8 (Step 006400): Train loss 4.440, Val loss 4.542\n",
            "Ep 8 (Step 006405): Train loss 4.512, Val loss 4.549\n",
            "Ep 8 (Step 006410): Train loss 3.905, Val loss 4.539\n",
            "Ep 8 (Step 006415): Train loss 4.622, Val loss 4.536\n",
            "Ep 8 (Step 006420): Train loss 4.275, Val loss 4.542\n",
            "Ep 8 (Step 006425): Train loss 4.234, Val loss 4.543\n",
            "Ep 8 (Step 006430): Train loss 4.018, Val loss 4.574\n",
            "Ep 8 (Step 006435): Train loss 4.391, Val loss 4.588\n",
            "Ep 8 (Step 006440): Train loss 4.279, Val loss 4.582\n",
            "Ep 8 (Step 006445): Train loss 4.088, Val loss 4.589\n",
            "Ep 8 (Step 006450): Train loss 4.537, Val loss 4.576\n",
            "Ep 8 (Step 006455): Train loss 4.329, Val loss 4.556\n",
            "Ep 8 (Step 006460): Train loss 4.425, Val loss 4.548\n",
            "Ep 8 (Step 006465): Train loss 4.200, Val loss 4.543\n",
            "Ep 8 (Step 006470): Train loss 4.634, Val loss 4.545\n",
            "Ep 8 (Step 006475): Train loss 4.435, Val loss 4.528\n",
            "Ep 8 (Step 006480): Train loss 4.550, Val loss 4.517\n",
            "Ep 8 (Step 006485): Train loss 4.186, Val loss 4.511\n",
            "Ep 8 (Step 006490): Train loss 4.477, Val loss 4.529\n",
            "Ep 8 (Step 006495): Train loss 4.083, Val loss 4.550\n",
            "Ep 8 (Step 006500): Train loss 4.497, Val loss 4.540\n",
            "Ep 8 (Step 006505): Train loss 3.969, Val loss 4.543\n",
            "Ep 8 (Step 006510): Train loss 4.347, Val loss 4.548\n",
            "Ep 8 (Step 006515): Train loss 4.379, Val loss 4.534\n",
            "Ep 8 (Step 006520): Train loss 4.483, Val loss 4.521\n",
            "Ep 8 (Step 006525): Train loss 4.504, Val loss 4.513\n",
            "Ep 8 (Step 006530): Train loss 4.371, Val loss 4.526\n",
            "Ep 8 (Step 006535): Train loss 3.965, Val loss 4.522\n",
            "Ep 8 (Step 006540): Train loss 4.496, Val loss 4.529\n",
            "Ep 8 (Step 006545): Train loss 4.678, Val loss 4.522\n",
            "Ep 8 (Step 006550): Train loss 4.276, Val loss 4.532\n",
            "Ep 8 (Step 006555): Train loss 4.424, Val loss 4.546\n",
            "Ep 8 (Step 006560): Train loss 4.429, Val loss 4.547\n",
            "Ep 8 (Step 006565): Train loss 4.243, Val loss 4.545\n",
            "Ep 8 (Step 006570): Train loss 4.374, Val loss 4.539\n",
            "Ep 8 (Step 006575): Train loss 4.384, Val loss 4.553\n",
            "Ep 8 (Step 006580): Train loss 4.244, Val loss 4.540\n",
            "Ep 8 (Step 006585): Train loss 4.405, Val loss 4.529\n",
            "Ep 8 (Step 006590): Train loss 4.380, Val loss 4.520\n",
            "Ep 8 (Step 006595): Train loss 4.209, Val loss 4.527\n",
            "Ep 8 (Step 006600): Train loss 4.607, Val loss 4.533\n",
            "Ep 8 (Step 006605): Train loss 3.957, Val loss 4.504\n",
            "Ep 8 (Step 006610): Train loss 4.025, Val loss 4.500\n",
            "Ep 8 (Step 006615): Train loss 4.294, Val loss 4.501\n",
            "Ep 8 (Step 006620): Train loss 4.464, Val loss 4.499\n",
            "Ep 8 (Step 006625): Train loss 4.286, Val loss 4.493\n",
            "Ep 8 (Step 006630): Train loss 4.062, Val loss 4.509\n",
            "Ep 8 (Step 006635): Train loss 4.398, Val loss 4.517\n",
            "Ep 8 (Step 006640): Train loss 4.197, Val loss 4.527\n",
            "Ep 8 (Step 006645): Train loss 4.455, Val loss 4.503\n",
            "Ep 8 (Step 006650): Train loss 4.242, Val loss 4.493\n",
            "Ep 8 (Step 006655): Train loss 4.434, Val loss 4.496\n",
            "Ep 8 (Step 006660): Train loss 4.061, Val loss 4.502\n",
            "Ep 8 (Step 006665): Train loss 4.381, Val loss 4.512\n",
            "Ep 8 (Step 006670): Train loss 4.284, Val loss 4.494\n",
            "Ep 8 (Step 006675): Train loss 4.397, Val loss 4.497\n",
            "Ep 8 (Step 006680): Train loss 4.434, Val loss 4.501\n",
            "Ep 8 (Step 006685): Train loss 4.360, Val loss 4.514\n",
            "Ep 8 (Step 006690): Train loss 4.404, Val loss 4.502\n",
            "Ep 8 (Step 006695): Train loss 4.540, Val loss 4.520\n",
            "Ep 8 (Step 006700): Train loss 4.480, Val loss 4.503\n",
            "Ep 8 (Step 006705): Train loss 4.589, Val loss 4.516\n",
            "Ep 8 (Step 006710): Train loss 4.411, Val loss 4.515\n",
            "Ep 8 (Step 006715): Train loss 4.233, Val loss 4.526\n",
            "Ep 8 (Step 006720): Train loss 4.268, Val loss 4.548\n",
            "Ep 8 (Step 006725): Train loss 4.137, Val loss 4.538\n",
            "Ep 8 (Step 006730): Train loss 3.941, Val loss 4.525\n",
            "Ep 8 (Step 006735): Train loss 4.420, Val loss 4.510\n",
            "Ep 8 (Step 006740): Train loss 4.332, Val loss 4.494\n",
            "Every effort moves you to be a woman of the the whole, and the the whole the the the whole of the the the the the the other-humard-hum, and the the the the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "GO4pg85a7EpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "d72458ac-bcf5-4186-d51e-f82511862e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcl9JREFUeJzt3Xd8E/UbwPHPJenerA5KW3ahlE0RCoJsRBRBQERluX5sURREmSKiiCggigMcDEEEkT1k703Ze0PLbGlLV3K/P9qmSZvuQlp43q8X2lwud88lTZ/7bkVVVRUhhBBCFEoaawcghBBCiMxJohZCCCEKMUnUQgghRCEmiVoIIYQoxCRRCyGEEIWYJGohhBCiEJNELYQQQhRikqiFEEKIQkwStRBCCFGISaIWQgghCjFJ1EIIIUQ6mzdvpn379vj4+KAoCkuWLMn1MVRVZdKkSVSqVAk7OztKly7N+PHjc30cSdRCFDEXLlxAURQOHjxo7VCEeGzFxMRQo0YNpk+fnudjDBo0iJ9++olJkyZx4sQJli5dSkhISK6Po8tzBEKIPFMUJcvnR40axejRox9NMEKIDNq2bUvbtm0zfT4+Pp4RI0Ywb9487t27R7Vq1Zg4cSJNmzYF4Pjx48yYMYMjR45QuXJlAMqWLZunWCRRC2EF169fN/78559/MnLkSE6ePGnc5uzsbI2whBA51L9/f44dO8b8+fPx8fFh8eLFtGnThrCwMCpWrMi///5LuXLlWLZsGW3atEFVVVq0aMEXX3xBsWLFcnUuqfoWwgq8vLyM/9zc3FAUxfi4VKlSTJ48GV9fX+zs7KhZsyarVq3K9Fh6vZ7evXsTGBjIpUuXAPjnn3+oXbs29vb2lCtXjjFjxpCUlGR8jaIo/PTTT7z44os4OjpSsWJFli5danz+7t27dO/enZIlS+Lg4EDFihWZNWtWpjH89ddfBAcH4+DgQPHixWnRogUxMTHG53/66SeqVKmCvb09gYGBfPfdd2avv3z5Ml26dMHd3Z1ixYrxwgsvcOHCBePzPXv2pEOHDkyaNAlvb2+KFy9Ov379SExMzPF7LkRBuXTpErNmzWLhwoU0btyY8uXL8/7779OoUSPj9+TcuXNcvHiRhQsX8ttvvzF79mz27dvHSy+9lPsTqkIIq5o1a5bq5uZmfDx58mTV1dVVnTdvnnrixAn1gw8+UG1sbNRTp06pqqqq58+fVwH1wIEDalxcnPriiy+qtWrVUiMiIlRVVdXNmzerrq6u6uzZs9WzZ8+qa9asUQMCAtTRo0cbzwGovr6+6ty5c9XTp0+rAwcOVJ2dndXbt2+rqqqq/fr1U2vWrKnu2bNHPX/+vLp27Vp16dKlFuO/du2aqtPp1MmTJ6vnz59XDx8+rE6fPl29f/++qqqq+scff6je3t7qokWL1HPnzqmLFi1SixUrps6ePVtVVVVNSEhQq1Spovbu3Vs9fPiweuzYMfWVV15RK1eurMbHx6uqqqo9evRQXV1d1XfeeUc9fvy4+u+//6qOjo7qzJkzC/bDEMICQF28eLHx8bJly1RAdXJyMvun0+nULl26qKqqqm+++aYKqCdPnjS+bt++fSqgnjhxInfnL5CrEELkWfpE7ePjo44fP95sn3r16ql9+/ZVVTUtUW/ZskVt3ry52qhRI/XevXvGfZs3b65+9tlnZq///fffVW9vb+NjQP3444+Nj6Ojo1VAXblypaqqqtq+fXu1V69eOYo/9Y/PhQsXLD5fvnx5de7cuWbbxo0bpzZo0MAYW+XKlVWDwWB8Pj4+XnVwcFBXr16tqmpyovb391eTkpKM+3Tu3Fnt2rVrjmIUIj/SJ+r58+erWq1WPXHihHr69Gmzf9evX1dVVVVHjhyp6nQ6s+PExsaqgLpmzZpcnV/aqIUoRKKiorh27RqhoaFm20NDQzl06JDZtm7duuHr68t///2Hg4ODcfuhQ4fYtm2b2TAQvV5PXFwcsbGxODo6AlC9enXj805OTri6uhIREQHA//73Pzp16sT+/ftp1aoVHTp0oGHDhhZjrlGjBs2bNyc4OJjWrVvTqlUrXnrpJTw8PIiJieHs2bP06dOHN9980/iapKQk3NzcjPGeOXMGFxcXs+PGxcVx9uxZ4+OgoCC0Wq3xsbe3N2FhYVm8m0I8HLVq1UKv1xMREUHjxo0t7hMaGkpSUhJnz56lfPnyAJw6dQoAf3//XJ1PErUQRdSzzz7LH3/8wY4dO2jWrJlxe3R0NGPGjKFjx44ZXmNvb2/82cbGxuw5RVEwGAxAco/XixcvsmLFCtauXUvz5s3p168fkyZNynBMrVbL2rVr2b59O2vWrGHq1KmMGDGCXbt2GW8KfvzxR+rXr5/hdanx1qlThzlz5mQ4dsmSJXMUrxAFLTo6mjNnzhgfnz9/noMHD1KsWDEqVapE9+7def311/nqq6+oVasWN2/eZP369VSvXp127drRokULateuTe/evZkyZQoGg4F+/frRsmVLKlWqlLtg8l0nIITIl5xWfffr109VVfM26m+//VZ1cnJSN27caNy3YcOGau/evbM8J+mq8lRVVd3c3NRZs2ZZ3P/7779XXVxccnQ9SUlJaunSpdWvvvrKeD1jx47NdP+ZM2eqHh4eamRkZKb79OjRQ33hhRfMtg0aNEht0qRJjmISIrc2bNigAhn+9ejRQ1XV5L4VI0eOVAMCAlQbGxvV29tbffHFF9XDhw8bj3H16lW1Y8eOqrOzs+rp6an27NnT2A8kN6RELUQhM3ToUEaNGkX58uWpWbMms2bN4uDBgxZLnAMGDECv1/Pcc8+xcuVKGjVqxMiRI3nuuefw8/PjpZdeQqPRcOjQIY4cOcKnn36aoxhGjhxJnTp1CAoKIj4+nmXLllGlShWL++7atYv169fTqlUrSpUqxa5du7h586Zx/zFjxjBw4EDc3Nxo06YN8fHx7N27l7t37zJkyBC6d+/Ol19+yQsvvMDYsWPx9fXl4sWL/P3333zwwQf4+vrm/c0UIo+aNm2KqqqZPm9jY8OYMWMYM2ZMpvv4+PiwaNGifMciiVqIQmbgwIFERkby3nvvERERQdWqVVm6dCkVK1a0uP/gwYMxGAw8++yzrFq1itatW7Ns2TLGjh3LxIkTsbGxITAwkDfeeCPHMdja2jJ8+HAuXLiAg4MDjRs3Zv78+Rb3dXV1ZfPmzUyZMoWoqCj8/f356quvjJNFvPHGGzg6OvLll18ydOhQnJycCA4OZvDgwQA4OjqyefNmPvzwQzp27Mj9+/cpXbo0zZs3x9XVNXdvnhCPIUXN6pZBCCGEEFYlE54IIYQQhZgkaiGEEKIQk0QthBBCFGKSqIUQQohCTBK1EEIIUYg9cYl6+vTpBAQEYG9vT/369dm9e3eW+y9cuJDAwEDs7e0JDg5mxYoVjyjSvMvNNc6ePRtFUcz+mc5eVdhs3ryZ9u3b4+Pjg6IoLFmyJNvXbNy4kdq1a2NnZ0eFChWYPXv2Q48zP3J7jRs3bszwGSqKwo0bNx5NwLk0YcIE6tWrh4uLC6VKlaJDhw5mS3xmpih9F/NyjUXtuzhjxgyqV6+Oq6srrq6uNGjQgJUrV2b5mqL0GULur/FhfYZPVKL+888/GTJkCKNGjWL//v3UqFGD1q1bG+c3Tm/79u1069aNPn36cODAATp06ECHDh04cuTII44853J7jZA8Dvb69evGfxcvXnyEEedOTEwMNWrUYPr06Tna//z587Rr145nnnmGgwcPMnjwYN544w1Wr179kCPNu9xeY6qTJ0+afY6lSpV6SBHmz6ZNm+jXrx87d+5k7dq1JCYm0qpVK7NlMdMrat/FvFwjFK3voq+vL59//jn79u1j7969NGvWjBdeeIGjR49a3L+ofYaQ+2uEh/QZFshca0VESEiIcRpGVVVVvV6v+vj4qBMmTLC4f5cuXdR27dqZbatfv7769ttvP9Q48yO315h++sqiBAvTYKb3wQcfqEFBQWbbunbtqrZu3fohRlZwcnKNqVMd3r1795HEVNAiIiJUQN20aVOm+xTF76KpnFxjUf4upvLw8FB/+ukni88V9c8wVVbX+LA+wyemRJ2QkMC+ffto0aKFcZtGo6FFixbs2LHD4mt27Nhhtj9A69atM93f2vJyjZA8+by/vz9lypTJ9m6xqClqn2F+1KxZE29vb1q2bMm2bdusHU6ORUZGAlCsWLFM9ynqn2NOrhGK7ndRr9czf/58YmJiaNCggcV9ivpnmJNrhIfzGT4xifrWrVvo9Xo8PT3Ntnt6embalnfjxo1c7W9tebnGypUr88svv/DPP//wxx9/YDAYaNiwIVeuXHkUIT90mX2GUVFRPHjwwEpRFSxvb2++//57Fi1axKJFiyhTpgxNmzZl//791g4tWwaDgcGDBxMaGkq1atUy3a+ofRdN5fQai+J3MSwsDGdnZ+zs7HjnnXdYvHgxVatWtbhvUf0Mc3OND+szlLm+n3ANGjQwuzts2LAhVapU4YcffmDcuHFWjEzkVOXKlalcubLxccOGDTl79ixff/01v//+uxUjy16/fv04cuQIW7dutXYoD01Or7EofhcrV67MwYMHiYyM5K+//qJHjx5s2rQp00RWFOXmGh/WZ/jEJOoSJUqg1WoJDw832x4eHo6Xl5fF13h5eeVqf2vLyzWmZ2NjQ61atczWYS3KMvsMXV1dcXBwsFJUD19ISEihT379+/dn2bJlbN68OdsVsoradzFVbq4xvaLwXbS1taVChQoA1KlThz179vDNN9/www8/ZNi3qH6GubnG9ArqM3xiqr5tbW2pU6cO69evN24zGAysX78+0/aGBg0amO0PsHbt2izbJ6wpL9eYnl6vJywsDG9v74cV5iNV1D7DgnLw4MFC+xmqqkr//v1ZvHgx//33H2XLls32NUXtc8zLNaZXFL+LBoOB+Ph4i88Vtc8wM1ldY3oF9hkWePe0Qmz+/PmqnZ2dOnv2bPXYsWPqW2+9pbq7u6s3btxQVVVVX3vtNXXYsGHG/bdt26bqdDp10qRJ6vHjx9VRo0apNjY2alhYmLUuIVu5vcYxY8aoq1evVs+ePavu27dPffnll1V7e3v16NGj1rqELN2/f189cOCAeuDAARVQJ0+erB44cEC9ePGiqqqqOmzYMPW1114z7n/u3DnV0dFRHTp0qHr8+HF1+vTpqlarVVetWmWtS8hWbq/x66+/VpcsWaKePn1aDQsLUwcNGqRqNBp13bp11rqELP3vf/9T3dzc1I0bN6rXr183/ouNjTXuU9S/i3m5xqL2XRw2bJi6adMm9fz58+rhw4fVYcOGqYqiqGvWrFFVteh/hqqa+2t8WJ/hE5WoVVVVp06dqvr5+am2trZqSEiIunPnTuNzTZo0UXv06GG2/4IFC9RKlSqptra2alBQkLp8+fJHHHHu5eYaBw8ebNzX09NTffbZZ9X9+/dbIeqcSR2KlP5f6jX16NFDbdKkSYbX1KxZU7W1tVXLlSunzpo165HHnRu5vcaJEyeq5cuXV+3t7dVixYqpTZs2Vf/77z/rBJ8Dlq4NMPtcivp3MS/XWNS+i71791b9/f1VW1tbtWTJkmrz5s2NCUxVi/5nqKq5v8aH9RnKetRCCCFEIfbEtFELIYQQRZEkaiGEEKIQk0QthBBCFGKSqIUQQohCTBK1EEIIUYhJohZCCCEKMUnUQgghRCEmidpEfHw8o0ePzvH0cEXR436Nj/v1gVzj40Ku8fHwKK5RJjwxERUVhZubG5GRkbi6ulo7nIficb/Gx/36QK7xcSHX+Hh4FNcoJWohhBCiEJNELYQQQhRiRXo96qSkJA4cOICnpycaTf7vOe7fvw/A1atXiYqKyvfxCqPH/Rof9+sDucbHhVzj4yGv12gwGAgPD6dWrVrodFmn4iLdRr1nzx5CQkKsHYYQQgiRJ7t376ZevXpZ7lOkS9Senp5A8oUWpcXVhRBCPNmuX79OSEiIMY9lpUgn6tTqbm9vb3x9fa0cjRBCCJE7OWm2lc5kQgghRCEmiVoIIYQoxCRRCyGEEIVYkW6jFkKIgqbX60lMTLR2GKKIs7GxQavVFsixJFGnCLsSybjlx/Ar5sikzjWsHY4Q4hFTVZUbN25w7949a4ciHhPu7u54eXmhKEq+jiOJOkVUXCK7z98hMlbupIV4EqUm6VKlSuHo6JjvP67iyaWqKrGxsURERADke/iwJOoUqV9JQ9Gd/0UIkUd6vd6YpIsXL27tcMRjwMHBAYCIiAhKlSqVr2pwSdQpnO4cZontx9yJLQ00sXY4QohHKLVN2tHR0cqRiMdJ6u9TYmKiJOqCoEuIIUhzjvOGJGuHIoSwEqnuFgWpoH6fZHhWCkWT/IYqGKwciRBCWFdAQABTpkzJ8f4bN25EUZSH3hFv9uzZuLu7P9RzFEaSqI1SErU0UQshighFUbL8N3r06Dwdd8+ePbz11ls53r9hw4Zcv34dNze3PJ1PZE2qvlMoKfOtKkimFkIUDdevXzf+/OeffzJy5EhOnjxp3Obs7Gz8WVVV9Hp9tksqApQsWTJXcdja2uLl5ZWr14ickxJ1irS2BEnUQoiiwcvLy/jPzc0NRVGMj0+cOIGLiwsrV66kTp062NnZsXXrVs6ePcsLL7yAp6cnzs7O1KtXj3Xr1pkdN33Vt6Io/PTTT7z44os4OjpSsWJFli5danw+fdV3ahX16tWrqVKlCs7OzrRp08bsxiIpKYmBAwfi7u5O8eLF+fDDD+nRowcdOnTI1XswY8YMypcvj62tLZUrV+b33383PqeqKqNHj8bPzw87Ozt8fHwYOHCg8fnvvvuOihUrYm9vj6enJy+99FKuzv2oSKJOpaS2UQshxONj2LBhfP755xw/fpzq1asTHR3Ns88+y/r16zlw4ABt2rShffv2XLp0KcvjjBkzhi5dunD48GGeffZZunfvzp07dzLdPzY2lkmTJvH777+zefNmLl26xPvvv298fuLEicyZM4dZs2axbds2oqKiWLJkSa6ubfHixQwaNIj33nuPI0eO8Pbbb9OrVy82bNgAwKJFi/j666/54YcfOH36NEuWLCE4OBiAvXv3MnDgQMaOHcvJkydZtWoVTz/9dK7O/6hI1bdR6j2LlKiFEMmlsQeJequc28FGW2A9hseOHUvLli2Nj4sVK0aNGmmzL44bN47FixezdOlS+vfvn+lxevbsSbdu3QD47LPP+Pbbb9m9ezdt2rSxuH9iYiLff/895cuXB6B///6MHTvW+PzUqVMZPnw4L774IgDTpk1jxYoVubq2SZMm0bNnT/r27QvAkCFD2LlzJ5MmTeKZZ57h0qVLeHl50aJFC2xsbPDz8yMkJASAS5cu4eTkxHPPPYeLiwv+/v7UqlUrV+d/VCRRp1CMJWpJ1EIIeJCop+rI1VY597GxrXG0LZg/z3Xr1jV7HB0dzejRo1m+fDnXr18nKSmJBw8eZFuirl69uvFnJycnXF1djTNvWeLo6GhM0pA8O1fq/pGRkYSHhxuTJoBWq6VOnToYDDkfeXP8+PEMnd5CQ0P55ptvAOjcuTNTpkyhXLlytGnThmeffZb27duj0+lo2bIl/v7+xufatGljrNovbKTqO4UkaiHE48jJycns8fvvv8/ixYv57LPP2LJlCwcPHiQ4OJiEhIQsj2NjY2P2WFGULJOqpf3VRzzzY5kyZTh58iTfffcdDg4O9O3bl6effprExERcXFzYv38/8+bNw9vbm5EjR1KjRo1COde7lKhTSKIWQphysNFybGxrq537Ydm2bRs9e/Y0VjlHR0dz4cKFh3Y+S9zc3PD09GTPnj3GdmG9Xs/+/fupWbNmjo9TpUoVtm3bRo8ePYzbtm3bRtWqVY2PHRwcaN++Pe3bt6dfv34EBgYSFhZG7dq10el0tGjRghYtWjBq1Cjc3d3577//6NixY4Fda0GQRJ0qNVHLXN9CCJJv3guq+rkwqVixIn///Tft27dHURQ++eSTXFU3F5QBAwYwYcIEKlSoQGBgIFOnTuXu3bu5apsfOnQoXbp0oVatWrRo0YJ///2Xv//+29iLffbs2ej1eurXr4+joyN//PEHDg4O+Pv7s2zZMs6dO8fTTz+Nh4cHK1aswGAwULly5Yd1yXn2+P0W5pEincmEEE+AyZMn07t3bxo2bEiJEiX48MMPiYqKeuRxfPjhh9y4cYPXX38drVbLW2+9RevWrXM1J3aHDh345ptvmDRpEoMGDaJs2bLMmjWLpk2bAsnLTH7++ecMGTIEvV5PcHAw//77L8WLF8fd3Z2///6b0aNHExcXR8WKFZk3bx5BQUEP6YrzTlEfdaNBAbpy5QplypTh8uXL+Pr65utYJ08eZ9Pv49DbufK/T74voAiFEEVBXFwc58+fp2zZstjb21s7nCeSwWCgSpUqdOnShXHjxlk7nAKR1e9VbvKXlKhTJDr78FlSd0o52PE/awcjhBCPuYsXL7JmzRqaNGlCfHw806ZN4/z587zyyivWDq3QkV7fKTQp7SJFtnpBCCGKEI1Gw+zZs6lXrx6hoaGEhYWxbt06qlSpYu3QCh0pUadQDAmU5iZuBgdrhyKEEI+9MmXKsG3bNmuHUSRIok5hf+802+wHcVPvAXS1djhCCCEEIFXfaRQtcaoN8dhkv68QQgjxiEiJOkViiSoExv+Kh6MNB6wdjBBCCJFCStQpNClj7KUzmRBCiMJEErVRSq9vydRCCCEKEan6TmETfZVfbL4gXnUEWlk7HCGEEAKQErWRJjGGZtqDPEWYtUMRQohHqmnTpgwePNj4OCAggClTpmT5GkVRWLJkSb7PXVDHycro0aNztdhHYSOJOoWiSX4rZFEOIURR0b59e9q0aWPxuS1btqAoCocPH871cffs2ZNhnef8yixZXr9+nbZt2xbouR43kqiNZJlLIUTR0qdPH9auXcuVK1cyPDdr1izq1q1L9erVc33ckiVL4ujoWBAhZsvLyws7O7tHcq6iShJ1Co2SUqKWRC2EKCKee+45SpYsyezZs822R0dHs3DhQvr06cPt27fp1q0bpUuXxtHRkeDgYObNm5flcdNXfZ8+fZqnn34ae3t7qlatytq1azO85sMPP6RSpUo4OjpSrlw5PvnkExITE4Hk5SbHjBnDoUOHUBQFRVGMMaev+g4LC6NZs2Y4ODhQvHhx3nrrLaKjo43P9+zZkw4dOjBp0iS8vb0pXrw4/fr1M54rJwwGA2PHjsXX1xc7Oztq1qzJqlWrjM8nJCTQv39/vL29sbe3x9/fnwkTJgCgqiqjR4/Gz88POzs7fHx8GDhwYI7PnRfSmSyFopEStRDCgoSY3L9GawfalD+v+iTQx4OiARuTKYozO66tU45Po9PpeP3115k9ezYjRowwruW8cOFC9Ho93bp1Izo6mjp16vDhhx/i6urK8uXLee211yhfvjwhISHZnsNgMNCxY0c8PT3ZtWsXkZGRZu3ZqVxcXJg9ezY+Pj6EhYXx5ptv4uLiwgcffEDXrl05cuQIq1atMq4V7ebmluEYMTExtG7dmgYNGrBnzx4iIiJ444036N+/v9nNyIYNG/D29mbDhg2cOXOGrl27UrNmTd58880cvW/ffPMNX331FT/88AO1atXil19+4fnnn+fo0aNUrFiRb7/9lqVLl7JgwQL8/Py4fPkyly9fBmDRokV8/fXXzJ8/n6CgIG7cuMGhQ4dydN68kkRtpJj8VwghUnzmk/vXdJ4NQS8m/3ziX1jYE/wbQa/laftMCYbY2xlfOzoyV6fq3bs3X375JZs2bTKuwzxr1iw6deqEm5sbbm5uvP/++8b9BwwYwOrVq1mwYEGOEvW6des4ceIEq1evxscn+b347LPPMrQrf/zxx8afAwICeP/995k/fz4ffPABDg4OODs7o9Pp8PLyyvRcc+fOJS4ujt9++w0np+QblmnTptG+fXsmTpyIp6cnAB4eHkybNg2tVktgYCDt2rVj/fr1OU7UkyZN4sMPP+Tll18GYOLEiWzYsIEpU6Ywffp0Ll26RMWKFWnUqBGKouDv72987aVLl/Dy8qJFixbY2Njg5+eXo/cxP6TqO5UiJWohRNETGBhIw4YN+eWXXwA4c+YMW7ZsoU+fPgDo9XrGjRtHcHAwxYoVw9nZmdWrV3Pp0qUcHf/48eOUKVPGmKQBGjRokGG/P//8k9DQULy8vHB2dubjjz/O8TlMz1WjRg1jkgYIDQ3FYDBw8uRJ47agoCC0Wq3xsbe3NxERETk6R1RUFNeuXSM0NNRse2hoKMePHweSq9cPHjxI5cqVGThwIGvWrDHu17lzZx48eEC5cuV48803Wbx4MUlJSbm6ztySEnWK1F7fMjeZEMLMR9dy/xqtSeeowPbJx1DSlYsGF9xQ0D59+jBgwACmT5/OrFmzKF++PE2aNAHgyy+/5JtvvmHKlCkEBwfj5OTE4MGDSUhIKLDz79ixg+7duzNmzBhat26Nm5sb8+fP56uvviqwc5iysTFfk0FRFAwGQ4Edv3bt2pw/f56VK1eybt06unTpQosWLfjrr78oU6YMJ0+eZN26daxdu5a+ffsaazTSx1VQpESdQkn3fyGEAJLbjHP7T2tSBtLqkrfZOOTsuHnQpUsXNBoNc+fO5bfffqN3797G9upt27bxwgsv8Oqrr1KjRg3KlSvHqVOncnzsKlWqcPnyZa5fv27ctnPnTrN9tm/fjr+/PyNGjKBu3bpUrFiRixcvml+urS16vT7bcx06dIiYmLT2+23btqHRaKhcuXKOY86Kq6srPj4+GZbY3LZtG1WrVjXbr2vXrvz444/8+eefLFq0iDt37gDg4OBA+/bt+fbbb9m4cSM7duwgLOzhzcEhJeoUiia5GkWqvoUQRY2zszNdu3Zl+PDhREVF0bNnT+NzFStW5K+//mL79u14eHgwefJkwsPDzZJSVlq0aEGlSpXo0aMHX375JVFRUYwYMcJsn4oVK3Lp0iXmz59PvXr1WL58OYsXLzbbJyAggPPnz3Pw4EF8fX1xcXHJMCyre/fujBo1ih49ejB69Ghu3rzJgAEDeO2114zt0wVh6NChjBo1ivLly1OzZk1mzZrFwYMHmTNnDgCTJ0/G29ubWrVqodFoWLhwIV5eXri7uzN79mz0ej3169fH0dGRP/74AwcHB7N27IImJeoUqXefGknUQogiqE+fPty9e5fWrVubtSd//PHH1K5dm9atW9O0aVO8vLzo0KFDjo+r0WhYvHgxDx48ICQkhDfeeIPx48eb7fP888/z7rvv0r9/f2rWrMn27dv55JNPzPbp1KkTbdq04ZlnnqFkyZIWh4g5OjqyevVq7ty5Q7169XjppZdo3rw506ZNy92bkY2BAwcyZMgQ3nvvPYKDg1m1ahVLly6lYsWKQHIP9i+++IK6detSr149Lly4wIoVK9BoNLi7u/Pjjz8SGhpK9erVWbduHf/++y/Fixcv0BhNKapadKfiunLlCmXKlOHy5cv4+vrm61i3rp6lxI+1iVd12I2x0BNTCPHYiouL4/z585QtWxZ7e3trhyMeE1n9XuUmf0nVdwrVxpFF+kYkoaOrtYMRQgghUli16luv1/PJJ59QtmxZHBwcKF++POPGjcMqhXyHYryX2JcPEwt2flshhBAiP6xaop44cSIzZszg119/JSgoiL1799KrVy/c3Nwe+pRs6WlMunurqmpssxZCCCGsyaqJevv27bzwwgu0a9cOSO4VOG/ePHbv3v3IY1EAe+IBUFXj/CdCCCGEVVm16rthw4asX7/eOKbv0KFDbN26NdMlz+Lj44mKijL+u3//foHFoomJ4IR9L47Z9ZZ+30IIIQoNq5aohw0bRlRUFIGBgWi1WvR6PePHj6d79+4W958wYQJjxox5KLGkLsqhUVSSVBWZ+kSIJ08RHgQjCqGC+n2yaol6wYIFzJkzh7lz57J//35+/fVXJk2axK+//mpx/+HDhxMZGWn8d+zYsYILxrEkVeN+oUrcL1KiFuIJkzr1Y2xsrJUjEY+T1N+n/E4tatUS9dChQxk2bJhxBZPg4GAuXrzIhAkT6NGjR4b97ezszGayiYqKKrhgNBpiSR7nZpC7aiGeKFqtFnd3d+PCDo6OjtKhVOSZqqrExsYSERGBu7u72QIieWHVRB0bG4tGY16o12q1BTq5ek6Z9/p+5KcXQlhZ6vKLOV2FSYjsuLu7Z7msZ05ZNVG3b9+e8ePH4+fnR1BQEAcOHGDy5Mn07t37kceiJMbwpe775N7ehpZA/u6AhBBFi6IoeHt7U6pUKRITE60djijibGxs8l2STmXVRD116lQ++eQT+vbtS0REBD4+Prz99tuMHDnykcei6BPprNsMwAMrlOiFEIWDVqstsD+wQhQEqyZqFxcXpkyZwpQpU6wZBoBZe5SKJGohhBCFg6yelUJR0u6gVYM0UgshhCgcJFGnUEzeCYMh68XNhRBCiEdFEnUK86pvKVELIYQoHCRRp1BMitRS9S2EEKKwkESdwrREfSs6zoqRCCGEEGkkUacwTdSf/luAU5MKIYQQ+SCJOoVp1fflOzFWjEQIIYRII4k6hcZ0DlHpTCaEEKKQkESdwnQcteRpIYQQhYUk6lQmbdQamZlMCCFEIWHVKUQLFUXDRn0NAJJs5P5FCCFE4SCJOpVGS8/EDwHwdLDLZmchhBDi0ZCiowWyHrUQQojCQhK1BZKnhRBCFBaSqE0csevNMbtelLa5b+1QhBBCCEDaqM04K8lTh5bxcLByJEIIIUQyKVGb2NRmHY3ip7DibALhUTLftxBCCOuTRG3CpkRZrqil0KNlzs6L1g5HCCGEkERtysXexvizo520CgghhLA+SdQmvPd9wTDdXFyJwd3BJvsXCCGEEA+ZJGoTxcN+4h3dMlyVWOxkdjIhhBCFgGQjE4pjcQCKEUWiXkZTCyGEsD5J1KacSwFQSrlHol4W5hBCCGF9eUrUly9f5sqVK8bHu3fvZvDgwcycObPAArMKZ08ASiiRPEjQkyTJWgghhJXlKVG/8sorbNiwAYAbN27QsmVLdu/ezYgRIxg7dmyBBvhI2bkC4MQDPl1+nGZfbcJgkCpwIYQQ1pOnRH3kyBFCQkIAWLBgAdWqVWP79u3MmTOH2bNnF2R8j5adMwBOxANw6U4ssYl6a0YkhBDiCZenRJ2YmIidXfJSkOvWreP5558HIDAwkOvXrxdcdI+arRMAjorMSiaEEKJwyFOiDgoK4vvvv2fLli2sXbuWNm3aAHDt2jWKFy9eoAE+UrbJJWpnHhg36aXqWwghhBXlKVFPnDiRH374gaZNm9KtWzdq1KgBwNKlS41V4kWSsUQdb9wkiVoIIYQ15WmezKZNm3Lr1i2ioqLw8PAwbn/rrbdwdHQssOAeuZRE7URa1XeSQXp+CyGEsJ48lagfPHhAfHy8MUlfvHiRKVOmcPLkSUqVKlWgAT5SKVXfjiaJWvK0EEIIa8pTon7hhRf47bffALh37x7169fnq6++okOHDsyYMaNAA3ykUkvUipSohRBCFA55StT79++ncePGAPz11194enpy8eJFfvvtN7799tsCDfCRcvYkqngNThl8jZt+2XqBkzfuWzEoIYQQT7I8JerY2FhcXFwAWLNmDR07dkSj0fDUU09x8WIRXsfZty4RXZczLOkt46Zftp2n9ZTNAFy+Eyudy4QQQjxSeUrUFSpUYMmSJVy+fJnVq1fTqlUrACIiInB1dS3QAB81LzcHi9uXH75O4y82MPjPg482ICGEEE+0PCXqkSNH8v777xMQEEBISAgNGjQAkkvXtWrVKtAAHzVnOx0NNEexI8Fs+9T/TgPw76Fr1ghLCCHEEypPifqll17i0qVL7N27l9WrVxu3N2/enK+//rrAgrOK9eOYZzue+prjZpsTkqRTmRBCiEcvT+OoAby8vPDy8jKuouXr61u0JztJdWUPAMUw70AWL4laCCGEFeSpRG0wGBg7dixubm74+/vj7++Pu7s748aNw1DUhzN1nMl39dbwr6GB2ebo+CQrBSSEEOJJlqcS9YgRI/j555/5/PPPCQ0NBWDr1q2MHj2auLg4xo8fX6BBPlIuXvRuVZIvtqwy2xz5INFKAQkhhHiS5SlR//rrr/z000/GVbMAqlevTunSpenbt2/RTtSAvY3W2iEIIYQQQB6rvu/cuUNgYGCG7YGBgdy5cyffQVnV/Ruw6iOm2hThiVuEEEI8NvKUqGvUqMG0adMybJ82bRrVq1fPd1BWlRQHO6fTSrMPDZbb2xP1OW+HP3Ytiv2X7hZUdEIIIZ4wear6/uKLL2jXrh3r1q0zjqHesWMHly9fZsWKFQUa4CPnVga0ttjpE/BRbnNFLZlhl9FLjzL+xeBsD6WqKs9+uwWAA5+0xMPJtsDDFUII8XjLU4m6SZMmnDp1ihdffJF79+5x7949OnbsyNGjR/n9998LOsZHS6MFVx8APLFcjT9n1yUeJOjZfuZWlqVr0+lGb0bHZ7qfEEIIkZk8j6P28fHJ0Gns0KFD/Pzzz8ycOTPfgVmVizfcvYCnchcymdp74PwDrD0WzoBmFXivVWWL++hVmRdcCCFE/uSpRF2Qrl69yquvvkrx4sVxcHAgODiYvXv3WjcoFy8ASiu3Mt1l7bFwAH7Zej7TfYr6kHIhhBDWZ9VEfffuXUJDQ7GxsWHlypUcO3aMr776Cg8PD2uGBV7J7c81NWdordmND5kn7KzKzFKiFkIIkV95rvouCBMnTqRMmTLMmjXLuK1s2bJWjCiFZzUA2ml30067m0RVS8V4y23vsQl67sQkUMxCRzG9Pi1RKw8nUiGEEI+5XCXqjh07Zvn8vXv3cnXypUuX0rp1azp37symTZuME6a8+eabuTpOgUvpTJbKRtFnufvAeQf44436GbZLiVoIIUR+5SpRu7m5Zfv866+/nuPjnTt3jhkzZjBkyBA++ugj9uzZw8CBA7G1taVHjx4Z9o+Pjyc+Pq339P379zPsUyBcS5s9vKc6Zbn71jOWq8ZNe31LyhZCCJEXuUrUplXUBcFgMFC3bl0+++wzAGrVqsWRI0f4/vvvLSbqCRMmMGbMmAKNwSLHYlCpLZxaCYC7EoMWPXoyn1o0PkmPrVbDqKVHcbbT8UGbQAwmJWrTpC2EEELklFU7k3l7e1O1alWzbVWqVOHSpUsW9x8+fDiRkZHGf8eOHXt4wb0yn3Jxf2BQk1uXPYjOcvdxy45xMvw+v+24yHcbz6I3qCQZJFELIYTIH6t2JgsNDeXkyZNm206dOoW/v7/F/e3s7LCzszM+joqKeqjx9dauRKMkJ1gP5T631Myr/v/YeYnGFdNmMTOoKgaT5GyQ9mohhBB5YNUS9bvvvsvOnTv57LPPOHPmDHPnzmXmzJn069fPmmEZ+SkRxp+H6v7ElqyXuoyIijP+rDeoZqVoKVELIYTIC6sm6nr16rF48WLmzZtHtWrVGDduHFOmTKF79+7WDMso9KmGxp9bafexw64/fkq4xX01CsQnpc1wYlBVs17fUqIWQgiRF1afmey5554jLCyMuLg4jh8/bv2hWSbKPzcEAp8zPi6u3KeHdg0NNUcy7GtQ4dPlx42P9Qbzqu9d55PnDd9+9hav/byLC7diHmLkQgghHhdWT9SFXvlnzB720a1kru1nvKJdn+XLDCpmncm+WJXcFv/Kj7vYcvoWg/48WOChCiGEePxIos5OnV7Q5EOo0t5sc2/tyixfFpeo50xE5j3FwyPjzB6rFqrG4xL1bDtzi4QkmTRcCCGeVJKos6PRwjMfQYcZZpsDtLfQkHkC7fLDDgbMO2C2LTI2rTOaVpM2qejJG/epN34dv+24YLb/sEWH6f7TLsYte4jD0IQQQhRqkqhzys6FQ2Ve5apaHACdmkB9zfFMd794OzbDthpj1xh/1pi880P/OsSt6ARG/nPUbP8lB68B8PvOi/mJXAghRBEmiToXYpqMITR+Kv/qnwLgu5DbeT7W5TsPjD/fiUnId2xCCCEeT5Koc6FhhRLM6F6b+s06AOBx+Ce8yXuyjktMXuwjOj6pIMITQgjxGJJEnUttg70p1eQtKFEZDHqqac7n+VipCTo6Lv+JWlVV5u++xL6Ld/N9LCGEEIWHJOq80Gih2zz44Bw1vO0ZqpuPPfG4EEsN5QyOxGV/DOBBgh5VNZ8TPK+2n73NsL/D6DRje76PJYQQovCw6lzfRVrx8mDQ8/bdydjo4uinW2p86h99QwYl9s/2EC9+t43S7g45Ot0/B68S6OVKZS8Xi8+fvPGQlvwUQghhVVKizg+NlgNeL2XY/IJ2O69q12b78lvRCRy6Eml87Gir5cSNKH7aco4kvfnQr0HzD9J6yuZMj5VkkLHWQgjxOJISdT5t8BvAwHP1mWL7HccNfvTSrQbgU5tZ7DRU4Yzqm+NjxSboaTNlCwCZTQ3+IEGPg23GdbET9TKXuBBCPI6kRJ1PBlXlBsV5OeETxiT1IDTuG+NzU22m5fm441dYHqN9PfKB2eMjVyN55cedHLiU1ols3bFwjlyNJCou69W+MnM3JsHiTGnWsOfCHf7ad8XaYQghhNVIos6n9PnsKiVZqa8HQBXNJTppMq+uzouEdFXinWZsZ/vZ26w7nrYk5xu/7eW5qVt59pstuT7+qiPXqTVuLZ+vPJHpPr/vvEiDCeuznCI1P1RVNd4odP5+B+8vPMTBy/ceyrmEEKKwk0SdT5ZKnh8kvm382V2Jpr5ynP7axShZTDmaU/GJ5seIz2Ie8Ct300rfMfFJjF56lF3nsh73Pfbf5OlKf9h8LtN9PllyhOuRcYxYHJaTkHMlUW+g7TdbePO3fWbbL93JONNbfqmqyuU7sYWm9kAIISyRRJ1PlkZW3ceRFfoQ9KpCmKEsjbWHed9mIfWUk/k+X/oSdU5N23CG2dsv0HXmziz3UxTF7LGqqhy+co/YhIxjvR+kTNhSkPZfvMuJG/dZd9zyut8Facy/x2j8xQaZolUIUahJos4ng0lprFPttI5j7yb2pUnC1+xWq9Bf9w8AzsoDApTr+TrfG7/uZc+FO8TEJ7Hm6I0cv+78zbT1r/vN2Z+hV3lm/j18neenbaPbj7syPPcwVvXSZzKmXLG4NfcMBpX7KW33s7dfAOCzTPoDPGwPEvQZ+hwIIUR6kqjzqWOt5ORc1duVr7rUMG6Px5YraikAAuLmUiHuNz53W8wGu/cJtc/7bGaRDxLp/P0Ogkat5q3f92X/ghSmi4AsD7vO8rDrfL7yBDci44hNSGL434fZaaFa/M89lwA4dPker/y406xdOjGLZH8rOj7HNwOmEjNL1AWUqfv8uofg0Ws4fyvtxsVay4g2+2ojDSb8xwWTWIQQIj0ZnpVPwb5u7BjejOJOdlnup6IQp3NFQWWkw0KejxuMDUlE4/jQY4yJT0KTLtMNmn8QgO83nTVum7f7stkELOduRqOYlGW3n71Nvzn7jY8zGxJ2Ovw+Lb/eTC0/dxb3DeW/E+FoFIWmlUtlG6tpcs+sdJ0fG07eBGB+yg0IWG6+eBSup6xJvunUTQJKOFknCCFEoScl6gLg7eaArS7rt1KPlrXlPwKNjsoPDnLSvif77N6hNDcfamztp24laNRqlh3OWZW76cQpzb7alOH5k+FpM6BlVqJetP8qAAcu3SPyQSK9Z++l56w9hF2JpMXkTSzPIhbT6VRNS7rpbzTyq6CPZ8np8Ps8SMi+HV+nffixFHWjlx5l2KLD1g5DCKuQRP2QVPZ0oZSLHd+8XNO4LdLRH8o3Mz62U5LYZj+IeTafUls59VDiCLsamf1OJtL3IlfJvLiZWiI0219VzUrFpkt4vvrzLs5ERNNv7n7iEvWsPRaeoZNakt5yolZSjh35IG1s+Npj4ZyJyNvUqeduFvzQsrhEPZdS1iGft/sSLb/ezPPTtmb7Op1GEnVW4hL1zN5+gfl7LnPtnrTpiyePJOqHpLa/O7s+as4LNUtTwjm5WrxlVS9oOjzDvg20xxisW4QO6y93mb4EuO1M1sO5lh66RkRUcsJWVZVXf97FT1vT2uCjTBKraZIdsfgIb/62lw/+Mi8lmZbo45LMY/nknyPUGLOGbWdusffCHd78bS8tJiePUzcYVPrN3c9XazL2rL927wF95+xjz4U7xm2rj2bsVW4wqExZd4qtp29lec2ZeW7qVp7+cgOHr9xj+N/JQ9dOZzLW3LRaX6eRr2FWTDtsJskMfOIJJH8hHhJFUYxDnTYObcr695oQ7OsGpWvTv/xqqsf9yPSk5437J6DDgQR+tPmKRbajmPJc6QzH7Kldxd+2I6nv+fDizmpctiUD5x2g3dTkUmPkg8QMif2rtZZrChbtT55tbNnh6xgMlv8QT1l32vizosAfO5Pblbv/tIvNp8ybDPZfusvyw9eZ+t8Zs+2qqjL0r0OsCLtB5+93ZHodE1YcZ1nYdaasO82rP+8yu6lIb/3xcH7aknGceWpHu+Vh2TczxJvchBSGqu+Fey8X2kllHkZfBSGKEknUBaxp5ZIAvFrf37jN2U5H+ZLOxscJqo4onPgy6WUC4uYyvvwc+icOpI7mFMGac+w0VMHBxcO4v68SgTOxjLb5jdqaM8xqWbj+cN28H4/BoHIjKmNVePqEChBc2s3s8a7zaSVd0xL1vN1pHb7Sz0nybbqEbDq+/Ni1KOIS9ew4e5uyw1dkWysAyRO8XLmbNqlKjTFrOH49yuK+fX7dy6fLj2ea2Gy12X+tTGsutFau+t5+5hZD/zpMh+nbrBpHZkwTdVZNMUI8rqTXdwH7pUc9ouIScXe0zXQf04lCnq5Ukjc7VOfHo+uprTmFA/HEqba4JETwlc0MOmnNpwG9oXrgVrE1sOZhXUKeLNp/hUNX7uXptQ8S06r8j1+33Ob8xeqsJ4sxrT5+9tstNK5YgsNXctc+n76t+LcdF5nQMTjT/W/dj+fI1UjikwzU8U+7sUrfUe3b9afpFuJHSZe0kQGmvwM5LTGqqsqi/VeJjkvktQYBBZbgTTsIPiyqqhIdn4SLvU2uX2vawVAK1+JJJCXqAqbRKFkmaYDBLSpRysWOr7vW4LfeIZRysQdgclIXasT/hEfbj3G/fSBDkgaYkfQ8Nrq01bM8XTMfFvZr75A8XkXuDf3rsLFqOjvx6dqeN5+6hcGgsu/i3UznDz+fzVjj9J24t5y+ZXE2tax8tiL9/OYZs4JpUj14+R7PTd1Kpxnbmb4hrYRvSFf8n7z2FP3n7jfbFpdoeRhaVFyiWcne1NJD13h/4SFG/3uMBXsvZ3s9OfWwkt/2s7d449e9XL33gBFLjhA8eg2H83AzZ/r+6POwnGt0yuRAcQ9hJj1ribgfx09bznEvNiH7nS34acs5VuagiUYUDpKoraCOvwe7R7TgxVppM5n5eiSPX7bVaejdqCylG3XP8Lpxid25phZHN86DH20mAdC3aQX2vVudztqNVFUuMFY3i/JK8vCoJpVKPvyLyYNT4ebJePb2C6w4cp1OM7az9UzuO3ItPXQtwxzokP+lP1OPGROfxNmb0ey7eNfsJmOaSXL+0qTEn76dHMyr929Fx/NryqxokNYubzCoVB+9hkYTN/D273vNXr9g72Xj2Hcg0znbczPJzIMEPfFJ+hzNdZ6kNxiPfexaFFez6H19Py6ROzEJvPLjLtYdD2fcv8eYuyv5Ju7b9aczfV2m5zbtw5CHu4rB8w/w1u/7GLvsWK5fWxhsPX2LiatOmH22vWbt4dPlx3l/Ye6HrB25Gsmny4/zvzn7s9/5Iflm3Wnm7JKpe3NKEnUhMbtXCG2CvFjctyEAbs5OxLy2ivCgPpw0+PJnUlNm69vQWpv8B3yNoS4APje3UPyXUL60mckKu494XbeW9XZDGa2bDUnmd9vllasssB1DE82hXMW2eegz+b/AbPSfeyDPrx047wCv/pxxitP8+vvAVTafukmH6dto/tUmOs3YzuZTeesRbmrKulNm84vfe5BAfJLeLPGvPhpuLC1du/cgQ+/41Or1uEQ9hy7fQ1VVPl95glpj13LxtuXah/CoOI5eS24OiIpLJGT8Orp8vyNDDUB6eoPK019soMXkTVy5G8uz324h9PP/Mt0/9PP/qD1urfHx7Zh448+WOivGJiSxYM9lbkXHZ3gOYMGetNqDzHp9Z3azcScmwbiyXOrNQk5FxiZyOd1iMHGJOb+x2X42Y63O5Tuxue609+rPu5ix8SwL9qYt93r0WnL/iXXHwzPMx7/9zK0shy3ezOR9flTO3ozm63WnGLH4iFXjKEqkjbqQqFDKme9fq2O2zal8Ay44VKXdvubGbTOS2vNS4xos/68WL2k30fLADxaP11O3Bm4cRsFATeUsN3HnLe1yQjQnCbGdSEjcdEoq9/hQNx97Wy2vxQwmHltciKWXdhVLDKFcUpO7lzvaaZnRvbZV78Ct5fVfdps9fuePnE/bmpn0TQSfrTjBr9svci3dvN+pVb6WeqD/feAq527FGP/oT+wUbJxlrsmXG+lU25dJnaujKAoGg4pGo1D/s/UAbHy/KYeu3ON+fBKHrkTSNtjbeNyv1pzkvVaVzc517d4DrqWMmd9/6Z5x+52YBOx0Gpzs0v6MJCQZiIozT04eJk1BpmPjr9yN5di1KLaeucVvOy5Sw9eNf/o3ynCt35iUwpMMyeP0o+KSKOaUfNxDl+/xxm97GdYmkE510mqp9l28S6cZ2zMcb/GBKzja6gitUILF+6/QOsiLUq72ZvuER8XR7tutRMUlsuWDZ4iOT+LF6duIikuiTZCX8bsacT+O13/ezQs1S/O/puWNr/9u41kmrz3FM5VLMqtXWhNU4y82ALBpaFP8izuRkGRAp1HQ5KC/wUeLw2hSuaTZ7IFajcLiA1cZsuAQ9QI8mNylJq/8lHzTeu6zZ82OG5eo56XvtxMTn1YrlKg3YJODzo8FybQjpTXOXxRJoi7kTH+Jq3q7MqlzY/BxJfa/5SjZ9YCNjqCPdiUf28wB4JzBy/jUbvt+afvpobVmL0sNDQlQbtBZu4nG2sNcVL3QkYT7/G8ILt8F8Ebk3enw+/gXtzxVqKWqZL2qEpeoZ/tZy9XcpiWzDxeZLzm6aP8VIh8k4uqgY83RcNrX8DE+t+54uFnJ1jR5Tv3vDF3qlqFMsbSpbU2rm99fmFYbU3vcWgK9XFg1+GnjNkszsdnbpPWp2HX+DvN2X6J5YCkaTdxgtt+hHHT+0xsM9J2znzXHwlk9+Gkqe7nwwV+HuXk/nvcWHjJL1D9vzTiE7sClu7z7Z/I1vFLfj7m7LvHz1vOUcrWnfElnJnQMZs+FO2ZD+U6F3+e7DWeNNyCrjt5gRdh1GlUswceLj3Dixn1OrDphlqhTmzZSp6xN7+i1KEq62NFgwn9U8nRm4TsNs712gPcWHGT+Ww1M3g+VX7Ylz1uw58Jddpg0iZyOiKayl4vx8Zpj4Ry5aj6S4UGivsAS5a3oeIo72RqHpR66fA8HWy3Odjq+WXeanqEBVPF2NetsGVeA53+cyTtUyJn27B3SshJVfVyNjxfqm7Kk/nx48Qf4+Ca8vhTe3gKj7kHPFeBVjRtqMcJVd7rGf0K7hM84aChn8TxvV4lnSrFFlC7mRBnNTeppTvGSdjMdtNvRXdmF76b3mGzzHaYdrNpVt5y4p1c/zxrboXTUbKaccq1A1uF+HLT8ejN7TSZdyU6iXuXjJUcYl8e21XXHw/l7/1Wi45PMhrp9uvw48SYdq9J3skot9aXFYTmpA5y4YV7FGpuYsQPfxXTVx8P/DuOZSRtzdA2GdG3SiXqVNceSJ6tJTcSZzQZrqYb6xe/SStipVeEXbseyO+UG4sCluxna0b9YddIsAQL0nbOffnP2s//SXeO2fRfvMmTBQW7ejzcryf5hYRlVg6qy6/wdIh8ksufC3QzPJ8evZph85+LtjB0NTZOvaRPJqXS9+fUGAwHKdRpojhq3xeVgitsMsRtU4yRHqRbtu0LdT9cxKWXCodvR8bwwfRutvt7MwHkH+HPvZeMsfaafV2ZL5Z4Ov8+ifVf45+DVDL8DTyIpURdypkOGLE2MEVssCGr4JT8o1yTtiYBQAJYZGrAsPu0OvFfCB4yzmc1z2uR1qTfqa9Bk9HqCxnsTZEjkhVdeRt3dAuXMugzn6qjdyuyk1hxWy+GvhDOwaSjD2gTS+IsNeHIHJyWOntrVtDu1FjQw2fZ7AMYnvsICfVNclFjaaPZQU3OWUYk9uI0bvspN3tMtoLRyC38lgrlJzfhG3wktenToiSdjD/pQTRgXDF5cJfvOciW5hx4Nd3DNdB8bkrAngfuPYIEUm2zmhDc1df1p/tp3Jfsd8+C2ydSulmZP23nuNiEBxdBolGxXF1NV1ViKMq1WTXXIQptsTCYJ4sSNKP45eI0eDQK4FR1Pr9l7zJ437QG+/ext7sQkmA0vM40lu7Z3S0wTearMpuHdki6Jplazrwi7btarf8q6U7z6lL9Z23bkg0TsTUZvZHb89E0v1yPjmGxh9j1LUvsq6A0qny4/xtU70cyzHY+3cofZSa0YndSTF6ZvY8zzQbQK8jJ7rd6gsuv8bYJLu5kNqdt6+paxP8jsXvXYdf4OrvY2TP0v+eZm+obk5peWVdOOl/r+pXbuNO1nYKkTKCTf1JrG0rGmj/kSgAUkPCoOVQUvN/vsd7YiSdSFnGmJ2sU+7eMa2royW07fpGPtjDOYZeUurvRPHMjkpJeIUp24hRsX9AlgSG4HVexcoPOv3Nszj19XbaOJ5jA1PW3h5nHOGzy5ppYgRDnBArtxMHMI0e+eI0g5z3K7EZmec4TNXGprTtNWm/ZHN/VGIb1rFAfgW5uptNAcYHBiX66oJZlu8w2nVV/Oq168oVsJwFZ9EBOTunFZLck9XKiqXOA17VqclDhmJ7WmqfYgA3VLAIi09eTzmOcordziturKWU05Ag2nWKpvyE77AQA0iJvKaJtfqaRc5qzqw8SkbpxTvdGT9R9UDQZsSSSOrFdQC/RyITFdCeIt7b98ZDOPv/WNGJv4GvdxNJ5v/p6CG4KVRkWLgTkmHavWHss4nerLM3cy9oUgXm8QkGE4nemxHIjHoELqPWROFiHJyovTt/MgUc+MjWctPm86R/uVuw/MOq0BjPznKKPaV2XpoWtm7enZsSURZx4Yb+i06NGjZZB2EZU0lwlSLvJlUleWG55CRxLNNQe4rbqQhI6DanlG6X7jpurGD/r21NGHsYdAbEmkqnKRPdGVuXY3lr4mHSZHLD5iNvb+6r0H2Ok0xumGAbZlMgJi+n8nKUUUESS/frhuDr7KTQYn9ifR5E965INEBsw7wJmIaI5fj+JV7Vq8bZJrdJbqk6var0fG8dbv+/B0teOtp8vTqXZpXO1tmLPrIiP/OUpIQDEWvJN2o2/aaXPYojCLkxxN33DWbKW89D31TScn2nbmFlW8XalRxh3uXQY7F/R2bgQql4hQ3WmgOUbHpa+QtKc2i6t/T5ta5TIdix+bkMTW07doXLEkcYl6ouOTzJpwwPxGLlFvMPbbOPlpG+yyuXGyJknUhVwJZztKudhhq9NQw9fduL3fMxXo90yFXB3LwUZrrGo6p6a1WWLrBAMPgKIBjwAA3Bu9SR3PF9E46MDXnft3rtPry+Xcwg2NmvxFu11/KO7OHhnayiMd/Oh5rw+L7UYBUD9uGgN1iwE4a/CmvCbz8ZtxanIJeochiHba3ago3MUFP81N/NKtNNZIe5RG2o+ZkdSeiUndsCGJFtp9JKLjB9rzQE27S3ZLCGeCzc/mJ9NCm5Sbh9+TWhCLHf5KOGU14ZQlnBbaA8SrNszRN2dSUhfe0i3jsqEUKw0hfKz7gyqaS3RKGM0b2uV00/7H24lDOKWWob5ynOba/Tyn3cEVtSQb9TWYoX8er6TrNJzTng6aviwxJHea+shmHpBcW9FRu5VThtK0SZiIAQ2gokFN+RnqKidwVh6w01A1y5uCEOU4WsVAhOpOMe6zR60MKHhxm0V2o4lTbXkpYRR1NKfZbahMFM4WjzPyn6McvRrF7f1L8MGfa5SgknKZl7UbqKU5Q3nlGs48YP+8MLqG1aar4z5aBNgAtQCorFzitOprjN+Un5J8c1CCSParlYzbM6sKTfXJP0eNSdSS33depEH54gxZkNaWXlm5hL8SzhpDPX60mcQl1RN3JRo/JZxw1YMN+lr01f1Dec11/tI/TZihLGNsfqVm3A+8o/sXByW59mG67bdM59sM55yU2JmmmoOU1YSzWN+YqTZTKaakq6X4BpYA1+2K8XbCuxxWy7PvYlqVd2ov+rbVvOhc15dmgZ7otArllav4KRFsMtTAgAYdSSy2HUmw5gLjE1/BWXnA27rlAPytP8R6Q1qH1NnbLxhLsQ7EMSDlO/hTUluz91yDgQdRd5i7/BxhK87zXKmbOD9IoBhtuXYxghX7StB34SnjrIupYhKS8FUieF27liX6UI6pAQQp52ml3cvpE85oMGBAk2FCn8QkPSWIJBY7hv99iLe0y6nmvAJtfCT6mq8SWeUVVtkNI0a1Q01ZZld3fT8eV95mwoXP+Kxb8k3Gvot3sNFqqH7pDwxX9vJR/BssORqJm4ONsQPmn289xdJD1xjQrCJ3YxN47eddDGpekdcaBJh10ox6kISrQ3IHSW83BwobRc3JWINC6sqVK5QpU4bLly/j6+ub/QuKqLhEPQZVxdE29/dVAcOSv8SBXi4sG9CICiNWZtjnwuftcnSsT5Yc4U5sAssPX6e1ZjefdG6Eb62W1Bv2B920G3hFtx4FldXNVzNyxVlKc5MEdNzEg1aaPQzR/cVv+lbUUk7TWZdWtTUgoT9halkqO8WxOrosAC7E8j/dUhbqm7B0VA9cPi+RaVxTkzrwXdLzPMCOdppd3FLdcAlsSsCpX4wd6TJTI24mjTVh7DYE4qPcZondyCz3v6m68VrCcNpod9NN+x/r9bV5RZc2VMmgKmiUjF+pXglDCXa6x5DEH3k7YTCN3W/zauzvFs/RLv4zjqoB7LRL7vDXPeEjrqolOGHfy2y/75Oe4x3dMuYmNaOtdjc7DVUYkDiA/XZv46o84IghgGqaCxw3+KFDT0XN1Syv7aX4kXgq9xht8yv9EgZyQvUjCkfm2oynluYMAxP786Pt5CyPATAm8TWuqSX4n24pXsodXk74mOaaAzTTHCBUezTD/uv1teibOIiqykUm23xHmFqODfqaNNQcpZN2CxpF5YShDN7KbdyUtDbaSYmd2Wyozoe6+czSt6G8cg13JZoZ2u4o8VFE4UgZ5Sab7d5llyGQ/gkDWWA7hrKacA4ZylFDk7GzmanJiS+xwhDCi9qt9NMtzfa69arCUkNDXtRmPhVrgqqldvwPeCl3qKhcZZWhHioabEjiXd1f7DJUYZOhOluDlpF0+j8CNMk3NLOSWjM5qTPllGv8k8nvaPP4L2mv3UE95STf6Z9nuyGIgdrFvGuzyGy/5+PHEY0Dw3Xz2GOoxC5DlUyPCTA4IfnGspJymUtqKeNNYlm7KDYo7xj3u2QoiZ/G/Gb66fivuaO6UE65zmG1PN81jOHZ/W9meq5IxYU/Ep+hn24p85OastsQaGxCS3Vf68bZEs3oe/EZHJU41tl9ACR/XidVX06pZVhj+wHLDfUZnNgPb+7wneuvRGnc+d+9V4jFnguft+PynVhjf4zdHzXnlZ+SV/dbNbgxgV5pTWU7z91m0b4rjGhXJdvJrHIjN/lLStRFgGmv2fzQaTU0CyzFfyci8vT6cR2qEROfxPLD11ltCGFo6acAuIkH3+o78q2+Y/J+NsklWdM25DWGeiQk2XDSUIa5JJdQ7+BKUJkSxt7LiToHILn3830c+SLpZQDsdFoC4uZSVzlBiOYkmw3BHFHLMkj7N6HaIxwylOcByedcbkiOaaCPK98eb8dP+nbGaq0Bc/ez4fBZXteu5QObP3kr4V0icWaZIblqL0L1ICBuLhWUK7ypXUEX3aYMtQXjEl/jhOqHnT6BCspV5uqbsc1Qjem2ySUtS0n6x6Rn2WoIxi92PfE6HfHY0iZuldk+m/TVaaJN7gj0inY9I5N6EqPaU15znVDNEebpm2c47ju6Zcn7p9woHDKUJwktrkrye1hNcwGAKpqcjR/+y26s8ecFduMAmJv0DBG446Ak8I7uX7P9zxs8KasxrzaPVe3YZajCU5rjBCqXWGGoz/OaHQyx+SvT8zbXHqCh/iieyl3KasKZkfg8NTRnzW7mAjUZmwFCNCe4hRvBmvO8xlrj+/c//gV7uKKWwFdJrj6urzmRctPozoakWjTVHMz2/dhiCOaM6suXSS9zVS1JJ+1m6mhOc0t1ZVJSF0I0J+ioTe4g9UC1pX78NKJwxp4EYzPPwqSnqaK5ZPwsxie9SjSOBCqHmGY7lYOGcnRIGEdrzR766pbSl6X8mdQU37Mbzbr6/pD0HFNtptJUe4iRiT0Ya/OrWazzk5pyTvVmsO5v9hsqcMZQmm9spvO8NuNCNFfVErTV7qaldh8ttdkPN5xi+x1L4xryoW4+p1RfJiZ1A2ABw8z2S5+kb6mulFZuUUW5xA+2XzMhsRu/7qhALdtieCuWO1X+lticH5PacUt1Y5G+MVE4sziuEctsRxCkSe6U56KPpGb4YrbbL2ZCYjd6JQylpuYswZpzDNGm/Z510G6nFPeorjmHc0JyFf0bWncOqeUx/LUY1bsl4Iwzsbj/0QL/W604T01WhN0wS9Qvz0xupnO01TLmhWrZvl8PgyTqJ8zUbrXYdf42u87f4YdNWZcoLDHt0JZao7X+vSZERMXT7cfkX+gKpVwsvZSNhprGn8MpBsAHrSsbx31amrv61af8sE3pgLVXDWSvPtD43Df6Tnyj74SrvQ7Sjd11sNHyR5/6aDWKse1p7AvV+NxWR/N6zcF/JmtSahtm9ayHooCPuwOtvt7MGdWX793fxf6Zb/h44R766ZZwWS3Fn/qmJKV8ZQ6pFeifOAiAI2o5DseXpZHmCP5KOE9rwrDVKnwb/xw7DFW5iTsAv+pb86u+FaAwymsa7ewOs/lUOIv1jYjHhvJJ14hSHYnCCT0ahia+zVW1hPG9ahU/kd9tJ+Cp3LP4/l5TSwAKz8eP45jqj4pCmN0bOCrmE1z8mtSSZpqDHFLL8UliL36w/ZoQjeUOSsdVf5JULVfVEnyZ1JUqyiUqKFdZZQgxtod21W5gos2PjEl8jVn6tgAc0wcwS98aFQ3PaA5w3uBJGeUmOiWtfXK7vioNtck92i+oXtTWnGZi4sss1DehlWYvPS3MZ296Q3NQLU+AEo6CysiknmzSDjHbNzVJp4rEmS4Jyc0xY3kNZx4QjSMliOQuzhTjPh7KfWpozrJGX5dIkyaBufrmzE13szRf34z3Et+hq3YjBw0VjE0I7yb2ZaH+KBsNNY3V/hoMeHPbePN6CzfiVR2/JD0LKCwzPMU0pia/n7qNZud5I+E9HmBHHU3ySnTbDNX4KLEPn6U05Zw3eDIx6WVUFDbrgzmnenMfR4KUC8Zj7NBXpaRyjz2GytzGlT/0LbiuFkOLgSrKJWOp+52Ewew1VOZX288J0lzkuMGP/okDeFpzKPmGSj3KTkNVNhuCKZnye7hYH0p9zXF8lDsMSujLCsNTNNQcZYhuIX4pCwodN5TBU7nLRbUUG/Q1cFTiaawJo7iS3BFwVlJroqq+wtTDCknojL9HkNwA9HzCpxy16429Yj6nQCnlHj/on2ODoRbTbL5J/+ti/P1KfQ++0z9PZ+0mEk6tx+f4MkryFVE4YhsRxs+2YcSrNpw4+zwc2wcBjQl/ABfsf+egoRxTb3+X4fiPilR9P+ZMq75Nx7tOWn3SOBNWTqu+IbkHZvmPVgAYx7GmOnj5Hmcionmpjq/xvNlZNqARz6Usk1m2hJPZnN5PVyrJbynzlWd1vAuftyNJb+DavTie/jK5Kuu33iE8nc0UqhduxXA9Mo4G5Ysbt6Wep2wJJ95tWYmB8/I2Y9qY54MYtTRjNW+q5oGlCPR2MfaSzS1bErEjkfs44sQDYrDcrqZFTyPNEQ4ZyvG2bhlHDQHGGoQ0Km01u/na5jsuqp5cVksSqLnMX/qn+Tapo8U25oJSgkhclFjOqxmH+vkp4USqThjQEI8NCWTsRGRHAnYkEIUztZTT1NScIVz14JJaip9tJ+Gp3CNe1fFG4vtsMVR/aNeRFyW5Syz2xs+urnLCrFZjuT6EaUkvclxNXomvseYwSWjZYQgCkmcatCXJ+Hx67tynnXYXy/X1uYflm2cABQMeRGcYGeFAnLGmCqCqcoFSyl02GWpgRyLf20zBlkT6JL5vtp+lODbbDWZaUgdm6ttn865kRcVfCSdadaCK5hJ1lFMsNjQyTsz0gmYrY21mc1UtyeSkl6itOc2L2q3o0fBjUjt+1bcGIFC5xFzbT3k9YRhH1HI48YCj9n2yPfsxh7pU/XB9PuI3J1Xfwqi4ky23YxJokq4jSF6XCzQt9RZ3Nm+vqVnGnZpl3IHkUmr6oTWQvARkaq/PaqVdzXqyVyvtZpao7XMxlEmn1eBX3JGxLwRx6348jStm3qadKqCEEwElLE9AElA8f0O17G3SYv+ue23G/HuU8Ki0ku35WzEE+WQ+ZCw7CSaJK7MkDaBHyyZDDQBjlWWDcsWN44I9HG24G5vISkN9Dto9jaIoxlnIHoVbuHFLdbP4XOof4KzEY2scwndArcgBfUXjc03iv8aVWGPv6LyY+Vodft1xIUdLpVryxUvVjWObHW21xJr0ir+ZLq69aiBjEl/jde0a3k3sx0HVvLNo+huNs2rWIz7u4cIcfYtsY1QzGb6YPvkeUwM4pgYAEIcdPRM/ALKfUe0eLoTEf4ctuVskJyOFi2rysK+thmC2Yr6y3T+GRvwTnzaz3TpDHWPzmakTqh+142caH8fgwP5e51g4czwDdIvxsVAtH6U6stB3GKPyeQV5JYn6MbdsYCM2nrzJi7XMv9T5mUNgSb9QYhOSzIaSpPdMYKlMn/ulZ13uxiTSqY6v2fzOnev4Elza1biKlZ1J2/zUbrUY/ncY0fFZf9lfbxCQw6uwbOE7Dfh1+wU+ea4qu89bbkfLCQeTjn+VPF1wSNfP4NytGLQPYVxodkY8W4VEg8GYqMuWcOJuyjCmoW0CzRYXMVXMyZY7MZZXarK30ZiNG34YUm8ociMOO2PHpy86VWfssmPZ/v6YSp3ms6afOyHjc1aSCq1Q3JjUO9T0wcVketX0y6haMkvf1qza15KWVT0Z2Kwig+Yf4Fw2q8pl5Z0m5Y3TzuZNzpdZNf0sCqOOM3YCzZmnb44tiejRoENPIrq0GqWwBK7/vo/vutfO0ZSvBUlmJnvMebs50C3EL0OHtLxMBpGqZhl3GpbPvsSamWaBnsapHp1N/pDpDSpvPZ02DaOdSYm6fQ0fDo9q9dBXBKsXUIxpr9TG0zXrCRAymw0rVTUfV95rWYkBzSpQoZSzxQRhaebEKt55L2Wn6hbiZ1YjYPo+2tloeL1BAM0DSzGpcw3jmFKA56r7WPy9GNY2MMuFKDrVfvjNTjNfr5uv11fxduX1BpariC159Sk/43SvpuNrbSxMOmRqarfaxp+93BzMpsdsXLFgfneLO9kS7OtGz9CAfB2nkqflYXlPugRs0KMlHtsMzT6rjt5gZyYr1z1MkqifUO1SFmIwneD/kUj3d840iaSfWMO0+hiS1/r2cLQ82cGjllneWvPu08x9oz7lSjozoHlF4wIX99N1dqtZxt0sSaZK0hs4Ma4N64Y0MXtvcsrD0YYJHYPN7vj/6R9q/FlRFJztdPzcsx4v1fE1+zhsdRqLNS21/TyybChRFNg5vDnBpS1XYRcE0yaSvLC30WR7c2VKa7Kz6edgqcNjqhZVSpnt62KvM5uJ7n9Ny/P20+XY8kHBrEbX0KRvRV6YXsvH7arkN5wnxqNsGkolifoJVd3XnY3vN2XdkCbZ71wAmqVUhb/+lHmpxjRZJaRbwtDSTEEFOY4xO1n9YX+mcklstIrZ7FKQXM3dsELG2ob0yzv+kG6ltL5Ny6PVKHzWMRh7Gy0VSjlbXBIyvefSzbeusRB0JZNe+PfjzKuP0+9uaT1rR1ttpjcmqbzc7BnSKm0SjVp+7vQOLUvXumWyfiHQsXZpsz4FX7xUnYHNKhhvJgGc8jCHgKnczjpl+ntpa1IqzmouA4NqPirCVqsxS/il3R0Y/myVDLNl5VWFUi7M6lWP3/uEcPLTNtT2c8/V631MbtIz+16ZJvP3WlZi64cPf8nbwu7m/Ue/TKgk6idYQAknHGwf3rR5pqXfaa/U4tfeIXzQJjDDfp3r+OJf3JEWVczbtdMnQYDOdR9d735tFpnaw9GWsNGtWfh2+h7UlvU1WVkJwNPV3myxgf7NKnBsbGvqBRTL9BifPFc1w7Z3mpgf11Ip3bR0HZmunVdJV8Vh2rluxLNV6BUakG2nt9Rj2JkktBnd6zCyfVW6P+Vn3FaxlOWq1sldajK1Wy3j4yAfV4a0qmx2E+Gez5oUrVbJcK05Zfr+ze5Vj5IulttaDaqKzqTfQfq5+XMzz3uquv4evNeyUqbPP1O5FI0rlsROpzVbbjQnSrnY8Uef+izpF2rW5PFBm7RlTk2bpgY0r5inGjjTvizlSlruvFmYlUjXaTazz/9hkkQtHprf+9Sntp87C95ugKOtjiaVShrHRJv6snMNNr7f1FhaWfPu03zVuQZtq3ll2DfIx40tHzxjfC6/1X9ZeSawFIFeLnStW4bvutdm3ptPGZ9TFAV7G22OO5W827ISEzoG42KvY3jb5JsV02pmG60m21Jfn0Zl2f2R+VjeaqXdGNg8radzduFUSJcsq3ibD9sZ+3w1WlTxZP17TXjz6XKMah+EoigZ2qgXmNygWLqfcbJLvpbqvu780ac+G95vytohTSibSS9707bc1GRneqPmnEkS+rprDYvbMxw/l51/0tdMHBrVij0jWlDd153dHzVn89CMJUtVNS+Bpu88lpPOZOmbe+KS9PRuVDZHMefk+D0bBhh/ttFqaFSxBDXLuNO2mhclXexoV93b7NrTv++KorBjeLMMx03fNJG+CeDcZ89y4fN2BdIHw5TpjcM//UKz2DPvPnrWvFmgfY1Hv9yv9PoWD0210m783TdnXx7TkmAlTxcqeWY+7rNMMUe+6lKD1kFeWfYuzy97G63Z2HNTue30aaPV0C3Ej5frlTFeq+kQuZz8kQUo5WpPx9ql+Xt/2nSgQ1pWMi7NmPpHtlwJZ87dTOsRvGpwY3adu5Oh9/97rSujKArP10ye+z3Y142femTsuJW+5tu0VJEaeaLJnYdpFXEjk2rtzK7S9AYu9Y/8q0/5Y2+jpUG54hZrCrZ88Axlijka15f2drPneibthyVd7HLcRu1qr+PNp82To5tDWoleURT8LAzfS/8e6bTm7eJ5WXf5yNWoLNvFTZmOIhjXoRr1AjxoM2WL2T61/T1wsdcRm6A3q/p2sbdhx7BmaDUK35tMhGSpxs3SXNimv7+davvyYdvKxp7yGiWtVsLWwnuQfthablTxdjGu5V7d9+H0kQgpa17LZWOF0RqSqEWR5Giro0OtrMeRPkyW2oJzwjThmFZ9W0pEeVHMKbmabkjLSugNBjqntBEHermaTYuYytXehtHPB2V73KzWBDauRmTSpp5pckm3ObVEbKPV0LNhAFFxifinJMHUm5tUbzcpx3/HI/iycw1KudiZJRqA4NJuaBTF+Ic71fev1s7R+1uzjDvz3nwKG62CLgdJ9X9Ny5ut8pW+1iH9zZfpe9KjgT+/7si4TnXLql78e+ia8fEbjcrmOMGb7vZaSl+Qt5uUM5uBUAFjB8f0Uq/ZtBrcMYdNY6bX1i2kDKVc0kZNmH5XLN2QOtrq8pyoK3q6sO548pTIiqJk+ExywlanYVq3Wrz1u+XpVH09zG/KHvXQLJBELUSeFMSXNTdj2b9/tXaWz//coy7frj/NV12SE19VH1dm9QrJT3hm0ndss1QVHVKuGIqSeVt0er/0rEuzwLRJTbK7YRjetgrD22beO1mnVdg0tGmGhWd02ZSA1r77NJ5u9rhmsnxiZkxXs4OMIwFstJoMVdmpRrUPspiox79YjZpl3GkX7M21yAdU83HLUHuTWce+52uUZvXRcLOheW80Speoc/Bra3pTltN1BsyScbobC9MkbtpO7+Vqz42oOF57yp+v150ye41fMUcu3YklO74e5jdrH7YJpP8zFTh8JdI4pXF2XO11tAryoqq3K8euR1ncp1GFEmzNZNnRR6HQtFF//vnnKIrC4MGDrR2KENkqiJvq7GaH++n1ujStXJLdI5rTplpau5ilTlHNq3jyT/9Gmc6znl+1/dLaiz/vGGxe9Z0Sjqu9DUdGt2bFwMaZHsc0ctMkXRBUNWOSgLQqadNzNzdpMnFzsMl1koaMNyvpP0//4o7U9vOgY63SDG5R0ey5zG70XO1t6NOoLF5u9tT288BWp8lxbcuzwV789U4D/umfNjtXSRc75r5Z3/g4Jx3qTG8gB6X0f+hQ08dsnwkdg2lZNe3zy6pt3rRfhOlNwLy3nmJWz3r0fca8QyTAxE5ps7CFjW5l1rae6tDIVhZrG5zsdJl2ehtj4WYw9canRBadxFL7XFhLoUjUe/bs4YcffqB69cI1F68QmTEtQVRN6SBTK5fDY7IrUbeo6snsXiFm1YjW8mXn6rSq6smi/zXkZZPqaDD/4+9kp8uy2jh1opz0M7UVhNQ/uCOfq0r3+mkxWprEZWyHtFWQclLNbUnD8sXpYjIKwZBS6TD3jfpM6BhMLT8PFEVhcteaDG6Rsef2iGerUNzJ1mIHy6xklrcVRaFuQDGz9vTkONP6COTkBtP0/QqtUIK9H7dgcpeaZvt0C/HjR5NJaEy/D6nX07dpebxc7fm4XdpohdopHQQbVyxB2RJOPBNYymKyNX1PdBpNhmaFkLLFcHO0yXQCmszeo/TvDaTdyMVnsRZ6bof3FTSrJ+ro6Gi6d+/Ojz/+iIdH3ufkFeJRSJ0ZzXTY0axe9RjSslKGsdHZyc/scI+af3EnZr5e1+KQudw0rw9rG8iwtoGsHJR5qTuvUku0vRuV5VOTRJz6Rz61w1xVb1ezoXc57ayVnkaj8MVLab3OU8/fsEIJs7b1zLz5dDn2ftyCCiWzbyooqGbRnHxW6ZNiCWe7TGsAKqd0+uxYO62/SGqJ+oM2gewY3gwvt7QbzS51y7Dlg2fMkrwlpr3GNRrwdLN8s9os0BMnW22Guf0zu8708w5A2vcwq3kLcnszVdCs3kbdr18/2rVrR4sWLfj000+z3Dc+Pp74+LTB5vfv33/Y4QlhZlbPekTFJZpNEOHpam82RCrH8pinC6jfmVU42ekyjP0uKKb5xbS6OHUOlwqlXNgzogXujjbExqeVnrKbFjSn8jJ/vqIoOfo8D49uTbVRq4HM26hzeMZs99Dn4gT/9A8lPCqOC7fT2pNNS8iWqu1zMuGL6ZhwraLQO7QsF27FsGDvleSNKSG6Odiwf2TLDD2xLbWtv97AH51WQ9PKJdl4Mm3t7NTq+NiEtNkDf+8TwqU7sTxVLnn4Z15mCSxIVk3U8+fPZ//+/ezZk3GVJUsmTJjAmDFjHnJUQmROo1EKbHa0olSizkphuW/I7N00nSwltW3dzVHDpx2qoVGULGcby9X58/h55iRRZzaOPKdSF06p7e+e7b65ueGwt9HiX9yJy3fSetqnn+glLwKKO9K5ji+uDjbotBp0WvjipRrGRG3aH8BStXQJZzuGtq5stsjMkJSJY9JfX+rHduVu2jWkn5fd2lXfVkvUly9fZtCgQaxduxZ7+5y1wQ0fPpwhQ9IWh7969SpVq2acrUmIoiCvK5gVlsSYyjGfSaSgFHcyv4H6umsNTly/n+mkOK+mm842rzRK8mdZv2zeJt/J6YxpznY6ouOTaFYl93MH7Pu4JTHxSTnq75CXG0jTm43setnn7HgKX3bOfDKbnITY75kKxkT95UvVjTfY/Z+pwOZTaSXq/s2SlxNtUcWTpYeuUS8gY/OOcz7nms8vq5193759REREULt22rATvV7P5s2bmTZtGvHx8Wi15ncxdnZ22Nml9cyLirLclV6IoqCol6hHPleV5WHXeaNxzmbOelhmdK/Nn3svM7S1+fjgF2v5Qq1MXlSA/nuvKRtORuSoXdqSnDZlbBralFPh0TxVLvNpZjPjZKfL8RSj+f21LKimhKzkNkTTKviQssU4OLIlTnY6zt6MNrazj3sheZKY9jV8Mry+T2hZVoRd59ngRz8rGVgxUTdv3pywsDCzbb169SIwMJAPP/wwQ5IW4nHTpFJJZm27YPX2r7zq3ahsjqe3fJjaBnvT1kp/QCF5fvReJfL+PjxTuRSHr0RmuzJccWc7GmSxBnxB0eehqsf0NXntRZ8bOW1meKW+H7vO3TZb4AXSFiExnQTIzdGG1zJZz97N0eaRLWBkidUStYuLC9WqVTPb5uTkRPHixTNsF+Jx1KRSSea/9RTlc9DrVzy++j5THl8PB7OpVq2pZVVPft56PkNTQlZMO6DldDrc/MjprcRnLwajqmqBzfxnLYWjcUmIJ5CiKMZepeLJZafTGqd6LQyeKlecZQMaUcYj58txmpZw8zKneW7lpnq+qCdpKGSJeuPGjdYOQYhCr98zFVi0/wqv1M9bm6gQ2alWOncLXJguY57XcempWlXNfsa6ot27I/cKVaIWQmQvoIQTJ8a1tfokDEKk0hsynywkJ/56pwFzdl0iyMeVLvUKT+1CYSGJWogiSJK0KEzyOtQwVd2AYtQNyEVv9iI+YiK35NsuhBAiX/LSUzw/nqw0LYlaCCFEPnm6PtqFY66lW3P8cSdV30IIIfKlXoAHHz0b+NCHGnao6cOSg9foXr9gZpUrKiRRCyGEyBdFUXjr6Yez2IqpzztVp1MdX0LK5n52tqJMErUQQogiwd5Gm2HBjCeBtFELIYQQhZgkaiGEEKIQk0QthBBCFGKSqIUQQohCTBK1EEIIUYgV6V7fhpT5Za9fv27lSIQQQoicS81bhhzMk16kE3V4eDgAISEhVo5ECCGEyL3w8HD8/LJeCU9R1aI7u3lSUhIHDhzA09MTjSb/tfj379+natWqHDt2DBcXlwKI8NGTaygc5BoKB7mGwkGuISODwUB4eDi1atVCp8u6zFykE3VBi4qKws3NjcjISFxdXa0dTp7INRQOcg2Fg1xD4SDXkD/SmUwIIYQoxCRRCyGEEIWYJGoTdnZ2jBo1Cjs7O2uHkmdyDYWDXEPhINdQOMg15I+0UQshhBCFmJSohRBCiEJMErUQQghRiEmiFkIIIQoxSdQppk+fTkBAAPb29tSvX5/du3dbO6Rc2bx5M+3bt8fHxwdFUViyZIm1Q8q1CRMmUK9ePVxcXChVqhQdOnTg5MmT1g4rV2bMmEH16tVxdXXF1dWVBg0asHLlSmuHlWeff/45iqIwePBga4eSK6NHj0ZRFLN/gYGB1g4r165evcqrr75K8eLFcXBwIDg4mL1791o7rBwLCAjI8DkoikK/fv2sHVqO6fV6PvnkE8qWLYuDgwPly5dn3LhxPMruXZKogT///JMhQ4YwatQo9u/fT40aNWjdujURERHWDi3HYmJiqFGjBtOnT7d2KHm2adMm+vXrx86dO1m7di2JiYm0atWKmJgYa4eWY76+vnz++efs27ePvXv30qxZM1544QWOHj1q7dBybc+ePfzwww9Ur17d2qHkSVBQENevXzf+27p1q7VDypW7d+8SGhqKjY0NK1eu5NixY3z11Vd4eHhYO7Qc27Nnj9lnsHbtWgA6d+5s5chybuLEicyYMYNp06Zx/PhxJk6cyBdffMHUqVMfXRCqUENCQtR+/foZH+v1etXHx0edMGGCFaPKO0BdvHixtcPIt4iICBVQN23aZO1Q8sXDw0P96aefrB1Grty/f1+tWLGiunbtWrVJkybqoEGDrB1SrowaNUqtUaOGtcPIlw8//FBt1KiRtcMoUIMGDVLLly+vGgwGa4eSY+3atVN79+5ttq1jx45q9+7dH1kMT3yJOiEhgX379tGiRQvjNo1GQ4sWLdixY4cVIxORkZEAFCtWzMqR5I1er2f+/PnExMTQoEEDa4eTK/369aNdu3Zm34ui5vTp0/j4+FCuXDm6d+/OpUuXrB1SrixdupS6devSuXNnSpUqRa1atfjxxx+tHVaeJSQk8Mcff9C7d28URbF2ODnWsGFD1q9fz6lTpwA4dOgQW7dupW3bto8shiK9elZBuHXrFnq9Hk9PT7Ptnp6enDhxwkpRCYPBwODBgwkNDaVatWrWDidXwsLCaNCgAXFxcTg7O7N48WKqVq1q7bBybP78+ezfv589e/ZYO5Q8q1+/PrNnz6Zy5cpcv36dMWPG0LhxY44cOVJkFoU4d+4cM2bMYMiQIXz00Ufs2bOHgQMHYmtrS48ePawdXq4tWbKEe/fu0bNnT2uHkivDhg0jKiqKwMBAtFoter2e8ePH071790cWwxOfqEXh1K9fP44cOVLk2hUBKleuzMGDB4mMjOSvv/6iR48ebNq0qUgk68uXLzNo0CDWrl2Lvb29tcPJM9PSTvXq1alfvz7+/v4sWLCAPn36WDGynDMYDNStW5fPPvsMgFq1anHkyBG+//77Ipmof/75Z9q2bYuPj4+1Q8mVBQsWMGfOHObOnUtQUBAHDx5k8ODB+Pj4PLLP4YlP1CVKlECr1RrXtk4VHh6Ol5eXlaJ6svXv359ly5axefNmfH19rR1Ortna2lKhQgUA6tSpw549e/jmm2/44YcfrBxZ9vbt20dERAS1a9c2btPr9WzevJlp06YRHx+PVqu1YoR54+7uTqVKlThz5oy1Q8kxb2/vDDd3VapUYdGiRVaKKO8uXrzIunXr+Pvvv60dSq4NHTqUYcOG8fLLLwMQHBzMxYsXmTBhwiNL1E98G7WtrS116tRh/fr1xm0Gg4H169cXuXbFok5VVfr378/ixYv577//KFu2rLVDKhAGg4H4+Hhrh5EjzZs3JywsjIMHDxr/1a1bl+7du3Pw4MEimaQBoqOjOXv2LN7e3tYOJcdCQ0MzDE88deoU/v7+Vooo72bNmkWpUqVo166dtUPJtdjYWDQa81Sp1WoxGAyPLIYnvkQNMGTIEHr06EHdunUJCQlhypQpxMTE0KtXL2uHlmPR0dFmpYXz589z8OBBihUrhp+fnxUjy7l+/foxd+5c/vnnH1xcXLhx4wYAbm5uODg4WDm6nBk+fDht27bFz8+P+/fvM3fuXDZu3Mjq1autHVqOuLi4ZOgT4OTkRPHixYtUX4H333+f9u3b4+/vz7Vr1xg1ahRarZZu3bpZO7Qce/fdd2nYsCGfffYZXbp0Yffu3cycOZOZM2daO7RcMRgMzJo1ix49eqDTFb2U0759e8aPH4+fnx9BQUEcOHCAyZMn07t370cXxCPrX17ITZ06VfXz81NtbW3VkJAQdefOndYOKVc2bNigAhn+9ejRw9qh5Zil+AF11qxZ1g4tx3r37q36+/urtra2asmSJdXmzZura9assXZY+VIUh2d17dpV9fb2Vm1tbdXSpUurXbt2Vc+cOWPtsHLt33//VatVq6ba2dmpgYGB6syZM60dUq6tXr1aBdSTJ09aO5Q8iYqKUgcNGqT6+fmp9vb2arly5dQRI0ao8fHxjywGWT1LCCGEKMSe+DZqIYQQojCTRC2EEEIUYpKohRBCiEJMErUQQghRiEmiFkIIIQoxSdRCCCFEISaJWgghhCjEJFELIYQQhZgkaiFEvimKwpIlS6wdhhCPJUnUQhRxPXv2RFGUDP/atGlj7dCEEAWg6M2QLoTIoE2bNsyaNctsm52dnZWiEUIUJClRC/EYsLOzw8vLy+yfh4cHkFwtPWPGDNq2bYuDgwPlypXjr7/+Mnt9WFgYzZo1w8HBgeLFi/PWW28RHR1tts8vv/xCUFAQdnZ2eHt7079/f7Pnb926xYsvvoijoyMVK1Zk6dKlxufu3r1L9+7dKVmyJA4ODlSsWDHDjYUQwjJJ1EI8AT755BM6derEoUOH6N69Oy+//DLHjx8HICYmhtatW+Ph4cGePXtYuHAh69atM0vEM2bMoF+/frz11luEhYWxdOlSKlSoYHaOMWPG0KVLFw4fPsyzzz5L9+7duXPnjvH8x44dY+XKlRw/fpwZM2ZQokSJR/cGCFGUPbJ1uoQQD0WPHj1UrVarOjk5mf0bP368qqrJy4e+8847Zq+pX7+++r///U9VVVWdOXOm6uHhoUZHRxufX758uarRaNQbN26oqqqqPj4+6ogRIzKNAVA//vhj4+Po6GgVUFeuXKmqqqq2b99e7dWrV8FcsBBPGGmjFuIx8MwzzzBjxgyzbcWKFTP+3KBBA7PnGjRowMGDBwE4fvw4NWrUwMnJyfh8aGgoBoOBkydPoigK165do3nz5lnGUL16dePPTk5OuLq6EhERAcD//vc/OnXqxP79+2nVqhUdOnSgYcOGebpWIZ40kqiFeAw4OTllqIouKA4ODjnaz8bGxuyxoigYDAYA2rZty8WLF1mxYgVr166lefPm9OvXj0mTJhV4vEI8bqSNWognwM6dOzM8rlKlCgBVqlTh0KFDxMTEGJ/ftm0bGo2GypUr4+LiQkBAAOvXr89XDCVLlqRHjx788ccfTJkyhZkzZ+breEI8KaRELcRjID4+nhs3bpht0+l0xg5bCxcupG7dujRq1Ig5c+awe/dufv75ZwC6d+/OqFGj6NGjB6NHj+bmzZsMGDCA1157DU9PTwBGjx7NO++8Q6lSpWjbti33799n27ZtDBgwIEfxjRw5kjp16hAUFER8fDzLli0z3igIIbImiVqIx8CqVavw9vY221a5cmVOnDgBJPfInj9/Pn379sXb25t58+ZRtWpVABwdHVm9ejWDBg2iXr16ODo60qlTJyZPnmw8Vo8ePYiLi+Prr7/m/fffp0SJErz00ks5js/W1pbhw4dz4cIFHBwcaNy4MfPnzy+AKxfi8aeoqqpaOwghxMOjKAqLFy+mQ4cO1g5FCJEH0kYthBBCFGKSqIUQQohCTNqohXjMSeuWEEWblKiFEEKIQkwStRBCCFGISaIWQgghCjFJ1EIIIUQhJolaCCGEKMQkUQshhBCFmCRqIYQQohCTRC2EEEIUYpKohRBCiELs/8MalM17Gq5FAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/model_weights_apr_4.pth\")\n"
      ],
      "metadata": {
        "id": "52YXrhRNTCDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = GPTModel(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2.load_state_dict(torch.load(\"/content/model_weights_apr_4.pth\", map_location=device))\n",
        "model2.eval();"
      ],
      "metadata": {
        "id": "XVy1_t_z4NNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text=\"Life gives you one chance\"\n",
        "model2.to(device)\n",
        "generate_and_print_sample(model2, tokenizer, device, context_text)"
      ],
      "metadata": {
        "id": "1dBn9nRE4Gaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed75c852-2244-4154-dd1f-7a75b856e9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Life gives you one chance of the the fire, and the the other-humoured, and the the the the the the the the the man, and the the the the the the the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fexcPnM-xFRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3043a178-1e9c-443c-bbab-fdaddcb44bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/model_weights_apr_4.pth\" \"./model_weights_apr_4.pth\"\n"
      ],
      "metadata": {
        "id": "dCic_nNpxq2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ecc4c06-3472-465a-eecc-6124ecc98d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/model_weights_apr_4.pth': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMS-EyPU5IDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
